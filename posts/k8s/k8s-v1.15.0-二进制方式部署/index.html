<!DOCTYPE html>
<html lang="zh-CN">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1">
        <title>k8s v1.15.0 二进制方式部署 - MyBlog</title><meta name="Description" content="我的博客"><meta property="og:title" content="k8s v1.15.0 二进制方式部署" />
<meta property="og:description" content="部署k8s集群 参考： https://github.com/opsnull/follow-me-install-kubernetes-cluster https://zhangguanzhang.github.io/2019/03/03/kubernetes-1-13-4/ 集群信息： 主机名 IP 系统版本 角色 k8s-m1 192.168.15.5 CentOS Linux release 7.6.1810 master&#43;worker k8s-m2 192.168.15.6 CentOS Linux release 7.6.1810 master&#43;worker k8s-m3 192.168.15.7 CentOS Linux release 7.6.1810 master&#43;worker c48-1 192.168.15.161 CentOS Linux release 7.6.1810 worker c48-2 192.168.15.143 CentOS Linux release 7.6.1810 worker 网络信息： 名称 地" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://msdemt.github.io/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/" />
<meta property="og:image" content="https://msdemt.github.io/logo.png"/>
<meta property="article:published_time" content="2019-10-13T21:07:16+08:00" />
<meta property="article:modified_time" content="2019-10-13T21:07:16+08:00" />
<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://msdemt.github.io/logo.png"/>

<meta name="twitter:title" content="k8s v1.15.0 二进制方式部署"/>
<meta name="twitter:description" content="部署k8s集群 参考： https://github.com/opsnull/follow-me-install-kubernetes-cluster https://zhangguanzhang.github.io/2019/03/03/kubernetes-1-13-4/ 集群信息： 主机名 IP 系统版本 角色 k8s-m1 192.168.15.5 CentOS Linux release 7.6.1810 master&#43;worker k8s-m2 192.168.15.6 CentOS Linux release 7.6.1810 master&#43;worker k8s-m3 192.168.15.7 CentOS Linux release 7.6.1810 master&#43;worker c48-1 192.168.15.161 CentOS Linux release 7.6.1810 worker c48-2 192.168.15.143 CentOS Linux release 7.6.1810 worker 网络信息： 名称 地"/>
<meta name="application-name" content="LoveIt">
<meta name="apple-mobile-web-app-title" content="LoveIt"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://msdemt.github.io/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/" /><link rel="prev" href="https://msdemt.github.io/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" /><link rel="next" href="https://msdemt.github.io/posts/k8s/metallb-v0.8.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" /><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/normalize.css@8.0.1/normalize.min.css"><link rel="stylesheet" href="/css/style.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.13.0/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.7.2/animate.min.css"><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "k8s v1.15.0 二进制方式部署",
        "inLanguage": "zh-CN",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/msdemt.github.io\/posts\/k8s\/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2\/"
        },"image": ["https:\/\/msdemt.github.io\/images\/Apple-Devices-Preview.png"],"genre": "posts","keywords": "docker, k8s, devops","wordcount":  35086 ,
        "url": "https:\/\/msdemt.github.io\/posts\/k8s\/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2\/","datePublished": "2019-10-13T21:07:16+08:00","dateModified": "2019-10-13T21:07:16+08:00","license": "This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher": {
            "@type": "Organization",
            "name": "xxxx","logo": "https:\/\/msdemt.github.io\/images\/avatar.png"},"author": {
                "@type": "Person",
                "name": "hekai"
            },"description": ""
    }
    </script></head>
    <body header-desktop="fixed" header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="MyBlog"><span class="header-title-pre"><i class='far fa-kiss-wink-heart fa-fw'></i></span>MyBlog</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> 所有文章 </a><a class="menu-item" href="/tags/"> 标签 </a><a class="menu-item" href="/categories/"> 分类 </a><a class="menu-item" href="/categories/documentation/"> 文档 </a><a class="menu-item" href="/about/"> 关于 </a><a class="menu-item" href="https://github.com/dillonzq/LoveIt" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw'></i>  </a><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="搜索">
                            <i class="fas fa-search fa-fw"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="清空">
                            <i class="fas fa-times-circle fa-fw"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                    <i class="fas fa-adjust fa-fw"></i>
                </a>
            </div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="MyBlog"><span class="header-title-pre"><i class='far fa-kiss-wink-heart fa-fw'></i></span>MyBlog</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="搜索">
                            <i class="fas fa-search fa-fw"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="清空">
                            <i class="fas fa-times-circle fa-fw"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        取消
                    </a>
                </div><a class="menu-item" href="/posts/" title="">所有文章</a><a class="menu-item" href="/tags/" title="">标签</a><a class="menu-item" href="/categories/" title="">分类</a><a class="menu-item" href="/categories/documentation/" title="">文档</a><a class="menu-item" href="/about/" title="">关于</a><a class="menu-item" href="https://github.com/dillonzq/LoveIt" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw'></i></a><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                <i class="fas fa-adjust fa-fw"></i>
            </a></div>
    </div>
</header>
<div class="search-dropdown desktop">
    <div id="search-dropdown-desktop"></div>
</div>
<div class="search-dropdown mobile">
    <div id="search-dropdown-mobile"></div>
</div>
<main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">目录</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animated flipInX">k8s v1.15.0 二进制方式部署</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="/" title="Author" rel=" author" class="author"><i class="fas fa-user-circle fa-fw"></i>hekai</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime="2019-10-13">2019-10-13</time>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;约 35086 字&nbsp;
                <i class="far fa-clock fa-fw"></i>&nbsp;预计阅读 71 分钟&nbsp;</div>
        </div><div class="details toc" id="toc-static"  kept="">
                <div class="details-summary toc-title">
                    <span>目录</span>
                    <span><i class="details-icon fas fa-angle-right"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#部署k8s集群">部署k8s集群</a>
      <ul>
        <li><a href="#准备工作">准备工作</a></li>
        <li><a href="#配置-ntp-时间同步">配置 NTP 时间同步</a></li>
        <li><a href="#使用环境变量声明集群信息">使用环境变量声明集群信息</a></li>
        <li><a href="#分发二进制文件">分发二进制文件</a></li>
        <li><a href="#建立集群-ca-keys-与-certificates">建立集群 CA keys 与 Certificates</a>
          <ul>
            <li><a href="#准备-openssl-证书配置文件">准备 openssl 证书配置文件</a></li>
            <li><a href="#生成证书">生成证书</a></li>
            <li><a href="#分发证书">分发证书</a></li>
          </ul>
        </li>
        <li><a href="#安装-etcd">安装 etcd</a></li>
        <li><a href="#kubernetes-master">Kubernetes Master</a>
          <ul>
            <li><a href="#安装-haproxy--keepalived">安装 haproxy + keepalived</a></li>
            <li><a href="#在所有节点安装-kube-nginx">在所有节点安装 kube-nginx</a></li>
            <li><a href="#master-组件">Master 组件</a></li>
            <li><a href="#配置-bootstrap">配置 bootstrap</a></li>
          </ul>
        </li>
        <li><a href="#all-kubernetes-nodes">All Kubernetes Nodes</a></li>
      </ul>
    </li>
    <li><a href="#kubernetes-core-addons">Kubernetes Core Addons</a>
      <ul>
        <li><a href="#kube-proxy">kube-proxy</a>
          <ul>
            <li><a href="#二进制方式部署-kube-proxy">二进制方式部署 kube-proxy</a></li>
            <li><a href="#daemonset-方式部署-kube-proxy">daemonset 方式部署 kube-proxy</a></li>
          </ul>
        </li>
        <li><a href="#网络插件-flannel-或-calico">网络插件 flannel 或 calico</a>
          <ul>
            <li><a href="#flannel">flannel</a></li>
            <li><a href="#calico">calico</a></li>
          </ul>
        </li>
        <li><a href="#coredns-dns-and-service-discovery">CoreDNS: DNS and Service Discovery</a></li>
        <li><a href="#dns-horizontal-autoscaler">dns-horizontal-autoscaler</a></li>
        <li><a href="#metrics-server">metrics-server</a></li>
      </ul>
    </li>
    <li><a href="#kubernetes-extra-addons">Kubernetes Extra Addons</a>
      <ul>
        <li><a href="#kubernetes-dashboard">kubernetes-dashboard</a>
          <ul>
            <li><a href="#阅读-dashboard-官方文档">阅读 Dashboard 官方文档</a></li>
            <li><a href="#安装-dashboard">安装 Dashboard</a></li>
          </ul>
        </li>
        <li><a href="#helm">Helm</a>
          <ul>
            <li><a href="#阅读-helm-官方文档">阅读 Helm 官方文档</a></li>
            <li><a href="#helm-介绍">Helm 介绍</a></li>
            <li><a href="#安装-helm">安装 Helm</a>
              <ul>
                <li><a href="#安装-helm-客户端">安装 helm 客户端</a></li>
                <li><a href="#安装-helm-服务端tiller">安装 helm 服务端（tiller）</a></li>
              </ul>
            </li>
          </ul>
        </li>
        <li><a href="#metallb">MetalLB</a>
          <ul>
            <li><a href="#阅读-metallb官方文档">阅读 MetalLB官方文档</a></li>
            <li><a href="#metallb-介绍">MetalLB 介绍</a></li>
            <li><a href="#安装-metallb">安装 MetalLB</a></li>
            <li><a href="#测试-loadbalancer">测试 LoadBalancer</a></li>
          </ul>
        </li>
        <li><a href="#ingress-nginx">ingress-nginx</a>
          <ul>
            <li><a href="#阅读-ingress-nginx-官方文档">阅读 ingress-nginx 官方文档</a></li>
            <li><a href="#ingress-nginx-介绍">ingress-nginx 介绍</a></li>
            <li><a href="#部署-ingress-controller">部署 Ingress Controller</a>
              <ul>
                <li><a href="#load-balancer-方式部署">load-balancer 方式部署</a></li>
                <li><a href="#hostnetwork-方式部署">hostNetwork 方式部署</a></li>
              </ul>
            </li>
            <li><a href="#测试-ingress">测试 Ingress</a></li>
            <li><a href="#ingress-配置-https">Ingress 配置 https</a></li>
            <li><a href="#使用-ingress-为-kubernetes-dashboard-暴露服务">使用 ingress 为 kubernetes dashboard 暴露服务</a></li>
          </ul>
        </li>
        <li><a href="#cert-manager">cert-manager</a>
          <ul>
            <li><a href="#阅读-cert-manager官方文档">阅读 cert-manager官方文档</a></li>
            <li><a href="#cert-manager-介绍">cert-manager 介绍</a>
              <ul>
                <li><a href="#配置-ca-issuers">配置 CA Issuers</a></li>
                <li><a href="#配置自签名-issuer">配置自签名 Issuer</a></li>
                <li><a href="#为-ingress-资源自动创建证书">为 ingress 资源自动创建证书</a></li>
              </ul>
            </li>
            <li><a href="#安装-cert-manager">安装 cert-manager</a>
              <ul>
                <li><a href="#配置-ca-issuers-1">配置 CA Issuers</a></li>
              </ul>
            </li>
          </ul>
        </li>
        <li><a href="#nfs-storageclass">nfs-storageclass</a>
          <ul>
            <li><a href="#安装-nfs-服务器">安装 NFS 服务器</a></li>
            <li><a href="#安装-nfs-客户端">安装 NFS 客户端</a></li>
          </ul>
        </li>
        <li><a href="#harbor">Harbor</a>
          <ul>
            <li><a href="#阅读-harbor-官方文档">阅读 Harbor 官方文档</a></li>
            <li><a href="#安装-harbor">安装 Harbor</a></li>
          </ul>
        </li>
        <li><a href="#tidb">tidb</a>
          <ul>
            <li><a href="#官方文档">官方文档</a></li>
            <li><a href="#安装">安装</a></li>
          </ul>
        </li>
        <li><a href="#jenkins">jenkins</a>
          <ul>
            <li><a href="#jenkins-master">jenkins master</a></li>
            <li><a href="#jenkins-slaver">jenkins slaver</a></li>
            <li><a href="#jdk-images">jdk images</a></li>
            <li><a href="#jenkins-pipeline">jenkins pipeline</a></li>
          </ul>
        </li>
        <li><a href="#rook-ceph">rook-ceph</a></li>
        <li><a href="#istio">istio</a></li>
        <li><a href="#knative">knative</a></li>
        <li><a href="#wayne">wayne</a></li>
        <li><a href="#prometheus">prometheus</a></li>
        <li><a href="#efk">efk</a></li>
        <li><a href="#gitlab">gitlab</a></li>
        <li><a href="#external-dns">external-dns</a></li>
      </ul>
    </li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><h2 id="部署k8s集群">部署k8s集群</h2>
<blockquote>
<p>参考：<br>
<a href="https://github.com/opsnull/follow-me-install-kubernetes-cluster">https://github.com/opsnull/follow-me-install-kubernetes-cluster</a><br>
<a href="https://zhangguanzhang.github.io/2019/03/03/kubernetes-1-13-4/">https://zhangguanzhang.github.io/2019/03/03/kubernetes-1-13-4/</a></p>
</blockquote>
<p><strong>集群信息</strong>：</p>
<table>
<thead>
<tr>
<th>主机名</th>
<th>IP</th>
<th>系统版本</th>
<th>角色</th>
</tr>
</thead>
<tbody>
<tr>
<td>k8s-m1</td>
<td>192.168.15.5</td>
<td>CentOS Linux release 7.6.1810</td>
<td>master+worker</td>
</tr>
<tr>
<td>k8s-m2</td>
<td>192.168.15.6</td>
<td>CentOS Linux release 7.6.1810</td>
<td>master+worker</td>
</tr>
<tr>
<td>k8s-m3</td>
<td>192.168.15.7</td>
<td>CentOS Linux release 7.6.1810</td>
<td>master+worker</td>
</tr>
<tr>
<td>c48-1</td>
<td>192.168.15.161</td>
<td>CentOS Linux release 7.6.1810</td>
<td>worker</td>
</tr>
<tr>
<td>c48-2</td>
<td>192.168.15.143</td>
<td>CentOS Linux release 7.6.1810</td>
<td>worker</td>
</tr>
</tbody>
</table>
<p><strong>网络信息</strong>：</p>
<table>
<thead>
<tr>
<th>名称</th>
<th>地址</th>
</tr>
</thead>
<tbody>
<tr>
<td>Cluster IP CIDR</td>
<td>172.30.0.0/16</td>
</tr>
<tr>
<td>Service Cluster IP CIDR</td>
<td>10.96.0.0/12</td>
</tr>
<tr>
<td>Service DNS IP</td>
<td>10.96.0.10</td>
</tr>
<tr>
<td>DNS DN</td>
<td>cluster.local</td>
</tr>
</tbody>
</table>
<h3 id="准备工作">准备工作</h3>
<ol>
<li>
<p>所有主机彼此网络互通</p>
</li>
<li>
<p>所有主机的hosts文件中添加主机名与IP的映射。</p>
<p>执行后，在 k8s-m1 上查看，结果如下所示：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="o">[</span>root@k8s-m1 ~<span class="o">]</span><span class="c1"># cat /etc/hosts</span>
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
192.168.15.5  k8s-m1
192.168.15.6  k8s-m2
192.168.15.7  k8s-m3
192.168.15.161 c48-1
192.168.15.143 c48-2
</code></pre></td></tr></table>
</div>
</div><p>执行后，在 k8s-m1 上查看，结果如下所示：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="o">[</span>root@k8s-m1 ~<span class="o">]</span><span class="c1"># cat /etc/hosts</span>
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
192.168.15.5  k8s-m1
192.168.15.6  k8s-m2
192.168.15.7  k8s-m3
192.168.15.161 c48-1
192.168.15.143 c48-2
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>为方便部署，在 k8s-m1 上配置无密码 ssh 登录其他节点。</p>
<p><strong>方法一</strong>：在 k8s-m1 上执行如下命令：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">ssh-keygen -t rsa
ssh-copy-id root@k8s-1
ssh-copy-id root@k8s-2
ssh-copy-id root@k8s-3
ssh-copy-id root@c48-1
ssh-copy-id root@c48-2
</code></pre></td></tr></table>
</div>
</div><p><strong>方法二</strong>：在 k8s-m1 上安装sshpass，然后通过配置别名来让 ssh 和 scp 不输入密码，其中 123456 为所有主机的密码。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">yum install -y sshpass
<span class="nb">alias</span> <span class="nv">ssh</span><span class="o">=</span><span class="s1">&#39;sshpass -p 123456 ssh -o StrictHostKeyChecking=no&#39;</span>
<span class="nb">alias</span> <span class="nv">scp</span><span class="o">=</span><span class="s1">&#39;sshpass -p 123456 scp -o StrictHostKeyChecking=no&#39;</span>
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>所有主机关闭防火墙和 SELinux
所有主机关闭防火墙和 SELinux，否则后续挂载目录时可能报错：Permission denied。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">systemctl disable --now firewalld NetworkManager
setenforce <span class="m">0</span>
sed -ri <span class="s1">&#39;/^[^#]*SELINUX=/s#=.+$#=disabled#&#39;</span> /etc/selinux/config
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>关闭 dnsmasq
linux 系统开启了 dnsmasq 后（如GUI环境），将系统 DNS Server 设置为127.0.0.1，这会导致 docker 容器无法解析域名，需要关闭它。（最小安装的 centos7 系统默认不启动 dnsmasq ）</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">systemctl disable --now dnsmasq
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>关闭 swap 分区
Kubernetes v1.8+要求关闭系统Swap，若不关闭则需要修改kubelet设定参数（&ndash;fail-swap-on设置为false来忽略swap on），在所有主机上使用如下指令关闭swap并注释掉/etc/fstab中swap的行。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">swapoff -a <span class="o">&amp;&amp;</span> sysctl -w vm.swappiness<span class="o">=</span><span class="m">0</span>
sed -ri <span class="s1">&#39;/^[^#]*swap/s@^@#@&#39;</span> /etc/fstab
</code></pre></td></tr></table>
</div>
</div><blockquote>
<p>打开 swap 分区 ：执行下 sysctl -w vm.swappiness=1 然后编辑 vi /etc/fstab 去掉注释 在执行下这个 swapon -a</p>
</blockquote>
</li>
<li>
<p>升级内核（可选）</p>
<ol>
<li>
<p>更新系统，不升级内核，执行如下命令：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">yum install -y epel-release
yum install -y wget git jq psmisc socat
yum update -y
reboot
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>更新系统，升级内核</p>
<ol>
<li>
<p>更新系统</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">yum install -y epel-release
yum install -y wget git jq psmisc socat
yum update -y --exclude<span class="o">=</span>kernel*
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>perl 是内核的依赖包，如下命令检测是否存在 perl，如果不存在则安装</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="o">[</span> ! -f /usr/bin/perl <span class="o">]</span> <span class="o">&amp;&amp;</span> yum install perl -y
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>升级内核需要使用 elrepo 的 yum 源，首先我们导入 elrepo 的 key 并安装 elrepo 源</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org
rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm
</code></pre></td></tr></table>
</div>
</div><p>查看可用的内核</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">yum --disablerepo<span class="o">=</span><span class="s2">&#34;*&#34;</span> --enablerepo<span class="o">=</span><span class="s2">&#34;elrepo-kernel&#34;</span> list available  --showduplicates
</code></pre></td></tr></table>
</div>
</div><p>在 yum 的 elrepo 源中， mainline 为最新版本的内核， ipvs 依赖 nf_conntrack_ipv4 内核模块， 4.19即后续的内核版本将该内核模块改名为 nf_conntrack。</p>
<blockquote>
<p>下面的链接可以下载到其他的归档版本：<br>
ubuntu  <a href="http://kernel.ubuntu.com/~kernel-ppa/mainline/">http://kernel.ubuntu.com/~kernel-ppa/mainline/</a><br>
RHEL  <a href="http://mirror.rc.usf.edu/compute_lock/elrepo/kernel/el7/x86_64/RPMS/">http://mirror.rc.usf.edu/compute_lock/elrepo/kernel/el7/x86_64/RPMS/</a></p>
</blockquote>
<p>下面是 ml 的内核和上面归档内核版本任选其一的安装方法：</p>
<ol>
<li>
<p>自选内核版本安装</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="nb">export</span> <span class="nv">Kernel_Version</span><span class="o">=</span>4.18.9-1
wget  http://mirror.rc.usf.edu/compute_lock/elrepo/kernel/el7/x86_64/RPMS/kernel-ml<span class="o">{</span>,-devel<span class="o">}</span>-<span class="si">${</span><span class="nv">Kernel_Version</span><span class="si">}</span>.el7.elrepo.x86_64.rpm
yum localinstall -y kernel-ml*
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>最新内核版本安装</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">yum --disablerepo<span class="o">=</span><span class="s2">&#34;*&#34;</span> --enablerepo<span class="o">=</span><span class="s2">&#34;elrepo-kernel&#34;</span> list available  --showduplicates <span class="p">|</span> grep -Po <span class="s1">&#39;^kernel-ml.x86_64\s+\K\S+(?=.el7)&#39;</span>
yum --disablerepo<span class="o">=</span><span class="s2">&#34;*&#34;</span> --enablerepo<span class="o">=</span>elrepo-kernel install -y kernel-ml<span class="o">{</span>,-devel<span class="o">}</span>
</code></pre></td></tr></table>
</div>
</div></li>
</ol>
</li>
<li>
<p>修改内核启动顺序，默认启动的顺序应该为 1，升级后内核是往前插入，为 0</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">grub2-set-default  <span class="m">0</span> <span class="o">&amp;&amp;</span> grub2-mkconfig -o /etc/grub2.cfg
</code></pre></td></tr></table>
</div>
</div><p>使用下面的命令确认下启动的默认内核是否指向上面安装的内核</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">grubby --default-kernel
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>docker官方的内核检查脚本建议（RHEL7/CentOS7: User namespaces disabled; add &lsquo;user_namespace.enable=1&rsquo; to boot command line），使用下面命令开启</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">grubby --args<span class="o">=</span><span class="s2">&#34;user_namespace.enable=1&#34;</span> --update-kernel<span class="o">=</span><span class="s2">&#34;</span><span class="k">$(</span>grubby --default-kernel<span class="k">)</span><span class="s2">&#34;</span>
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>重启加载新内核</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">reboot
</code></pre></td></tr></table>
</div>
</div></li>
</ol>
</li>
</ol>
</li>
<li>
<p>所有主机安装 ipvs</p>
<p>centos:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">yum install -y ipvsadm ipset sysstat conntrack libseccomp
</code></pre></td></tr></table>
</div>
</div><p>ubuntu</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">sudo apt-get install -y wget git conntrack ipvsadm ipset jq sysstat curl iptables libseccomp
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>所有主机配置开机加载的内核模块</p>
<p>每台主机上执行如下命令：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">:&gt; /etc/modules-load.d/ipvs.conf
<span class="nv">module</span><span class="o">=(</span>
ip_vs
ip_vs_rr
ip_vs_wrr
ip_vs_sh
nf_conntrack
br_netfilter
rbd
  <span class="o">)</span>
<span class="k">for</span> kernel_module in <span class="si">${</span><span class="nv">module</span><span class="p">[@]</span><span class="si">}</span><span class="p">;</span><span class="k">do</span>
    /sbin/modinfo -F filename <span class="nv">$kernel_module</span> <span class="p">|&amp;</span> grep -qv ERROR <span class="o">&amp;&amp;</span> <span class="nb">echo</span> <span class="nv">$kernel_module</span> &gt;&gt; /etc/modules-load.d/ipvs.conf <span class="o">||</span> :
<span class="k">done</span>
systemctl <span class="nb">enable</span> --now systemd-modules-load.service
</code></pre></td></tr></table>
</div>
</div><p>执行后生成的文件内容如下所示：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="o">[</span>root@k8s-m1 ~<span class="o">]</span><span class="c1"># cat /etc/modules-load.d/ipvs.conf</span>
ip_vs
ip_vs_rr
ip_vs_wrr
ip_vs_sh
nf_conntrack
rbd
</code></pre></td></tr></table>
</div>
</div><blockquote>
<p>生成的文件中没有 br_netfilter ，因为新版本系统已经将 br_netfilter 集成在了内核中。
rook-ceph 需要使用 rbd 模块，所以加上了 rbd 模块进行开机加载。</p>
</blockquote>
</li>
<li>
<p>所有主机配置系统参数</p>
<p>配置 /etc/sysctl.d/k8s.conf 的系统参数</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">cat <span class="s">&lt;&lt;EOF &gt; /etc/sysctl.d/k8s.conf
</span><span class="s"># https://github.com/moby/moby/issues/31208
</span><span class="s"># ipvsadm -l --timout
</span><span class="s"># 修复ipvs模式下长连接timeout问题 小于900即可
</span><span class="s">net.ipv4.tcp_keepalive_time = 600
</span><span class="s">net.ipv4.tcp_keepalive_intvl = 30
</span><span class="s">net.ipv4.tcp_keepalive_probes = 10
</span><span class="s">net.ipv6.conf.all.disable_ipv6 = 1
</span><span class="s">net.ipv6.conf.default.disable_ipv6 = 1
</span><span class="s">net.ipv6.conf.lo.disable_ipv6 = 1
</span><span class="s">net.ipv4.neigh.default.gc_stale_time = 120
</span><span class="s">net.ipv4.conf.all.rp_filter = 0
</span><span class="s">net.ipv4.conf.default.rp_filter = 0
</span><span class="s">net.ipv4.conf.default.arp_announce = 2
</span><span class="s">net.ipv4.conf.lo.arp_announce = 2
</span><span class="s">net.ipv4.conf.all.arp_announce = 2
</span><span class="s">net.ipv4.ip_forward = 1
</span><span class="s">net.ipv4.tcp_max_tw_buckets = 5000
</span><span class="s">net.ipv4.tcp_syncookies = 1
</span><span class="s">net.ipv4.tcp_max_syn_backlog = 1024
</span><span class="s">net.ipv4.tcp_synack_retries = 2
</span><span class="s">net.bridge.bridge-nf-call-ip6tables = 1
</span><span class="s">net.bridge.bridge-nf-call-iptables = 1
</span><span class="s">net.netfilter.nf_conntrack_max = 2310720
</span><span class="s">fs.inotify.max_user_watches=89100
</span><span class="s">fs.may_detach_mounts = 1
</span><span class="s">fs.file-max = 52706963
</span><span class="s">fs.nr_open = 52706963
</span><span class="s">net.bridge.bridge-nf-call-arptables = 1
</span><span class="s">vm.swappiness = 0
</span><span class="s">vm.overcommit_memory=1
</span><span class="s">vm.panic_on_oom=0
</span><span class="s">net.ipv4.neigh.default.gc_thresh1=8192
</span><span class="s">net.ipv4.neigh.default.gc_thresh2=32768
</span><span class="s">net.ipv4.neigh.default.gc_thresh3=65536
</span><span class="s">EOF</span>

sysctl --system
</code></pre></td></tr></table>
</div>
</div><blockquote>
<p>添加 net.ipv4.neigh.default.gc_thresh1-3 是为了解决在 c48-1 上无法 ping 通其他网段 ip 的问题。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="o">[</span>root@c48-1 ~<span class="o">]</span><span class="c1"># ping 1.1.0.11</span>
PING 1.1.0.11 <span class="o">(</span>1.1.0.11<span class="o">)</span> 56<span class="o">(</span>84<span class="o">)</span> bytes of data.
ping: sendmsg: 无效的参数
ping: sendmsg: 无效的参数
ping: sendmsg: 无效的参数
</code></pre></td></tr></table>
</div>
</div><p>1.1.0.11 为 c48-1 主机（双网卡）另一个网卡连接的网段的地址<br>
参考：<a href="https://mozillazg.com/2017/10/linux-a-way-to-fix-haproxy-network-connection-timeout-ping-sendmsg-invalid-argument-socket-errno-110-connection-timed-out">https://mozillazg.com/2017/10/linux-a-way-to-fix-haproxy-network-connection-timeout-ping-sendmsg-invalid-argument-socket-errno-110-connection-timed-out</a></p>
</blockquote>
</li>
<li>
<p>所有主机安装 docker</p>
<p>检查系统内核和模块是否适合运行 docker (仅适用于 linux 系统)</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">curl https://raw.githubusercontent.com/docker/docker/master/contrib/check-config.sh &gt; check-config.sh
bash ./check-config.sh
</code></pre></td></tr></table>
</div>
</div><p>在官方查看 k8s 支持的 docker 版本： <a href="https://github.com/kubernetes/kubernetes">https://github.com/kubernetes/kubernetes</a> 里进对应版本的 changelog 里搜 <strong>The list of validated docker versions remain</strong></p>
<ol>
<li>
<p>使用 docker 的官方安装脚本进行安装，可以使用如下命令查看可用的 docker 版本。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">yum list docker-ce --showduplicates <span class="p">|</span> sort -r
</code></pre></td></tr></table>
</div>
</div><p>这里我使用的版本为 18.06.03</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="nb">export</span> <span class="nv">VERSION</span><span class="o">=</span>18.06
curl -fsSL <span class="s2">&#34;https://get.docker.com/&#34;</span> <span class="p">|</span> bash -s -- --mirror Aliyun
</code></pre></td></tr></table>
</div>
</div><blockquote>
<p>非 root 用户使用 docker<br>
If you would like to use Docker as a non-root user, you should now consider adding your user to the &ldquo;docker&rdquo; group with something like:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">sudo usermod -aG docker your-user
</code></pre></td></tr></table>
</div>
</div><p>Remember that you will have to log out and back in for this to take effect!</p>
</blockquote>
</li>
<li>
<p>配置 docker 加速源并配置 docker 的启动参数使用 systemd</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">mkdir -p /etc/docker/
cat&gt;/etc/docker/daemon.json<span class="s">&lt;&lt;EOF
</span><span class="s">{
</span><span class="s">  &#34;exec-opts&#34;: [&#34;native.cgroupdriver=systemd&#34;],
</span><span class="s">  &#34;registry-mirrors&#34;: [&#34;http://hub-mirror.c.163.com&#34;],
</span><span class="s">  &#34;storage-driver&#34;: &#34;overlay2&#34;,
</span><span class="s">  &#34;storage-opts&#34;: [
</span><span class="s">    &#34;overlay2.override_kernel_check=true&#34;
</span><span class="s">  ],
</span><span class="s">  &#34;log-driver&#34;: &#34;json-file&#34;,
</span><span class="s">  &#34;log-opts&#34;: {
</span><span class="s">    &#34;max-size&#34;: &#34;100m&#34;,
</span><span class="s">    &#34;max-file&#34;: &#34;3&#34;
</span><span class="s">  }
</span><span class="s">}
</span><span class="s">EOF</span>
</code></pre></td></tr></table>
</div>
</div><blockquote>
<p>k8s 官方建议使用 systemd，详见：<a href="https://kubernetes.io/docs/setup/cri/">https://kubernetes.io/docs/setup/cri/</a></p>
</blockquote>
</li>
<li>
<p>修改 docker 的数据存储目录</p>
<p>docker 默认的数据存储目录为 /var/lib/docker，由于该目录所属的磁盘分区较小，所以将其软链接到 /home/data/docker 目录</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">rm -rf /var/lib/docker
mkdir -p /home/data/docker
ln -s /home/data/docker /var/lib/docker
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>设置 docker 命令补全和开机启动</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">yum install -y epel-release bash-completion <span class="o">&amp;&amp;</span> cp /usr/share/bash-completion/completions/docker /etc/bash_completion.d/
systemctl <span class="nb">enable</span> --now docker
</code></pre></td></tr></table>
</div>
</div></li>
</ol>
</li>
</ol>
<h3 id="配置-ntp-时间同步">配置 NTP 时间同步</h3>
<p>略</p>
<h3 id="使用环境变量声明集群信息">使用环境变量声明集群信息</h3>
<p>将集群信息写进 <code>env.sh</code> 文件中，使用 source 命令将文件中的信息加载到环境变量中。</p>
<p><code>env.sh</code> 内容如下所示：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="o">[</span>root@k8s-m1 k8s<span class="o">]</span><span class="c1"># pwd</span>
/home/k8s
<span class="o">[</span>root@k8s-m1 k8s<span class="o">]</span><span class="c1"># cat env.sh</span>
<span class="c1"># 声明集群成员信息</span>
<span class="nb">declare</span> -A MasterArray otherMaster NodeArray AllNode Other
<span class="nv">MasterArray</span><span class="o">=([</span><span class="s1">&#39;k8s-m1&#39;</span><span class="o">]=</span>192.168.15.5 <span class="o">[</span><span class="s1">&#39;k8s-m2&#39;</span><span class="o">]=</span>192.168.15.6 <span class="o">[</span><span class="s1">&#39;k8s-m3&#39;</span><span class="o">]=</span>192.168.15.7<span class="o">)</span>
<span class="nv">otherMaster</span><span class="o">=([</span><span class="s1">&#39;k8s-m2&#39;</span><span class="o">]=</span>192.168.15.6 <span class="o">[</span><span class="s1">&#39;k8s-m3&#39;</span><span class="o">]=</span>192.168.15.7<span class="o">)</span>
<span class="nv">NodeArray</span><span class="o">=([</span><span class="s1">&#39;k8s-m1&#39;</span><span class="o">]=</span>192.168.15.5 <span class="o">[</span><span class="s1">&#39;k8s-m2&#39;</span><span class="o">]=</span>192.168.15.6 <span class="o">[</span><span class="s1">&#39;k8s-m3&#39;</span><span class="o">]=</span>192.168.15.7 <span class="o">[</span><span class="s1">&#39;c48-1&#39;</span><span class="o">]=</span>192.168.15.161 <span class="o">[</span><span class="s1">&#39;c48-2&#39;</span><span class="o">]=</span>192.168.15.143<span class="o">)</span>
<span class="c1"># 下面复制上面的信息粘贴即可</span>
<span class="nv">AllNode</span><span class="o">=([</span><span class="s1">&#39;k8s-m1&#39;</span><span class="o">]=</span>192.168.15.5 <span class="o">[</span><span class="s1">&#39;k8s-m2&#39;</span><span class="o">]=</span>192.168.15.6 <span class="o">[</span><span class="s1">&#39;k8s-m3&#39;</span><span class="o">]=</span>192.168.15.7 <span class="o">[</span><span class="s1">&#39;c48-1&#39;</span><span class="o">]=</span>192.168.15.161 <span class="o">[</span><span class="s1">&#39;c48-2&#39;</span><span class="o">]=</span>192.168.15.143<span class="o">)</span>
<span class="nv">Other</span><span class="o">=([</span><span class="s1">&#39;k8s-m2&#39;</span><span class="o">]=</span>192.168.15.6 <span class="o">[</span><span class="s1">&#39;k8s-m3&#39;</span><span class="o">]=</span>192.168.15.7 <span class="o">[</span><span class="s1">&#39;c48-1&#39;</span><span class="o">]=</span>192.168.15.161 <span class="o">[</span><span class="s1">&#39;c48-2&#39;</span><span class="o">]=</span>192.168.15.143<span class="o">)</span>

<span class="nb">export</span> <span class="nv">VIP</span><span class="o">=</span>127.0.0.1

<span class="o">[</span> <span class="s2">&#34;</span><span class="si">${#</span><span class="nv">MasterArray</span><span class="p">[@]</span><span class="si">}</span><span class="s2">&#34;</span> -eq <span class="m">1</span> <span class="o">]</span>  <span class="o">&amp;&amp;</span> <span class="nb">export</span> <span class="nv">VIP</span><span class="o">=</span><span class="si">${</span><span class="nv">MasterArray</span><span class="p">[@]</span><span class="si">}</span> <span class="o">||</span> <span class="nb">export</span> <span class="nv">API_PORT</span><span class="o">=</span><span class="m">8443</span>
<span class="nb">export</span> <span class="nv">KUBE_APISERVER</span><span class="o">=</span>https://<span class="si">${</span><span class="nv">VIP</span><span class="si">}</span>:<span class="si">${</span><span class="nv">API_PORT</span><span class="p">:=6443</span><span class="si">}</span>

<span class="c1">#声明需要安装的的k8s版本</span>
<span class="nb">export</span> <span class="nv">KUBE_VERSION</span><span class="o">=</span>v1.15.0

<span class="c1"># 网卡名</span>
<span class="nb">export</span> <span class="nv">interface</span><span class="o">=</span>ens33

<span class="c1"># cni</span>
<span class="nb">export</span> <span class="nv">CNI_URL</span><span class="o">=</span><span class="s2">&#34;https://github.com/containernetworking/plugins/releases/download&#34;</span>
<span class="nb">export</span> <span class="nv">CNI_VERSION</span><span class="o">=</span>v0.7.5

<span class="c1"># etcd</span>
<span class="nb">export</span> <span class="nv">ETCD_version</span><span class="o">=</span>v3.3.10

<span class="c1"># DNS DN</span>
<span class="nb">export</span> <span class="nv">CLUSTER_DNS_DOMAIN</span><span class="o">=</span>cluster.local

<span class="c1"># DNS IP</span>
<span class="nb">export</span> <span class="nv">CLUSTER_DNS_SVC_IP</span><span class="o">=</span>10.96.0.10
</code></pre></td></tr></table>
</div>
</div><blockquote>
<p>注意：API Server的负载均衡和高可用的方案若使用 haproxy+keepalived ，需要将 VIP 指定为当前局域网内未使用的 ip 。
网卡名 interface 为 keepalived 绑定的网卡。</p>
</blockquote>
<p>使用如下命令将文件中的信息加载到环境变量中</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="nb">source</span> env.sh
</code></pre></td></tr></table>
</div>
</div><h3 id="分发二进制文件">分发二进制文件</h3>
<ol>
<li>
<p>在 k8s-m1 上通过 git 获取部署需要的二进制配置和yml文件</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">git clone https://github.com/msdemt/k8s-manual-files.git /home/k8s/k8s-manual-files -b v1.15.0
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>在 k8s-m1 上下载 k8s v1.15.0 server端的发布包，并将 k8s 的二进制文件复制到 /usr/local/bin 目录下</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">wget https://storage.googleapis.com/kubernetes-release/release/<span class="si">${</span><span class="nv">KUBE_VERSION</span><span class="si">}</span>/kubernetes-server-linux-amd64.tar.gz
tar -zxvf kubernetes-server-linux-amd64.tar.gz  --strip-components<span class="o">=</span><span class="m">3</span> -C /usr/local/bin kubernetes/server/bin/kube<span class="o">{</span>let,ctl,-apiserver,-controller-manager,-scheduler,-proxy<span class="o">}</span>
</code></pre></td></tr></table>
</div>
</div><blockquote>
<p>需要翻墙下载</p>
</blockquote>
<p>也可以使用 k8s-manual-files 中提供的 v1.15.0 的二进制文件</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">cp /home/k8s/k8s-manual-files/master/bin/* /usr/local/bin/
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>将 k8s master 相关的二进制组件分发到其它 master 上</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="k">for</span> NODE in <span class="s2">&#34;</span><span class="si">${</span><span class="p">!otherMaster[@]</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">;</span> <span class="k">do</span>
    <span class="nb">echo</span> <span class="s2">&#34;--- </span><span class="nv">$NODE</span><span class="s2"> </span><span class="si">${</span><span class="nv">otherMaster</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span><span class="s2"> ---&#34;</span>
    scp /usr/local/bin/kube<span class="o">{</span>let,ctl,-apiserver,-controller-manager,-scheduler,-proxy<span class="o">}</span> <span class="si">${</span><span class="nv">otherMaster</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span>:/usr/local/bin/
<span class="k">done</span>
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>将 k8s worker 相关的二进制组件分发到其他的 worker 上</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="k">for</span> NODE in <span class="s2">&#34;</span><span class="si">${</span><span class="p">!NodeArray[@]</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">;</span> <span class="k">do</span>
    <span class="nb">echo</span> <span class="s2">&#34;--- </span><span class="nv">$NODE</span><span class="s2"> </span><span class="si">${</span><span class="nv">NodeArray</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span><span class="s2"> ---&#34;</span>
    scp /usr/local/bin/kube<span class="o">{</span>let,-proxy<span class="o">}</span> <span class="si">${</span><span class="nv">NodeArray</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span>:/usr/local/bin/
<span class="k">done</span>
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>在 k8s-m1 上下载 kubernetes cni 二进制文件并分发到其他主机</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">mkdir -p /opt/cni/bin
wget  <span class="s2">&#34;</span><span class="si">${</span><span class="nv">CNI_URL</span><span class="si">}</span><span class="s2">/</span><span class="si">${</span><span class="nv">CNI_VERSION</span><span class="si">}</span><span class="s2">/cni-plugins-amd64-</span><span class="si">${</span><span class="nv">CNI_VERSION</span><span class="si">}</span><span class="s2">.tgz&#34;</span>
tar -zxf cni-plugins-amd64-<span class="si">${</span><span class="nv">CNI_VERSION</span><span class="si">}</span>.tgz -C /opt/cni/bin

<span class="c1"># 分发cni文件</span>
<span class="k">for</span> NODE in <span class="s2">&#34;</span><span class="si">${</span><span class="p">!Other[@]</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">;</span> <span class="k">do</span>
    <span class="nb">echo</span> <span class="s2">&#34;--- </span><span class="nv">$NODE</span><span class="s2"> </span><span class="si">${</span><span class="nv">Other</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span><span class="s2"> ---&#34;</span>
    ssh <span class="si">${</span><span class="nv">Other</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span> <span class="s1">&#39;mkdir -p /opt/cni/bin&#39;</span>
    scp /opt/cni/bin/* <span class="si">${</span><span class="nv">Other</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span>:/opt/cni/bin/
<span class="k">done</span>
</code></pre></td></tr></table>
</div>
</div></li>
</ol>
<h3 id="建立集群-ca-keys-与-certificates">建立集群 CA keys 与 Certificates</h3>
<p>在这个部分，将需要产生多个元件的 Certificates，这包含 Etcd、 Kubernetes 元件等,并且每个集群都会有一个根数位凭证认证机构（Root Certificate Authority）被用在认证 API Server 与 Kubelet 端的凭证。</p>
<p>PS: 注意 CA JSON 中的 CN(Common Name) 与 O(Organization) 等内容是会影响 Kubernetes 元件认证的。<br>
<strong>CN (Common Name)</strong>: apiserver 会从证书中提取该字段作为请求的用户名 (User Name)
<strong>O (Organization)</strong>： apiserver  会从证书中提取该字段作为请求用户所属的组 (Group)</p>
<p><strong>CA (Certificate Authority)</strong> 是自签名的根证书，用来签名后续创建的其它证书。</p>
<p>本文使用<strong>openssl</strong>创建所有证书。</p>
<h4 id="准备-openssl-证书配置文件">准备 openssl 证书配置文件</h4>
<p>将 IP 信息注入到 openssl.cnf 文件中</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">mkdir -p /etc/kubernetes/pki/etcd
sed -i <span class="s2">&#34;/IP.2/a IP.3 = </span><span class="nv">$VIP</span><span class="s2">&#34;</span> /home/k8s/k8s-manual-files/pki/openssl.cnf
sed -ri <span class="s1">&#39;/IP.3/r &#39;</span>&lt;<span class="o">(</span> paste -d <span class="s1">&#39;&#39;</span> &lt;<span class="o">(</span>seq -f <span class="s1">&#39;IP.%g = &#39;</span> <span class="m">4</span> $<span class="o">[</span><span class="si">${#</span><span class="nv">AllNode</span><span class="p">[@]</span><span class="si">}</span>+3<span class="o">])</span>  &lt;<span class="o">(</span>xargs -n1<span class="o">&lt;&lt;&lt;</span><span class="si">${</span><span class="nv">AllNode</span><span class="p">[@]</span><span class="si">}</span> <span class="p">|</span> sort<span class="o">)</span> <span class="o">)</span> /home/k8s/k8s-manual-files/pki/openssl.cnf
sed -ri <span class="s1">&#39;$r &#39;</span>&lt;<span class="o">(</span> paste -d <span class="s1">&#39;&#39;</span> &lt;<span class="o">(</span>seq -f <span class="s1">&#39;IP.%g = &#39;</span> <span class="m">2</span> $<span class="o">[</span><span class="si">${#</span><span class="nv">MasterArray</span><span class="p">[@]</span><span class="si">}</span>+1<span class="o">])</span>  &lt;<span class="o">(</span>xargs -n1<span class="o">&lt;&lt;&lt;</span><span class="si">${</span><span class="nv">MasterArray</span><span class="p">[@]</span><span class="si">}</span> <span class="p">|</span> sort<span class="o">)</span> <span class="o">)</span> /home/k8s/k8s-manual-files/pki/openssl.cnf
cp /home/k8s/k8s-manual-files/pki/openssl.cnf /etc/kubernetes/pki/
<span class="nb">cd</span> /etc/kubernetes/pki
</code></pre></td></tr></table>
</div>
</div><h4 id="生成证书">生成证书</h4>
<ol>
<li>
<p>生成根证书</p>
<table>
<thead>
<tr>
<th>Path</th>
<th>Default CN</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>ca.crt/key</td>
<td>kubernetes-ca</td>
<td>Kubernetes general CA</td>
</tr>
<tr>
<td>etcd/ca.crt/key</td>
<td>etcd-ca</td>
<td>For all etcd-related functions</td>
</tr>
<tr>
<td>front-proxy-ca.crt/key</td>
<td>kubernetes-front-proxy-ca</td>
<td>For the front-end proxy</td>
</tr>
</tbody>
</table>
<p><strong>kubernetes-ca</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">openssl genrsa -out ca.key <span class="m">2048</span>
openssl req -x509 -new -nodes -key ca.key -config openssl.cnf -subj <span class="s2">&#34;/CN=kubernetes-ca&#34;</span> -extensions v3_ca -out ca.crt -days <span class="m">10000</span>
</code></pre></td></tr></table>
</div>
</div><p><strong>etcd-ca</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">openssl genrsa -out etcd/ca.key <span class="m">2048</span>
openssl req -x509 -new -nodes -key etcd/ca.key -config openssl.cnf -subj <span class="s2">&#34;/CN=etcd-ca&#34;</span> -extensions v3_ca -out etcd/ca.crt -days <span class="m">10000</span>
</code></pre></td></tr></table>
</div>
</div><p><strong>front-proxy-ca</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">openssl genrsa -out front-proxy-ca.key <span class="m">2048</span>
openssl req -x509 -new -nodes -key front-proxy-ca.key -config openssl.cnf -subj <span class="s2">&#34;/CN=kubernetes-ca&#34;</span> -extensions v3_ca -out front-proxy-ca.crt -days <span class="m">10000</span>
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>使用根证书签发证书</p>
<p>生成所有的证书信息</p>
<table>
<thead>
<tr>
<th>Default CN</th>
<th>Parent CA</th>
<th>O (in Subject)</th>
<th>kind</th>
</tr>
</thead>
<tbody>
<tr>
<td>kube-etcd</td>
<td>etcd-ca</td>
<td></td>
<td>server, client</td>
</tr>
<tr>
<td>kube-etcd-peer</td>
<td>etcd-ca</td>
<td></td>
<td>server, client</td>
</tr>
<tr>
<td>kube-etcd-healthcheck-client</td>
<td>etcd-ca</td>
<td></td>
<td>client</td>
</tr>
<tr>
<td>kube-apiserver-etcd-client</td>
<td>etcd-ca</td>
<td>system:masters</td>
<td>client</td>
</tr>
<tr>
<td>kube-apiserver</td>
<td>kubernetes-ca</td>
<td></td>
<td>server</td>
</tr>
<tr>
<td>kube-apiserver-kubelet-client</td>
<td>kubernetes-ca</td>
<td>system:masters</td>
<td>client</td>
</tr>
<tr>
<td>front-proxy-client</td>
<td>kubernetes-front-proxy-ca</td>
<td></td>
<td>client</td>
</tr>
</tbody>
</table>
<p>证书路径</p>
<table>
<thead>
<tr>
<th>Default CN</th>
<th>recommend key path</th>
<th>recommended cert path</th>
<th>command</th>
<th>key argument</th>
<th>cert argument</th>
</tr>
</thead>
<tbody>
<tr>
<td>etcd-ca</td>
<td></td>
<td>etcd/ca.crt</td>
<td>kube-apiserver</td>
<td></td>
<td>&ndash;etcd-cafile</td>
</tr>
<tr>
<td>etcd-client</td>
<td>apiserver-etcd-client.key</td>
<td>apiserver-etcd-client.crt</td>
<td>kube-apiserver</td>
<td>&ndash;etcd-keyfile</td>
<td>&ndash;etcd-cafile</td>
</tr>
<tr>
<td>kubernetes-ca</td>
<td></td>
<td>ca.crt</td>
<td>kube-apiserver</td>
<td></td>
<td>&ndash;client-ca-file</td>
</tr>
<tr>
<td>kube-apiserver</td>
<td>apiserver.key</td>
<td>apiserver.crt</td>
<td>kube-apiserver</td>
<td>&ndash;tls-private-key-file</td>
<td>&ndash;tls-cert-file</td>
</tr>
<tr>
<td>apiserver-kubelet-client</td>
<td></td>
<td>apiserver-kubelet-client.crt</td>
<td>kube-apiserver</td>
<td></td>
<td>&ndash;kubelet-client-certificate</td>
</tr>
<tr>
<td>front-proxy-ca</td>
<td></td>
<td>front-proxy-ca.crt</td>
<td>kube-apiserver</td>
<td></td>
<td>&ndash;requestheader-client-ca-file</td>
</tr>
<tr>
<td>front-proxy-client</td>
<td>front-proxy-client.key</td>
<td>front-proxy-client.crt</td>
<td>kube-apiserver</td>
<td>&ndash;proxy-client-key-file</td>
<td>&ndash;proxy-client-cert-file</td>
</tr>
<tr>
<td>etcd-ca</td>
<td></td>
<td>etcd/ca.crt</td>
<td>etcd</td>
<td></td>
<td>&ndash;trusted-ca-file, –peer-trusted-ca-file</td>
</tr>
<tr>
<td>kube-etcd</td>
<td>etcd/server.key</td>
<td>etcd/server.crt</td>
<td>etcd</td>
<td>&ndash;key-file</td>
<td>&ndash;cert-file</td>
</tr>
<tr>
<td>kube-etcd-peer</td>
<td>etcd/peer.key</td>
<td>etcd/peer.crt</td>
<td>etcd</td>
<td>&ndash;peer-key-file</td>
<td>&ndash;peer-cert-file</td>
</tr>
<tr>
<td>etcd-ca</td>
<td></td>
<td>etcd/ca.crt</td>
<td>etcdctl</td>
<td></td>
<td>-cacert</td>
</tr>
<tr>
<td>kube-etcd-healthcheck-client</td>
<td>etcd/healthcheck-client.key</td>
<td>etcd/healthcheck-client.crt</td>
<td>etcdctl</td>
<td>&ndash;key</td>
<td>&ndash;cert</td>
</tr>
</tbody>
</table>
<p>生成证书</p>
<p><strong>apiserver-etcd-client</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">openssl genrsa -out apiserver-etcd-client.key <span class="m">2048</span>
openssl req -new -key apiserver-etcd-client.key -subj <span class="s2">&#34;/CN=apiserver-etcd-client/O=system:masters&#34;</span> -out apiserver-etcd-client.csr
openssl x509 -in apiserver-etcd-client.csr -req -CA etcd/ca.crt -CAkey etcd/ca.key -CAcreateserial -extensions v3_req_etcd -extfile openssl.cnf -out apiserver-etcd-client.crt -days <span class="m">10000</span>
</code></pre></td></tr></table>
</div>
</div><p><strong>kube-etcd</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">openssl genrsa -out etcd/server.key <span class="m">2048</span>
openssl req -new -key etcd/server.key -subj <span class="s2">&#34;/CN=etcd-server&#34;</span> -out etcd/server.csr
openssl x509 -in etcd/server.csr -req -CA etcd/ca.crt -CAkey etcd/ca.key -CAcreateserial -extensions v3_req_etcd -extfile openssl.cnf -out etcd/server.crt -days <span class="m">10000</span>
</code></pre></td></tr></table>
</div>
</div><p><strong>kube-etcd-peer</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">openssl genrsa -out etcd/peer.key <span class="m">2048</span>
openssl req -new -key etcd/peer.key -subj <span class="s2">&#34;/CN=etcd-peer&#34;</span> -out etcd/peer.csr
openssl x509 -in etcd/peer.csr -req -CA etcd/ca.crt -CAkey etcd/ca.key -CAcreateserial -extensions v3_req_etcd -extfile openssl.cnf -out etcd/peer.crt -days <span class="m">10000</span>
</code></pre></td></tr></table>
</div>
</div><p><strong>kube-etcd-healthcheck-client</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">openssl genrsa -out etcd/healthcheck-client.key <span class="m">2048</span>
openssl req -new -key etcd/healthcheck-client.key -subj <span class="s2">&#34;/CN=etcd-client&#34;</span> -out etcd/healthcheck-client.csr
openssl x509 -in etcd/healthcheck-client.csr -req -CA etcd/ca.crt -CAkey etcd/ca.key -CAcreateserial -extensions v3_req_etcd -extfile openssl.cnf -out etcd/healthcheck-client.crt -days <span class="m">10000</span>
</code></pre></td></tr></table>
</div>
</div><p><strong>kube-apiserver</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">openssl genrsa -out apiserver.key <span class="m">2048</span>
openssl req -new -key apiserver.key -subj <span class="s2">&#34;/CN=kube-apiserver&#34;</span> -config openssl.cnf -out apiserver.csr
openssl x509 -req -in apiserver.csr -CA ca.crt -CAkey ca.key -CAcreateserial -days <span class="m">10000</span> -extensions v3_req_apiserver -extfile openssl.cnf -out apiserver.crt
</code></pre></td></tr></table>
</div>
</div><p><strong>apiserver-kubelet-client</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">openssl genrsa -out  apiserver-kubelet-client.key <span class="m">2048</span>
openssl req -new -key apiserver-kubelet-client.key -subj <span class="s2">&#34;/CN=apiserver-kubelet-client/O=system:masters&#34;</span> -out apiserver-kubelet-client.csr
openssl x509 -req -in apiserver-kubelet-client.csr -CA ca.crt -CAkey ca.key -CAcreateserial -days <span class="m">10000</span> -extensions v3_req_client -extfile openssl.cnf -out apiserver-kubelet-client.crt
</code></pre></td></tr></table>
</div>
</div><p><strong>front-proxy-client</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">openssl genrsa -out  front-proxy-client.key <span class="m">2048</span>
openssl req -new -key front-proxy-client.key -subj <span class="s2">&#34;/CN=front-proxy-client&#34;</span> -out front-proxy-client.csr
openssl x509 -req -in front-proxy-client.csr -CA front-proxy-ca.crt -CAkey front-proxy-ca.key -CAcreateserial -days <span class="m">10000</span> -extensions v3_req_client -extfile openssl.cnf -out front-proxy-client.crt
</code></pre></td></tr></table>
</div>
</div><p><strong>kube-scheduler</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">openssl genrsa -out  kube-scheduler.key <span class="m">2048</span>
openssl req -new -key kube-scheduler.key -subj <span class="s2">&#34;/CN=system:kube-scheduler&#34;</span> -out kube-scheduler.csr
openssl x509 -req -in kube-scheduler.csr -CA ca.crt -CAkey ca.key -CAcreateserial -days <span class="m">10000</span> -extensions v3_req_client -extfile openssl.cnf -out kube-scheduler.crt
</code></pre></td></tr></table>
</div>
</div><p><strong>sa.pub sa.key</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">openssl genrsa -out  sa.key <span class="m">2048</span>
openssl ecparam -name secp521r1 -genkey -noout -out sa.key
openssl ec -in sa.key -outform PEM -pubout -out sa.pub
openssl req -new -sha256 -key sa.key -subj <span class="s2">&#34;/CN=system:kube-controller-manager&#34;</span> -out sa.csr
openssl x509 -req -in sa.csr -CA ca.crt -CAkey ca.key -CAcreateserial -days <span class="m">10000</span> -extensions v3_req_client -extfile openssl.cnf -out sa.crt
</code></pre></td></tr></table>
</div>
</div><p><strong>admin</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">openssl genrsa -out  admin.key <span class="m">2048</span>
openssl req -new -key admin.key -subj <span class="s2">&#34;/CN=kubernetes-admin/O=system:masters&#34;</span> -out admin.csr
openssl x509 -req -in admin.csr -CA ca.crt -CAkey ca.key -CAcreateserial -days <span class="m">10000</span> -extensions v3_req_client -extfile openssl.cnf -out admin.crt
</code></pre></td></tr></table>
</div>
</div><p><strong>清理 csr srl</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">find . -name <span class="s2">&#34;*.csr&#34;</span> -o -name <span class="s2">&#34;*.srl&#34;</span><span class="p">|</span>xargs  rm -f
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>利用证书生成组件的 kubeconfig</p>
<table>
<thead>
<tr>
<th>filename</th>
<th>credential name</th>
<th>Default CN</th>
<th>O(In Subject)</th>
</tr>
</thead>
<tbody>
<tr>
<td>admin.kubeconfig</td>
<td>default-admin</td>
<td>kubernetes-admin</td>
<td>system:masters</td>
</tr>
<tr>
<td>controller-manager.kubeconfig</td>
<td>default-controller-manager</td>
<td>system:kube-controller-manager</td>
<td></td>
</tr>
<tr>
<td>scheduler.kubeconfig</td>
<td>default-manager</td>
<td>system:kube-scheduler</td>
<td></td>
</tr>
</tbody>
</table>
<p><strong>kube-controller-manager</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="nv">CLUSTER_NAME</span><span class="o">=</span><span class="s2">&#34;kubernetes&#34;</span>
<span class="nv">KUBE_USER</span><span class="o">=</span><span class="s2">&#34;system:kube-controller-manager&#34;</span>
<span class="nv">KUBE_CERT</span><span class="o">=</span><span class="s2">&#34;sa&#34;</span>
<span class="nv">KUBE_CONFIG</span><span class="o">=</span><span class="s2">&#34;controller-manager.kubeconfig&#34;</span>

<span class="c1"># 设置集群参数</span>
kubectl config set-cluster <span class="si">${</span><span class="nv">CLUSTER_NAME</span><span class="si">}</span> <span class="se">\
</span><span class="se"></span>  --certificate-authority<span class="o">=</span>/etc/kubernetes/pki/ca.crt <span class="se">\
</span><span class="se"></span>  --embed-certs<span class="o">=</span><span class="nb">true</span> <span class="se">\
</span><span class="se"></span>  --server<span class="o">=</span><span class="si">${</span><span class="nv">KUBE_APISERVER</span><span class="si">}</span> <span class="se">\
</span><span class="se"></span>  --kubeconfig<span class="o">=</span>/etc/kubernetes/<span class="si">${</span><span class="nv">KUBE_CONFIG</span><span class="si">}</span>

<span class="c1"># 设置客户端认证参数</span>
kubectl config set-credentials <span class="si">${</span><span class="nv">KUBE_USER</span><span class="si">}</span> <span class="se">\
</span><span class="se"></span>  --client-certificate<span class="o">=</span>/etc/kubernetes/pki/<span class="si">${</span><span class="nv">KUBE_CERT</span><span class="si">}</span>.crt <span class="se">\
</span><span class="se"></span>  --client-key<span class="o">=</span>/etc/kubernetes/pki/<span class="si">${</span><span class="nv">KUBE_CERT</span><span class="si">}</span>.key <span class="se">\
</span><span class="se"></span>  --embed-certs<span class="o">=</span><span class="nb">true</span> <span class="se">\
</span><span class="se"></span>  --kubeconfig<span class="o">=</span>/etc/kubernetes/<span class="si">${</span><span class="nv">KUBE_CONFIG</span><span class="si">}</span>

<span class="c1"># 设置上下文参数</span>
kubectl config set-context <span class="si">${</span><span class="nv">KUBE_USER</span><span class="si">}</span>@<span class="si">${</span><span class="nv">CLUSTER_NAME</span><span class="si">}</span> <span class="se">\
</span><span class="se"></span>  --cluster<span class="o">=</span><span class="si">${</span><span class="nv">CLUSTER_NAME</span><span class="si">}</span> <span class="se">\
</span><span class="se"></span>  --user<span class="o">=</span><span class="si">${</span><span class="nv">KUBE_USER</span><span class="si">}</span> <span class="se">\
</span><span class="se"></span>  --kubeconfig<span class="o">=</span>/etc/kubernetes/<span class="si">${</span><span class="nv">KUBE_CONFIG</span><span class="si">}</span>

<span class="c1"># 设置当前使用的上下文</span>
kubectl config use-context <span class="si">${</span><span class="nv">KUBE_USER</span><span class="si">}</span>@<span class="si">${</span><span class="nv">CLUSTER_NAME</span><span class="si">}</span> --kubeconfig<span class="o">=</span>/etc/kubernetes/<span class="si">${</span><span class="nv">KUBE_CONFIG</span><span class="si">}</span>

<span class="c1"># 查看生成的配置文件</span>
kubectl config view --kubeconfig<span class="o">=</span>/etc/kubernetes/<span class="si">${</span><span class="nv">KUBE_CONFIG</span><span class="si">}</span>
</code></pre></td></tr></table>
</div>
</div><p><strong>kube-scheduler</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="nv">CLUSTER_NAME</span><span class="o">=</span><span class="s2">&#34;kubernetes&#34;</span>
<span class="nv">KUBE_USER</span><span class="o">=</span><span class="s2">&#34;system:kube-scheduler&#34;</span>
<span class="nv">KUBE_CERT</span><span class="o">=</span><span class="s2">&#34;kube-scheduler&#34;</span>
<span class="nv">KUBE_CONFIG</span><span class="o">=</span><span class="s2">&#34;scheduler.kubeconfig&#34;</span>

<span class="c1"># 设置集群参数</span>
kubectl config set-cluster <span class="si">${</span><span class="nv">CLUSTER_NAME</span><span class="si">}</span> <span class="se">\
</span><span class="se"></span>  --certificate-authority<span class="o">=</span>/etc/kubernetes/pki/ca.crt <span class="se">\
</span><span class="se"></span>  --embed-certs<span class="o">=</span><span class="nb">true</span> <span class="se">\
</span><span class="se"></span>  --server<span class="o">=</span><span class="si">${</span><span class="nv">KUBE_APISERVER</span><span class="si">}</span> <span class="se">\
</span><span class="se"></span>  --kubeconfig<span class="o">=</span>/etc/kubernetes/<span class="si">${</span><span class="nv">KUBE_CONFIG</span><span class="si">}</span>

<span class="c1"># 设置客户端认证参数</span>
kubectl config set-credentials <span class="si">${</span><span class="nv">KUBE_USER</span><span class="si">}</span> <span class="se">\
</span><span class="se"></span>  --client-certificate<span class="o">=</span>/etc/kubernetes/pki/<span class="si">${</span><span class="nv">KUBE_CERT</span><span class="si">}</span>.crt <span class="se">\
</span><span class="se"></span>  --client-key<span class="o">=</span>/etc/kubernetes/pki/<span class="si">${</span><span class="nv">KUBE_CERT</span><span class="si">}</span>.key <span class="se">\
</span><span class="se"></span>  --embed-certs<span class="o">=</span><span class="nb">true</span> <span class="se">\
</span><span class="se"></span>  --kubeconfig<span class="o">=</span>/etc/kubernetes/<span class="si">${</span><span class="nv">KUBE_CONFIG</span><span class="si">}</span>

<span class="c1"># 设置上下文参数</span>
kubectl config set-context <span class="si">${</span><span class="nv">KUBE_USER</span><span class="si">}</span>@<span class="si">${</span><span class="nv">CLUSTER_NAME</span><span class="si">}</span> <span class="se">\
</span><span class="se"></span>  --cluster<span class="o">=</span><span class="si">${</span><span class="nv">CLUSTER_NAME</span><span class="si">}</span> <span class="se">\
</span><span class="se"></span>  --user<span class="o">=</span><span class="si">${</span><span class="nv">KUBE_USER</span><span class="si">}</span> <span class="se">\
</span><span class="se"></span>  --kubeconfig<span class="o">=</span>/etc/kubernetes/<span class="si">${</span><span class="nv">KUBE_CONFIG</span><span class="si">}</span>

<span class="c1"># 设置当前使用的上下文</span>
kubectl config use-context <span class="si">${</span><span class="nv">KUBE_USER</span><span class="si">}</span>@<span class="si">${</span><span class="nv">CLUSTER_NAME</span><span class="si">}</span> --kubeconfig<span class="o">=</span>/etc/kubernetes/<span class="si">${</span><span class="nv">KUBE_CONFIG</span><span class="si">}</span>

<span class="c1"># 查看生成的配置文件</span>
kubectl config view --kubeconfig<span class="o">=</span>/etc/kubernetes/<span class="si">${</span><span class="nv">KUBE_CONFIG</span><span class="si">}</span>
</code></pre></td></tr></table>
</div>
</div><p><strong>admin(kubectl)</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="nv">CLUSTER_NAME</span><span class="o">=</span><span class="s2">&#34;kubernetes&#34;</span>
<span class="nv">KUBE_USER</span><span class="o">=</span><span class="s2">&#34;kubernetes-admin&#34;</span>
<span class="nv">KUBE_CERT</span><span class="o">=</span><span class="s2">&#34;admin&#34;</span>
<span class="nv">KUBE_CONFIG</span><span class="o">=</span><span class="s2">&#34;admin.kubeconfig&#34;</span>

<span class="c1"># 设置集群参数</span>
kubectl config set-cluster <span class="si">${</span><span class="nv">CLUSTER_NAME</span><span class="si">}</span> <span class="se">\
</span><span class="se"></span>  --certificate-authority<span class="o">=</span>/etc/kubernetes/pki/ca.crt <span class="se">\
</span><span class="se"></span>  --embed-certs<span class="o">=</span><span class="nb">true</span> <span class="se">\
</span><span class="se"></span>  --server<span class="o">=</span><span class="si">${</span><span class="nv">KUBE_APISERVER</span><span class="si">}</span> <span class="se">\
</span><span class="se"></span>  --kubeconfig<span class="o">=</span>/etc/kubernetes/<span class="si">${</span><span class="nv">KUBE_CONFIG</span><span class="si">}</span>

<span class="c1"># 设置客户端认证参数</span>
kubectl config set-credentials <span class="si">${</span><span class="nv">KUBE_USER</span><span class="si">}</span> <span class="se">\
</span><span class="se"></span>  --client-certificate<span class="o">=</span>/etc/kubernetes/pki/<span class="si">${</span><span class="nv">KUBE_CERT</span><span class="si">}</span>.crt <span class="se">\
</span><span class="se"></span>  --client-key<span class="o">=</span>/etc/kubernetes/pki/<span class="si">${</span><span class="nv">KUBE_CERT</span><span class="si">}</span>.key <span class="se">\
</span><span class="se"></span>  --embed-certs<span class="o">=</span><span class="nb">true</span> <span class="se">\
</span><span class="se"></span>  --kubeconfig<span class="o">=</span>/etc/kubernetes/<span class="si">${</span><span class="nv">KUBE_CONFIG</span><span class="si">}</span>

<span class="c1"># 设置上下文参数</span>
kubectl config set-context <span class="si">${</span><span class="nv">KUBE_USER</span><span class="si">}</span>@<span class="si">${</span><span class="nv">CLUSTER_NAME</span><span class="si">}</span> <span class="se">\
</span><span class="se"></span>  --cluster<span class="o">=</span><span class="si">${</span><span class="nv">CLUSTER_NAME</span><span class="si">}</span> <span class="se">\
</span><span class="se"></span>  --user<span class="o">=</span><span class="si">${</span><span class="nv">KUBE_USER</span><span class="si">}</span> <span class="se">\
</span><span class="se"></span>  --kubeconfig<span class="o">=</span>/etc/kubernetes/<span class="si">${</span><span class="nv">KUBE_CONFIG</span><span class="si">}</span>

<span class="c1"># 设置当前使用的上下文</span>
kubectl config use-context <span class="si">${</span><span class="nv">KUBE_USER</span><span class="si">}</span>@<span class="si">${</span><span class="nv">CLUSTER_NAME</span><span class="si">}</span> --kubeconfig<span class="o">=</span>/etc/kubernetes/<span class="si">${</span><span class="nv">KUBE_CONFIG</span><span class="si">}</span>

<span class="c1"># 查看生成的配置文件</span>
kubectl config view --kubeconfig<span class="o">=</span>/etc/kubernetes/<span class="si">${</span><span class="nv">KUBE_CONFIG</span><span class="si">}</span>
</code></pre></td></tr></table>
</div>
</div></li>
</ol>
<h4 id="分发证书">分发证书</h4>
<p>分发到 kubeconfig 及证书其他 master 节点</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="k">for</span> NODE in <span class="s2">&#34;</span><span class="si">${</span><span class="p">!otherMaster[@]</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">;</span> <span class="k">do</span>
    <span class="nb">echo</span> <span class="s2">&#34;--- </span><span class="nv">$NODE</span><span class="s2"> </span><span class="si">${</span><span class="nv">otherMaster</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span><span class="s2"> ---&#34;</span>
    scp -r /etc/kubernetes <span class="si">${</span><span class="nv">otherMaster</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span>:/etc
<span class="k">done</span>
</code></pre></td></tr></table>
</div>
</div><h3 id="安装-etcd">安装 etcd</h3>
<p>etcd 是用来保存集群所有状态的 Key/Value 存储系统,所有 Kubernetes 组件会通过 API Server 来跟 Etcd 进行沟通从而保存或读取资源状态。</p>
<p>在 k8s 集群中的 master 节点上部署 etcd 集群。</p>
<ol>
<li>
<p>在 k8s-m1 上下载 etcd 的发布包，并将二进制文件解压到 /usr/local/bin 目录下</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">wget https://github.com/etcd-io/etcd/releases/download/<span class="si">${</span><span class="nv">ETCD_version</span><span class="si">}</span>/etcd-<span class="si">${</span><span class="nv">ETCD_version</span><span class="si">}</span>-linux-amd64.tar.gz
tar -zxvf etcd-<span class="si">${</span><span class="nv">ETCD_version</span><span class="si">}</span>-linux-amd64.tar.gz --strip-components<span class="o">=</span><span class="m">1</span> -C /usr/local/bin etcd-<span class="si">${</span><span class="nv">ETCD_version</span><span class="si">}</span>-linux-amd64/etcd<span class="o">{</span>,ctl<span class="o">}</span>
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>在 k8s-m1 上将 etcd 的二进制文件分发到其他 master 上</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="k">for</span> NODE in <span class="s2">&#34;</span><span class="si">${</span><span class="p">!otherMaster[@]</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">;</span> <span class="k">do</span>
    <span class="nb">echo</span> <span class="s2">&#34;--- </span><span class="nv">$NODE</span><span class="s2"> </span><span class="si">${</span><span class="nv">otherMaster</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span><span class="s2"> ---&#34;</span>
    scp /usr/local/bin/etcd* <span class="si">${</span><span class="nv">otherMaster</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span>:/usr/local/bin/
<span class="k">done</span>
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>在 k8s-m1 上修改 etcd 配置文件,注入 etcd 集群 ip 到 yml 文件中</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="nb">cd</span> /home/k8s/k8s-manual-files/master/
<span class="nv">etcd_servers</span><span class="o">=</span><span class="k">$(</span> xargs -n1<span class="o">&lt;&lt;&lt;</span><span class="si">${</span><span class="nv">MasterArray</span><span class="p">[@]</span><span class="si">}</span> <span class="p">|</span> sort <span class="p">|</span> sed <span class="s1">&#39;s#^#https://#;s#$#:2379#;$s#\n##&#39;</span> <span class="p">|</span> paste -d, -s - <span class="k">)</span>
<span class="nv">etcd_initial_cluster</span><span class="o">=</span><span class="k">$(</span> <span class="k">for</span> i in <span class="si">${</span><span class="p">!MasterArray[@]</span><span class="si">}</span><span class="p">;</span><span class="k">do</span>  <span class="nb">echo</span> <span class="nv">$i</span><span class="o">=</span>https://<span class="si">${</span><span class="nv">MasterArray</span><span class="p">[</span><span class="nv">$i</span><span class="p">]</span><span class="si">}</span>:2380<span class="p">;</span> <span class="k">done</span> <span class="p">|</span> sort <span class="p">|</span> paste -d, -s - <span class="k">)</span>
sed -ri <span class="s2">&#34;/initial-cluster:/s#&#39;.+&#39;#&#39;</span><span class="si">${</span><span class="nv">etcd_initial_cluster</span><span class="si">}</span><span class="s2">&#39;#&#34;</span> etc/etcd/config.yml
</code></pre></td></tr></table>
</div>
</div><blockquote>
<p>参考: <a href="https://github.com/etcd-io/etcd/blob/master/etcd.conf.yml.sample">https://github.com/etcd-io/etcd/blob/master/etcd.conf.yml.sample</a></p>
</blockquote>
</li>
<li>
<p>将相关文件分发到其他 master 上</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="k">for</span> NODE in <span class="s2">&#34;</span><span class="si">${</span><span class="p">!MasterArray[@]</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">;</span> <span class="k">do</span>
    <span class="nb">echo</span> <span class="s2">&#34;--- </span><span class="nv">$NODE</span><span class="s2"> </span><span class="si">${</span><span class="nv">MasterArray</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span><span class="s2"> ---&#34;</span>
    ssh <span class="si">${</span><span class="nv">MasterArray</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span> <span class="s2">&#34;mkdir -p /etc/etcd /var/lib/etcd&#34;</span>
    scp systemd/etcd.service <span class="si">${</span><span class="nv">MasterArray</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span>:/usr/lib/systemd/system/etcd.service
    scp etc/etcd/config.yml <span class="si">${</span><span class="nv">MasterArray</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span>:/etc/etcd/etcd.config.yml
    ssh <span class="si">${</span><span class="nv">MasterArray</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span> <span class="s2">&#34;sed -i &#34;</span>s/<span class="o">{</span>HOSTNAME<span class="o">}</span>/<span class="nv">$NODE</span>/g<span class="s2">&#34; /etc/etcd/etcd.config.yml&#34;</span>
    ssh <span class="si">${</span><span class="nv">MasterArray</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span> <span class="s2">&#34;sed -i &#34;</span>s/<span class="o">{</span>PUBLIC_IP<span class="o">}</span>/<span class="si">${</span><span class="nv">MasterArray</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span>/g<span class="s2">&#34; /etc/etcd/etcd.config.yml&#34;</span>
    ssh <span class="si">${</span><span class="nv">MasterArray</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span> <span class="s1">&#39;systemctl daemon-reload&#39;</span>
<span class="k">done</span>
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>在 k8s-m1 上启动所有 master 节点的 etcd</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="k">for</span> NODE in <span class="s2">&#34;</span><span class="si">${</span><span class="p">!MasterArray[@]</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">;</span> <span class="k">do</span>
    <span class="nb">echo</span> <span class="s2">&#34;--- </span><span class="nv">$NODE</span><span class="s2"> </span><span class="si">${</span><span class="nv">MasterArray</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span><span class="s2"> ---&#34;</span>
    ssh <span class="si">${</span><span class="nv">MasterArray</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span> <span class="s1">&#39;systemctl enable --now etcd&#39;</span> <span class="p">&amp;</span>
<span class="k">done</span>
<span class="nb">wait</span>
</code></pre></td></tr></table>
</div>
</div><p>输出到终端的时候多按几下回车直到等光标回到终端状态</p>
<blockquote>
<p>etcd 进程首次启动时会等待其它节点的 etcd 加入集群，命令 systemctl start etcd 会卡住一段时间，为正常现象。</p>
</blockquote>
</li>
<li>
<p>k8s-m1上执行下面命令验证 ETCD 集群状态</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">etcdctl <span class="se">\
</span><span class="se"></span>  --cert-file /etc/kubernetes/pki/etcd/healthcheck-client.crt <span class="se">\
</span><span class="se"></span>  --key-file /etc/kubernetes/pki/etcd/healthcheck-client.key <span class="se">\
</span><span class="se"></span>  --ca-file /etc/kubernetes/pki/etcd/ca.crt <span class="se">\
</span><span class="se"></span>  --endpoints <span class="nv">$etcd_servers</span> cluster-health
</code></pre></td></tr></table>
</div>
</div><p>使用 etcd v3版 API 查看当前 etcd 中存在的 key</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="nv">ETCDCTL_API</span><span class="o">=</span><span class="m">3</span> <span class="se">\
</span><span class="se"></span>etcdctl <span class="se">\
</span><span class="se"></span>  --cert /etc/kubernetes/pki/etcd/healthcheck-client.crt <span class="se">\
</span><span class="se"></span>  --key /etc/kubernetes/pki/etcd/healthcheck-client.key <span class="se">\
</span><span class="se"></span>  --cacert /etc/kubernetes/pki/etcd/ca.crt <span class="se">\
</span><span class="se"></span>  --endpoints <span class="nv">$etcd_servers</span> get / --prefix --keys-only
</code></pre></td></tr></table>
</div>
</div></li>
</ol>
<h3 id="kubernetes-master">Kubernetes Master</h3>
<p>本部分将说明如何建立与设定 Kubernetes Master 角色，过程中会部署以下元件：</p>
<p><strong>kubelet:</strong><br>
负责管理容器的生命周期，定期从 API Server 获取节点上的预期状态（如网络、存储等等配置）资源，并让对应的容器插件（CRI、CNI 等）来达成这个状态。任何 Kubernetes 节点（node）都会拥有这个元件。<br>
关闭只读端口，在安全端口 10250 接收 https 请求，对请求进行认证和授权，拒绝匿名访问和非授权访问。<br>
使用 kubeconfig 访问 apiserver 的安全端口</p>
<p><strong>kube-apiserver:</strong><br>
以 REST APIs 提供 Kubernetes 资源的 CRUD，如授权、认证、存取控制与 API 注册等机制。<br>
关闭非安全端口，在安全端口 6443 接收 https 请求<br>
严格的认证和授权策略 （x509、token、RBAC）<br>
开启 bootstrap token 认证，支持 kubelet TLS bootstrapping<br>
使用 https 访问 kubelet、etcd，加密通信</p>
<p><strong>kube-controller-manager:</strong><br>
通过核心控制循环（Core Control Loop）监听 Kubernetes API 的资源来维护集群的状态，这些资源会被不同的控制器所管理，如 Replication Controller、Namespace Controller 等等。而这些控制器会处理着自动扩展、滚动更新等等功能。<br>
关闭非安全端口，在安全端口 10252 接收 https 请求。<br>
使用 kubeconfig 访问 apiserver 的安全端口。</p>
<p><strong>kube-scheduler:</strong><br>
负责将一个(或多个)容器依据调度策略分配到对应节点上让容器引擎(如 Docker)执行。而调度受到 QoS 要求、软硬性约束、亲和性(Affinity)等等因素影响。</p>
<p><strong>HAProxy:</strong>
提供多个 API Server 的负载均衡(Load Balance),确保haproxy的端口负载到所有的apiserver的6443端口</p>
<p><strong>Keepalived:</strong>
提供虚拟IP位址(VIP),来让vip落在可用的master主机上供所有组件都能访问到可用的master,结合haproxy能访问到master上的apiserver的6443端口</p>
<p><strong>kube-nginx</strong>
使用 nginx 4 层透明代理功能实现 K8S 节点（ master 节点和 worker 节点）高可用访问 kube-apiserver 的策略。（等于 Haproxy + Keepalived）</p>
<p><strong>kube-nginx 方案和 haproxy+keepalived 方案选一个即可。</strong><br>
kube-nginx 方案需要在每一台主机上部署，而 haproxy+keepalived 方案只需要部署在 master 节点的主机上。<br>
kube-nginx 方案不需要额外的 IP ，而 haproxy+keepalived 方案需要额外的一个当前局域网未使用的 IP 做为 VIP。<br>
若使用 haproxy + keepalived 方案，则 master 节点的主机的网卡名需要相同。</p>
<p>本文档使用 kube-nginx 方案，haproxy + keepalived 方案只做介绍。</p>
<h4 id="安装-haproxy--keepalived">安装 haproxy + keepalived</h4>
<p>在 k8s-m1 上执行如下命令，在 master 节点上安装 haproxy 和 keepalived</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="k">for</span> NODE in <span class="s2">&#34;</span><span class="si">${</span><span class="p">!MasterArray[@]</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">;</span> <span class="k">do</span>
    <span class="nb">echo</span> <span class="s2">&#34;--- </span><span class="nv">$NODE</span><span class="s2"> </span><span class="si">${</span><span class="nv">MasterArray</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span><span class="s2"> ---&#34;</span>
    ssh <span class="si">${</span><span class="nv">MasterArray</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span> <span class="s1">&#39;yum install haproxy keepalived -y&#39;</span> <span class="p">&amp;</span>
<span class="k">done</span>
<span class="nb">wait</span>
</code></pre></td></tr></table>
</div>
</div><blockquote>
<p>如果没输出的话，多按几次</p>
</blockquote>
<p>在 k8s-m1 上把相关配置文件修改和分发</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="nb">cd</span> /home/k8s/k8s-manual-files/master/etc

<span class="c1"># 修改haproxy.cfg配置文件</span>
sed -i <span class="s1">&#39;$r &#39;</span>&lt;<span class="o">(</span>paste &lt;<span class="o">(</span> seq -f<span class="s1">&#39;  server k8s-api-%g&#39;</span>  <span class="si">${#</span><span class="nv">MasterArray</span><span class="p">[@]</span><span class="si">}</span> <span class="o">)</span> &lt;<span class="o">(</span> xargs -n1<span class="o">&lt;&lt;&lt;</span><span class="si">${</span><span class="nv">MasterArray</span><span class="p">[@]</span><span class="si">}</span> <span class="p">|</span> sort <span class="p">|</span> sed <span class="s1">&#39;s#$#:6443  check#&#39;</span><span class="o">))</span> haproxy/haproxy.cfg

<span class="c1"># 修改keepalived(网卡和VIP写进去,使用下面命令)</span>

sed -ri <span class="s2">&#34;s#\{\{ VIP \}\}#</span><span class="si">${</span><span class="nv">VIP</span><span class="si">}</span><span class="s2">#&#34;</span> keepalived/*
sed -ri <span class="s2">&#34;s#\{\{ interface \}\}#</span><span class="si">${</span><span class="nv">interface</span><span class="si">}</span><span class="s2">#&#34;</span> keepalived/keepalived.conf
sed -i <span class="s1">&#39;/unicast_peer/r &#39;</span>&lt;<span class="o">(</span>xargs -n1<span class="o">&lt;&lt;&lt;</span><span class="si">${</span><span class="nv">MasterArray</span><span class="p">[@]</span><span class="si">}</span> <span class="p">|</span> sort <span class="p">|</span> sed <span class="s1">&#39;s#^#\t#&#39;</span><span class="o">)</span> keepalived/keepalived.conf

<span class="c1"># 分发文件</span>
<span class="k">for</span> NODE in <span class="s2">&#34;</span><span class="si">${</span><span class="p">!MasterArray[@]</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">;</span> <span class="k">do</span>
    <span class="nb">echo</span> <span class="s2">&#34;--- </span><span class="nv">$NODE</span><span class="s2"> </span><span class="si">${</span><span class="nv">MasterArray</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span><span class="s2"> ---&#34;</span>
    scp -r haproxy/ <span class="si">${</span><span class="nv">MasterArray</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span>:/etc
    scp -r keepalived/ <span class="si">${</span><span class="nv">MasterArray</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span>:/etc
    ssh <span class="si">${</span><span class="nv">MasterArray</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span> <span class="s1">&#39;systemctl enable --now haproxy keepalived&#39;</span>
<span class="k">done</span>
</code></pre></td></tr></table>
</div>
</div><p>等待四五秒后，ping 下 vip 看看是否能通</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">ping <span class="nv">$VIP</span>
</code></pre></td></tr></table>
</div>
</div><p>如果 vip ping 不通，就是 keepalived 未启动或启动失败，在每个 master 节点上 restart 下 keepalived 或者确认下配置文件 /etc/keepalived/keepalived.conf 里网卡名和 ip 是否注入成功。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="k">for</span> NODE in <span class="s2">&#34;</span><span class="si">${</span><span class="p">!MasterArray[@]</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">;</span> <span class="k">do</span>
    <span class="nb">echo</span> <span class="s2">&#34;--- </span><span class="nv">$NODE</span><span class="s2"> </span><span class="si">${</span><span class="nv">MasterArray</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span><span class="s2"> ---&#34;</span>
    ssh <span class="si">${</span><span class="nv">MasterArray</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span>  <span class="s1">&#39;systemctl restart haproxy keepalived&#39;</span>
<span class="k">done</span>
</code></pre></td></tr></table>
</div>
</div><h4 id="在所有节点安装-kube-nginx">在所有节点安装 kube-nginx</h4>
<p><strong>基于 nginx 代理的 kube-apiserver 高可用方案：</strong></p>
<ul>
<li>控制节点（master 节点）的 kube-controller-manager、kube-scheduler 是多实例部署，所以只要有一个实例正常，就可以保证高可用；</li>
<li>集群内的 Pod 使用 K8S 服务域名 kubernetes 访问 kube-apiserver， kube-dns 会自动解析出多个 kube-apiserver 节点的 IP，所以也是高可用的；</li>
<li>在每个节点起一个 nginx 进程，后端对接多个 apiserver 实例，nginx 对它们做健康检查和负载均衡；</li>
<li>kubelet、kube-proxy、controller-manager、scheduler 通过本地的 nginx（监听 127.0.0.1）访问 kube-apiserver，从而实现 kube-apiserver 的高可用；</li>
</ul>
<p><strong>下载和编译 nginx ：</strong></p>
<ol>
<li>
<p>下载源码</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="nb">cd</span> /home/k8s
wget http://nginx.org/download/nginx-1.15.3.tar.gz
tar -xzvf nginx-1.15.3.tar.gz
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>配置编译参数</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="nb">cd</span> /home/k8s/nginx-1.15.3
mkdir nginx-prefix
./configure --with-stream --without-http --prefix<span class="o">=</span><span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>/nginx-prefix --without-http_uwsgi_module --without-http_scgi_module --without-http_fastcgi_module
</code></pre></td></tr></table>
</div>
</div><blockquote>
<p><code>--with-stream</code>：开启 4 层透明转发(TCP Proxy)功能；<br>
<code>--without-xxx</code>：关闭所有其他功能，这样生成的动态链接二进制程序依赖最小；</p>
</blockquote>
</li>
<li>
<p>编译和安装</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="nb">cd</span> /home/k8s/nginx-1.15.3
make <span class="o">&amp;&amp;</span> make install
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>验证编译的 nginx</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="nb">cd</span> /home/k8s/nginx-1.15.3
./nginx-prefix/sbin/nginx -v
</code></pre></td></tr></table>
</div>
</div><p>输出：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">nginx version: nginx/1.15.3
</code></pre></td></tr></table>
</div>
</div><p>查看 nginx 动态链接的库：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">ldd ./nginx-prefix/sbin/nginx
</code></pre></td></tr></table>
</div>
</div><p>输出：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">linux-vdso.so.1 <span class="o">=</span>&gt;  <span class="o">(</span>0x00007ffc945e7000<span class="o">)</span>
libdl.so.2 <span class="o">=</span>&gt; /lib64/libdl.so.2 <span class="o">(</span>0x00007f4385072000<span class="o">)</span>
libpthread.so.0 <span class="o">=</span>&gt; /lib64/libpthread.so.0 <span class="o">(</span>0x00007f4384e56000<span class="o">)</span>
libc.so.6 <span class="o">=</span>&gt; /lib64/libc.so.6 <span class="o">(</span>0x00007f4384a89000<span class="o">)</span>
/lib64/ld-linux-x86-64.so.2 <span class="o">(</span>0x00007f4385276000<span class="o">)</span>
</code></pre></td></tr></table>
</div>
</div><blockquote>
<p>由于只开启了 4 层透明转发功能，所以除了依赖 libc 等操作系统核心 lib 库外，没有对其它 lib 的依赖(如 libz、libssl 等)，这样可以方便部署到各版本操作系统中；</p>
</blockquote>
</li>
</ol>
<p><strong>安装和部署 nginx ：</strong></p>
<ol>
<li>
<p>创建目录结构并拷贝二进制程序：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="nb">cd</span> /home/k8s
<span class="nb">source</span> env.sh
<span class="k">for</span> NODE in <span class="s2">&#34;</span><span class="si">${</span><span class="p">!AllNode[@]</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">;</span> <span class="k">do</span>
    <span class="nb">echo</span> <span class="s2">&#34;--- </span><span class="nv">$NODE</span><span class="s2"> </span><span class="si">${</span><span class="nv">AllNode</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span><span class="s2"> ---&#34;</span>
    mkdir -p /usr/local/kube-nginx/<span class="o">{</span>conf,logs,sbin<span class="o">}</span>
    scp /home/k8s/nginx-1.15.3/nginx-prefix/sbin/nginx  <span class="si">${</span><span class="nv">AllNode</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span>:/usr/local/kube-nginx/sbin/kube-nginx
    ssh <span class="si">${</span><span class="nv">AllNode</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span> <span class="s2">&#34;chmod a+x /opt/k8s/kube-nginx/sbin/*&#34;</span>
<span class="k">done</span>
</code></pre></td></tr></table>
</div>
</div><blockquote>
<p>重命名二进制文件为 kube-nginx；</p>
</blockquote>
</li>
<li>
<p>配置 nginx，开启 4 层透明转发功能：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="nb">cd</span> /home/k8s
cat &gt; kube-nginx.conf <span class="s">&lt;&lt; \EOF
</span><span class="s">worker_processes 1;
</span><span class="s">
</span><span class="s">events {
</span><span class="s">    worker_connections  1024;
</span><span class="s">}
</span><span class="s">
</span><span class="s">stream {
</span><span class="s">    upstream backend {
</span><span class="s">        hash $remote_addr consistent;
</span><span class="s">        server 192.168.15.5:6443        max_fails=3 fail_timeout=30s;
</span><span class="s">        server 192.168.15.6:6443        max_fails=3 fail_timeout=30s;
</span><span class="s">        server 192.168.15.7:6443        max_fails=3 fail_timeout=30s;
</span><span class="s">    }
</span><span class="s">
</span><span class="s">    server {
</span><span class="s">        listen 127.0.0.1:8443;
</span><span class="s">        proxy_connect_timeout 1s;
</span><span class="s">        proxy_pass backend;
</span><span class="s">    }
</span><span class="s">}
</span><span class="s">EOF</span>
</code></pre></td></tr></table>
</div>
</div><blockquote>
<p>需要根据集群 kube-apiserver 的实际情况，替换 backend 中 server 列表；</p>
</blockquote>
</li>
<li>
<p>分发配置文件</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="nb">cd</span> /home/k8s
<span class="nb">source</span> env.sh
<span class="k">for</span> NODE in <span class="s2">&#34;</span><span class="si">${</span><span class="p">!AllNode[@]</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">;</span> <span class="k">do</span>
    <span class="nb">echo</span> <span class="s2">&#34;--- </span><span class="nv">$NODE</span><span class="s2"> </span><span class="si">${</span><span class="nv">AllNode</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span><span class="s2"> ---&#34;</span>
    scp kube-nginx.conf  <span class="si">${</span><span class="nv">AllNode</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span>:/usr/local/kube-nginx/conf/kube-nginx.conf
<span class="k">done</span>
</code></pre></td></tr></table>
</div>
</div></li>
</ol>
<p><strong>配置 systemd unit 文件，启动服务：</strong></p>
<p>配置 kube-nginx systemd unit 文件:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="nb">cd</span> /home/k8s
cat &gt; kube-nginx.service <span class="s">&lt;&lt;EOF
</span><span class="s">[Unit]
</span><span class="s">Description=kube-apiserver nginx proxy
</span><span class="s">After=network.target
</span><span class="s">After=network-online.target
</span><span class="s">Wants=network-online.target
</span><span class="s">
</span><span class="s">[Service]
</span><span class="s">Type=forking
</span><span class="s">ExecStartPre=/usr/local/kube-nginx/sbin/kube-nginx -c /usr/local/kube-nginx/conf/kube-nginx.conf -p /usr/local/kube-nginx -t
</span><span class="s">ExecStart=/usr/local/kube-nginx/sbin/kube-nginx -c /usr/local/kube-nginx/conf/kube-nginx.conf -p /usr/local/kube-nginx
</span><span class="s">ExecReload=/usr/local/kube-nginx/sbin/kube-nginx -c /usr/local/kube-nginx/conf/kube-nginx.conf -p /usr/local/kube-nginx -s reload
</span><span class="s">PrivateTmp=true
</span><span class="s">Restart=always
</span><span class="s">RestartSec=5
</span><span class="s">StartLimitInterval=0
</span><span class="s">LimitNOFILE=65536
</span><span class="s">
</span><span class="s">[Install]
</span><span class="s">WantedBy=multi-user.target
</span><span class="s">EOF</span>
</code></pre></td></tr></table>
</div>
</div><p>分发 systemd unit 文件：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="nb">cd</span> /home/k8s
<span class="nb">source</span> env.sh
<span class="k">for</span> NODE in <span class="s2">&#34;</span><span class="si">${</span><span class="p">!AllNode[@]</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">;</span> <span class="k">do</span>
    <span class="nb">echo</span> <span class="s2">&#34;--- </span><span class="nv">$NODE</span><span class="s2"> </span><span class="si">${</span><span class="nv">AllNode</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span><span class="s2"> ---&#34;</span>
    scp kube-nginx.service  <span class="si">${</span><span class="nv">AllNode</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span>:/usr/lib/systemd/system/
<span class="k">done</span>
</code></pre></td></tr></table>
</div>
</div><p>启动 kube-nginx 服务：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="nb">cd</span> /home/k8s
<span class="nb">source</span> env.sh
<span class="k">for</span> NODE in <span class="s2">&#34;</span><span class="si">${</span><span class="p">!AllNode[@]</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">;</span> <span class="k">do</span>
    <span class="nb">echo</span> <span class="s2">&#34;--- </span><span class="nv">$NODE</span><span class="s2"> </span><span class="si">${</span><span class="nv">AllNode</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span><span class="s2"> ---&#34;</span>
    ssh <span class="si">${</span><span class="nv">AllNode</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span> <span class="s2">&#34;systemctl daemon-reload &amp;&amp; systemctl enable kube-nginx &amp;&amp; systemctl restart kube-nginx&#34;</span>
<span class="k">done</span>
</code></pre></td></tr></table>
</div>
</div><p><strong>检查 kube-nginx 服务运行状态：</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="nb">cd</span> /home/k8s
<span class="nb">source</span> env.sh
<span class="k">for</span> NODE in <span class="s2">&#34;</span><span class="si">${</span><span class="p">!AllNode[@]</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">;</span> <span class="k">do</span>
    <span class="nb">echo</span> <span class="s2">&#34;--- </span><span class="nv">$NODE</span><span class="s2"> </span><span class="si">${</span><span class="nv">AllNode</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span><span class="s2"> ---&#34;</span>
    ssh <span class="si">${</span><span class="nv">AllNode</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span> <span class="s2">&#34;systemctl status kube-nginx |grep &#39;Active:&#39;&#34;</span>
<span class="k">done</span>
</code></pre></td></tr></table>
</div>
</div><p>确保状态为 active (running)，否则查看日志，确认原因：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">journalctl -u kube-nginx
</code></pre></td></tr></table>
</div>
</div><h4 id="master-组件">Master 组件</h4>
<ol>
<li>
<p>在k8s-1节点把相关配置文件修改后再分发</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="nb">cd</span> /home/k8s/k8s-manual-files/master/
<span class="nv">etcd_servers</span><span class="o">=</span><span class="k">$(</span> xargs -n1<span class="o">&lt;&lt;&lt;</span><span class="si">${</span><span class="nv">MasterArray</span><span class="p">[@]</span><span class="si">}</span> <span class="p">|</span> sort <span class="p">|</span> sed <span class="s1">&#39;s#^#https://#;s#$#:2379#;$s#\n##&#39;</span> <span class="p">|</span> paste -d, -s - <span class="k">)</span>

<span class="c1"># 注入VIP和etcd_servers,apiserver数量</span>
sed -ri <span class="s1">&#39;/--etcd-servers/s#=.+#=&#39;</span><span class="s2">&#34;</span><span class="nv">$etcd_servers</span><span class="s2">&#34;</span><span class="s1">&#39; \\#&#39;</span> systemd/kube-apiserver.service
sed -ri <span class="s1">&#39;/apiserver-count/s#=[^\]+#=&#39;</span><span class="s2">&#34;</span><span class="si">${#</span><span class="nv">MasterArray</span><span class="p">[@]</span><span class="si">}</span><span class="s2">&#34;</span><span class="s1">&#39; #&#39;</span> systemd/kube-apiserver.service

<span class="c1"># 分发文件</span>
<span class="k">for</span> NODE in <span class="s2">&#34;</span><span class="si">${</span><span class="p">!MasterArray[@]</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">;</span> <span class="k">do</span>
    <span class="nb">echo</span> <span class="s2">&#34;--- </span><span class="nv">$NODE</span><span class="s2"> </span><span class="si">${</span><span class="nv">MasterArray</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span><span class="s2"> ---&#34;</span>
    ssh <span class="si">${</span><span class="nv">MasterArray</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span> <span class="s1">&#39;mkdir -p /etc/kubernetes/manifests /var/lib/kubelet /var/log/kubernetes&#39;</span>
    scp systemd/kube-*.service <span class="si">${</span><span class="nv">MasterArray</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span>:/usr/lib/systemd/system/
    <span class="c1">#注入网卡ip</span>
    ssh <span class="si">${</span><span class="nv">MasterArray</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span> <span class="s2">&#34;sed -ri &#39;/bind-address/s#=[^\]+#=</span><span class="si">${</span><span class="nv">MasterArray</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span><span class="s2"> #&#39; /usr/lib/systemd/system/kube-apiserver.service &amp;&amp; sed -ri &#39;/--advertise-address/s#=[^\]+#=</span><span class="si">${</span><span class="nv">MasterArray</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span><span class="s2"> #&#39; /usr/lib/systemd/system/kube-apiserver.service&#34;</span>

<span class="k">done</span>
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>在 k8s-1 上给所有 master 机器启动 kube-apiserver、kube-controller-manager、kube-scheduler 和 kubelet 服务并设置 kubectl 补全脚本:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="k">for</span> NODE in <span class="s2">&#34;</span><span class="si">${</span><span class="p">!MasterArray[@]</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">;</span> <span class="k">do</span>
    <span class="nb">echo</span> <span class="s2">&#34;--- </span><span class="nv">$NODE</span><span class="s2"> </span><span class="si">${</span><span class="nv">MasterArray</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span><span class="s2"> ---&#34;</span>
    ssh <span class="si">${</span><span class="nv">MasterArray</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span> <span class="s1">&#39;systemctl enable --now  kube-apiserver kube-controller-manager kube-scheduler;
</span><span class="s1">    mkdir -p ~/.kube/
</span><span class="s1">    cp /etc/kubernetes/admin.kubeconfig ~/.kube/config;
</span><span class="s1">    kubectl completion bash &gt; /etc/bash_completion.d/kubectl&#39;</span>
<span class="k">done</span>
</code></pre></td></tr></table>
</div>
</div><blockquote>
<p>注意： k8s v1.14.0版本已经将admission-plugins: Initializers移除。<br>
详见：<a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.14.md#v1140">https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.14.md#v1140</a></p>
</blockquote>
</li>
<li>
<p>验证组件
在任意一台 master 节点通过简单指令验证：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">$ kubectl get cs
NAME                 STATUS    MESSAGE              ERROR
scheduler            Healthy   ok
controller-manager   Healthy   ok
etcd-2               Healthy   <span class="o">{</span><span class="s2">&#34;health&#34;</span>: <span class="s2">&#34;true&#34;</span><span class="o">}</span>
etcd-0               Healthy   <span class="o">{</span><span class="s2">&#34;health&#34;</span>: <span class="s2">&#34;true&#34;</span><span class="o">}</span>
etcd-1               Healthy   <span class="o">{</span><span class="s2">&#34;health&#34;</span>: <span class="s2">&#34;true&#34;</span><span class="o">}</span>

$ kubectl get svc
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT<span class="o">(</span>S<span class="o">)</span>   AGE
kubernetes   ClusterIP   10.96.0.1    &lt;none&gt;        443/TCP   36s
</code></pre></td></tr></table>
</div>
</div></li>
</ol>
<h4 id="配置-bootstrap">配置 bootstrap</h4>
<p>由于本次安装启用了 TLS 认证，因此每个节点的 kubelet 都必须使用 kube-apiserver 的 CA 的凭证后，才能与 kube-apiserver 进行沟通，而该过程需要手动针对每台节点单独签署凭证是一件繁琐的事情，且一旦节点增加会延伸出管理不易问题；而 TLS bootstrapping 目标就是解决该问题，通过让 kubelet 先使用一个预定低权限使用者连接到 kube-apiserver，然后在对 kube-apiserver 申请凭证签署，当授权 Token 一致时，Node 节点的 kubelet 凭证将由 kube-apiserver 动态签署提供。具体作法可以参考 <a href="https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/" target="_blank" rel="noopener noreffer">TLS Bootstrapping</a> 与 <a href="https://kubernetes.io/docs/admin/bootstrap-tokens/" target="_blank" rel="noopener noreffer">Authenticating with Bootstrap Tokens</a>。</p>
<p><strong>以下步骤在一个 master 节点上执行即可：</strong></p>
<ol>
<li>
<p>首先在 k8s-m1 建立一个变数来产生 BOOTSTRAP_TOKEN ，并建立 bootstrap 的 kubeconfig 文件，接着在 k8s-m1 建立 TLS bootstrap secret 来提供自动签证使用：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="nv">TOKEN_PUB</span><span class="o">=</span><span class="k">$(</span>openssl rand -hex 3<span class="k">)</span>
<span class="nv">TOKEN_SECRET</span><span class="o">=</span><span class="k">$(</span>openssl rand -hex 8<span class="k">)</span>
<span class="nv">BOOTSTRAP_TOKEN</span><span class="o">=</span><span class="s2">&#34;</span><span class="si">${</span><span class="nv">TOKEN_PUB</span><span class="si">}</span><span class="s2">.</span><span class="si">${</span><span class="nv">TOKEN_SECRET</span><span class="si">}</span><span class="s2">&#34;</span>

kubectl -n kube-system create secret generic bootstrap-token-<span class="si">${</span><span class="nv">TOKEN_PUB</span><span class="si">}</span> <span class="se">\
</span><span class="se"></span>        --type <span class="s1">&#39;bootstrap.kubernetes.io/token&#39;</span> <span class="se">\
</span><span class="se"></span>        --from-literal <span class="nv">description</span><span class="o">=</span><span class="s2">&#34;cluster bootstrap token&#34;</span> <span class="se">\
</span><span class="se"></span>        --from-literal token-id<span class="o">=</span><span class="si">${</span><span class="nv">TOKEN_PUB</span><span class="si">}</span> <span class="se">\
</span><span class="se"></span>        --from-literal token-secret<span class="o">=</span><span class="si">${</span><span class="nv">TOKEN_SECRET</span><span class="si">}</span> <span class="se">\
</span><span class="se"></span>        --from-literal usage-bootstrap-authentication<span class="o">=</span><span class="nb">true</span> <span class="se">\
</span><span class="se"></span>        --from-literal usage-bootstrap-signing<span class="o">=</span><span class="nb">true</span>
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>建立 bootstrap 的 kubeconfig 文件</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="nv">CLUSTER_NAME</span><span class="o">=</span><span class="s2">&#34;kubernetes&#34;</span>
<span class="nv">KUBE_USER</span><span class="o">=</span><span class="s2">&#34;kubelet-bootstrap&#34;</span>
<span class="nv">KUBE_CONFIG</span><span class="o">=</span><span class="s2">&#34;bootstrap.kubeconfig&#34;</span>

<span class="c1"># 设置集群参数</span>
kubectl config set-cluster <span class="si">${</span><span class="nv">CLUSTER_NAME</span><span class="si">}</span> <span class="se">\
</span><span class="se"></span>  --certificate-authority<span class="o">=</span>/etc/kubernetes/pki/ca.crt <span class="se">\
</span><span class="se"></span>  --embed-certs<span class="o">=</span><span class="nb">true</span> <span class="se">\
</span><span class="se"></span>  --server<span class="o">=</span><span class="si">${</span><span class="nv">KUBE_APISERVER</span><span class="si">}</span> <span class="se">\
</span><span class="se"></span>  --kubeconfig<span class="o">=</span>/etc/kubernetes/<span class="si">${</span><span class="nv">KUBE_CONFIG</span><span class="si">}</span>

<span class="c1"># 设置上下文参数</span>
kubectl config set-context <span class="si">${</span><span class="nv">KUBE_USER</span><span class="si">}</span>@<span class="si">${</span><span class="nv">CLUSTER_NAME</span><span class="si">}</span> <span class="se">\
</span><span class="se"></span>  --cluster<span class="o">=</span><span class="si">${</span><span class="nv">CLUSTER_NAME</span><span class="si">}</span> <span class="se">\
</span><span class="se"></span>  --user<span class="o">=</span><span class="si">${</span><span class="nv">KUBE_USER</span><span class="si">}</span> <span class="se">\
</span><span class="se"></span>  --kubeconfig<span class="o">=</span>/etc/kubernetes/<span class="si">${</span><span class="nv">KUBE_CONFIG</span><span class="si">}</span>

<span class="c1"># 设置客户端认证参数</span>
kubectl config set-credentials <span class="si">${</span><span class="nv">KUBE_USER</span><span class="si">}</span> <span class="se">\
</span><span class="se"></span>  --token<span class="o">=</span><span class="si">${</span><span class="nv">BOOTSTRAP_TOKEN</span><span class="si">}</span> <span class="se">\
</span><span class="se"></span>  --kubeconfig<span class="o">=</span>/etc/kubernetes/<span class="si">${</span><span class="nv">KUBE_CONFIG</span><span class="si">}</span>

<span class="c1"># 设置当前使用的上下文</span>
kubectl config use-context <span class="si">${</span><span class="nv">KUBE_USER</span><span class="si">}</span>@<span class="si">${</span><span class="nv">CLUSTER_NAME</span><span class="si">}</span> --kubeconfig<span class="o">=</span>/etc/kubernetes/<span class="si">${</span><span class="nv">KUBE_CONFIG</span><span class="si">}</span>

<span class="c1"># 查看生成的配置文件</span>
kubectl config view --kubeconfig<span class="o">=</span>/etc/kubernetes/<span class="si">${</span><span class="nv">KUBE_CONFIG</span><span class="si">}</span>
</code></pre></td></tr></table>
</div>
</div><blockquote>
<ul>
<li>若想要用手动签署凭证来进行授权的话，可以参考 <a href="https://kubernetes.io/docs/concepts/cluster-administration/certificates/" target="_blank" rel="noopener noreffer">Certificate</a>。</li>
<li>向 kubeconfig 写入的是 token，bootstrap 结束后 kube-controller-manager 为 kubelet 创建 client 和 server 证书；</li>
<li>token 有效期为 1 天，超期后将不能再被用来 boostrap kubelet，且会被 kube-controller-manager 的 tokencleaner 清理；</li>
<li>kube-apiserver 接收 kubelet 的 bootstrap token 后，将请求的 user 设置为 system:bootstrap:&lt;Token ID&gt;，group 设置为 system:bootstrappers，后续将为这个 group 设置 ClusterRoleBinding；</li>
</ul>
</blockquote>
</li>
<li>
<p>授权 kubelet 可以创建 csr</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">kubectl create clusterrolebinding kubeadm:kubelet-bootstrap <span class="se">\
</span><span class="se"></span>        --clusterrole system:node-bootstrapper --group system:bootstrappers
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>批准 csr 请求，允许 system:bootstrappers 组的所有 csr</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">cat <span class="s">&lt;&lt;EOF | kubectl apply -f -
</span><span class="s"># Approve all CSRs for the group &#34;system:bootstrappers&#34;
</span><span class="s">kind: ClusterRoleBinding
</span><span class="s">apiVersion: rbac.authorization.k8s.io/v1
</span><span class="s">metadata:
</span><span class="s">  name: auto-approve-csrs-for-group
</span><span class="s">subjects:
</span><span class="s">- kind: Group
</span><span class="s">  name: system:bootstrappers
</span><span class="s">  apiGroup: rbac.authorization.k8s.io
</span><span class="s">roleRef:
</span><span class="s">  kind: ClusterRole
</span><span class="s">  name: system:certificates.k8s.io:certificatesigningrequests:nodeclient
</span><span class="s">  apiGroup: rbac.authorization.k8s.io
</span><span class="s">EOF</span>
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>允许 kubelet 能够更新自己的证书</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">cat <span class="s">&lt;&lt;EOF | kubectl apply -f -
</span><span class="s"># Approve renewal CSRs for the group &#34;system:nodes&#34;
</span><span class="s">kind: ClusterRoleBinding
</span><span class="s">apiVersion: rbac.authorization.k8s.io/v1
</span><span class="s">metadata:
</span><span class="s">  name: auto-approve-renewals-for-nodes
</span><span class="s">subjects:
</span><span class="s">- kind: Group
</span><span class="s">  name: system:nodes
</span><span class="s">  apiGroup: rbac.authorization.k8s.io
</span><span class="s">roleRef:
</span><span class="s">  kind: ClusterRole
</span><span class="s">  name: system:certificates.k8s.io:certificatesigningrequests:selfnodeclient
</span><span class="s">  apiGroup: rbac.authorization.k8s.io
</span><span class="s">EOF</span>
</code></pre></td></tr></table>
</div>
</div></li>
</ol>
<h3 id="all-kubernetes-nodes">All Kubernetes Nodes</h3>
<p>本部分将说明如何建立与设定 Kubernetes Node 角色，Node 是主要执行容器实例（Pod）的工作节点。<br>
在开始部署前，先在 k8-m1 将需要用到的文件复制到所有其他节点上。<br>
kubelet的配置选项官方建议大多数的参数写一个 yaml 里用 &ndash;config 去指定，详见： <a href="https://godoc.org/k8s.io/kubernetes/pkg/kubelet/apis/config#KubeletConfiguration">https://godoc.org/k8s.io/kubernetes/pkg/kubelet/apis/config#KubeletConfiguration</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="k">for</span> NODE in <span class="s2">&#34;</span><span class="si">${</span><span class="p">!Other[@]</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">;</span> <span class="k">do</span>
    <span class="nb">echo</span> <span class="s2">&#34;--- </span><span class="nv">$NODE</span><span class="s2"> </span><span class="si">${</span><span class="nv">Other</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span><span class="s2"> ---&#34;</span>
    ssh <span class="si">${</span><span class="nv">Other</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span> <span class="s2">&#34;mkdir -p /etc/kubernetes/pki /etc/kubernetes/manifests /var/lib/kubelet/&#34;</span>
    <span class="k">for</span> FILE in /etc/kubernetes/pki/ca.crt /etc/kubernetes/bootstrap.kubeconfig<span class="p">;</span> <span class="k">do</span>
      scp <span class="si">${</span><span class="nv">FILE</span><span class="si">}</span> <span class="si">${</span><span class="nv">Other</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span>:<span class="si">${</span><span class="nv">FILE</span><span class="si">}</span>
    <span class="k">done</span>
<span class="k">done</span>
</code></pre></td></tr></table>
</div>
</div><p>在 k8s-m1 节点分发 kubelet.service 文件和其他配置文件到每台主机上管理kubelet：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="nb">cd</span> /home/k8s/k8s-manual-files/
<span class="k">for</span> NODE in <span class="s2">&#34;</span><span class="si">${</span><span class="p">!AllNode[@]</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">;</span> <span class="k">do</span>
    <span class="nb">echo</span> <span class="s2">&#34;--- </span><span class="nv">$NODE</span><span class="s2"> </span><span class="si">${</span><span class="nv">AllNode</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span><span class="s2"> ---&#34;</span>
    scp master/systemd/kubelet.service <span class="si">${</span><span class="nv">AllNode</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span>:/lib/systemd/system/kubelet.service
    scp master/etc/kubelet/kubelet-conf.yml <span class="si">${</span><span class="nv">AllNode</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span>:/etc/kubernetes/kubelet-conf.yml
    ssh <span class="si">${</span><span class="nv">AllNode</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span> <span class="s2">&#34;sed -ri &#39;/0.0.0.0/s#\S+\$#</span><span class="si">${</span><span class="nv">MasterArray</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span><span class="s2">#&#39; /etc/kubernetes/kubelet-conf.yml&#34;</span>
    ssh <span class="si">${</span><span class="nv">AllNode</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span> <span class="s2">&#34;sed -ri &#39;/127.0.0.1/s#\S+\$#</span><span class="si">${</span><span class="nv">MasterArray</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span><span class="s2">#&#39; /etc/kubernetes/kubelet-conf.yml&#34;</span>
<span class="k">done</span>
</code></pre></td></tr></table>
</div>
</div><p>在 k8s-m1 上去启动每个 node 节点的 kubelet 服务:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="k">for</span> NODE in <span class="s2">&#34;</span><span class="si">${</span><span class="p">!AllNode[@]</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">;</span> <span class="k">do</span>
    <span class="nb">echo</span> <span class="s2">&#34;--- </span><span class="nv">$NODE</span><span class="s2"> </span><span class="si">${</span><span class="nv">AllNode</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span><span class="s2"> ---&#34;</span>
    ssh <span class="si">${</span><span class="nv">AllNode</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span> <span class="s1">&#39;systemctl enable --now kubelet.service&#39;</span>
<span class="k">done</span>
</code></pre></td></tr></table>
</div>
</div><p>在任意一台master节点并通过简单指令验证集群状态：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="o">[</span>root@k8s-m1 k8s-manual-files<span class="o">]</span><span class="c1"># kubectl get nodes</span>
NAME     STATUS   ROLES    AGE   VERSION
c48-1    Ready    &lt;none&gt;   19s   v1.15.0
c48-2    Ready    &lt;none&gt;   20s   v1.15.0
k8s-m1   Ready    &lt;none&gt;   32s   v1.15.0
k8s-m2   Ready    &lt;none&gt;   31s   v1.15.0
k8s-m3   Ready    &lt;none&gt;   31s   v1.15.0
<span class="o">[</span>root@k8s-m1 k8s-manual-files<span class="o">]</span><span class="c1"># kubectl get csr</span>
NAME        AGE   REQUESTOR                 CONDITION
csr-cwsqq   70s   system:bootstrap:e006fa   Approved,Issued
csr-dqw9s   86s   system:bootstrap:e006fa   Approved,Issued
csr-lchwz   69s   system:bootstrap:e006fa   Approved,Issued
csr-pg4tq   86s   system:bootstrap:e006fa   Approved,Issued
csr-s2ts5   86s   system:bootstrap:e006fa   Approved,Issued
</code></pre></td></tr></table>
</div>
</div><blockquote>
<ul>
<li>kubelet 中的 &ndash;allow-privileged 参数在 1.15.0 版本已被废弃，详见：<a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.15.md#v1154">https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.15.md#v1154</a></li>
<li>若不想让（没有声明容忍该污点的）pod 跑在 master 上，则执行如下命令给 master 节点加上污点 traint</li>
</ul>
</blockquote>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">kubectl taint nodes <span class="si">${</span><span class="p">!MasterArray[@]</span><span class="si">}</span> node-role.kubernetes.io/master<span class="o">=</span><span class="s2">&#34;&#34;</span>:NoSchedule
</code></pre></td></tr></table>
</div>
</div><blockquote>
<ul>
<li>容忍与污点详见：<a href="https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/">https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/</a></li>
</ul>
</blockquote>
<p>为 node 打上标签，标记其为 master 或 worker （master 节点也作为 worker节点）</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">kubectl label node <span class="si">${</span><span class="p">!MasterArray[@]</span><span class="si">}</span> node-role.kubernetes.io/master<span class="o">=</span><span class="s2">&#34;&#34;</span>
kubectl label node <span class="si">${</span><span class="p">!NodeArray[@]</span><span class="si">}</span> node-role.kubernetes.io/worker<span class="o">=</span>worker
</code></pre></td></tr></table>
</div>
</div><p>查看当前集群 node 的状态</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="o">[</span>root@k8s-m1 k8s-manual-files<span class="o">]</span><span class="c1"># kubectl get nodes</span>
NAME     STATUS   ROLES           AGE     VERSION
c48-1    Ready    worker          2m47s   v1.15.0
c48-2    Ready    worker          2m48s   v1.15.0
k8s-m1   Ready    master,worker   3m      v1.15.0
k8s-m2   Ready    master,worker   2m59s   v1.15.0
k8s-m3   Ready    master,worker   2m59s   v1.15.0
</code></pre></td></tr></table>
</div>
</div><p>kubelet 的数据存储目录默认为 /var/lib/kubelet，为了避免 /var 目录所在的分区空间不足，可以通过 &ndash;root-dir 参数（使用 kubelet &ndash;help 查看）修改 kubelet 的默认数据存储目录，但是由于 rook-ceph 等插件默认读取的是 /var/lib/kubelet 目录，所以方便起见，本文使用软链接的方式修改 kubelet 的存储目录。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">systemctl stop kubelet
rm -rf /var/lib/kubelet
mkdir -p /home/data/kubelet
ln -s /home/data/kubelet /var/lib/kubelet
</code></pre></td></tr></table>
</div>
</div><p>上述命令将 /home/data/kubelet 软链接到 /var/lib/kubelet。<br>
若删除 /var/lib/kubelet 时报错，提示挂载问题，可以手动使用 umount 命令卸载正在挂载的目录。</p>
<h2 id="kubernetes-core-addons">Kubernetes Core Addons</h2>
<h3 id="kube-proxy">kube-proxy</h3>
<p>Kube-proxy 是实现 Service 的关键插件，kube-proxy 会在运行在每台 worker 节点，通过监听 API Server 的 Service 与 Endpoint 资源物件的改变，来依据变化执行 iptables 来实现网路的转发。</p>
<p>本文档使用 ipvs 模式部署 kube-proxy。提供了二进制部署方式和 daemonset 部署方式。建议使用 daemonset 方式进行部署。</p>
<h4 id="二进制方式部署-kube-proxy">二进制方式部署 kube-proxy</h4>
<p>在 k8s-m1 上新建一个 kube-proxy 的 serviceaccount：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">kubectl -n kube-system create serviceaccount kube-proxy
</code></pre></td></tr></table>
</div>
</div><p>将 kube-proxy 的 serviceaccount 绑定到 clusterrole system:node-proxier 以运行 RBAC ：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">kubectl create clusterrolebinding kubeadm:kube-proxy <span class="se">\
</span><span class="se"></span>        --clusterrole system:node-proxier <span class="se">\
</span><span class="se"></span>        --serviceaccount kube-system:kube-proxy
</code></pre></td></tr></table>
</div>
</div><p>创建 kube-proxy 的 kubeconfig</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="nv">CLUSTER_NAME</span><span class="o">=</span><span class="s2">&#34;kubernetes&#34;</span>
<span class="nv">KUBE_CONFIG</span><span class="o">=</span><span class="s2">&#34;kube-proxy.kubeconfig&#34;</span>

<span class="nv">SECRET</span><span class="o">=</span><span class="k">$(</span>kubectl -n kube-system get sa/kube-proxy <span class="se">\
</span><span class="se"></span>    --output<span class="o">=</span><span class="nv">jsonpath</span><span class="o">=</span><span class="s1">&#39;{.secrets[0].name}&#39;</span><span class="k">)</span>

<span class="nv">JWT_TOKEN</span><span class="o">=</span><span class="k">$(</span>kubectl -n kube-system get secret/<span class="nv">$SECRET</span> <span class="se">\
</span><span class="se"></span>    --output<span class="o">=</span><span class="nv">jsonpath</span><span class="o">=</span><span class="s1">&#39;{.data.token}&#39;</span> <span class="p">|</span> base64 -d<span class="k">)</span>

kubectl config set-cluster <span class="si">${</span><span class="nv">CLUSTER_NAME</span><span class="si">}</span> <span class="se">\
</span><span class="se"></span>  --certificate-authority<span class="o">=</span>/etc/kubernetes/pki/ca.crt <span class="se">\
</span><span class="se"></span>  --embed-certs<span class="o">=</span><span class="nb">true</span> <span class="se">\
</span><span class="se"></span>  --server<span class="o">=</span><span class="si">${</span><span class="nv">KUBE_APISERVER</span><span class="si">}</span> <span class="se">\
</span><span class="se"></span>  --kubeconfig<span class="o">=</span>/etc/kubernetes/<span class="si">${</span><span class="nv">KUBE_CONFIG</span><span class="si">}</span>

kubectl config set-context <span class="si">${</span><span class="nv">CLUSTER_NAME</span><span class="si">}</span> <span class="se">\
</span><span class="se"></span>  --cluster<span class="o">=</span><span class="si">${</span><span class="nv">CLUSTER_NAME</span><span class="si">}</span> <span class="se">\
</span><span class="se"></span>  --user<span class="o">=</span><span class="si">${</span><span class="nv">CLUSTER_NAME</span><span class="si">}</span> <span class="se">\
</span><span class="se"></span>  --kubeconfig<span class="o">=</span>/etc/kubernetes/<span class="si">${</span><span class="nv">KUBE_CONFIG</span><span class="si">}</span>

kubectl config set-credentials <span class="si">${</span><span class="nv">CLUSTER_NAME</span><span class="si">}</span> <span class="se">\
</span><span class="se"></span>  --token<span class="o">=</span><span class="si">${</span><span class="nv">JWT_TOKEN</span><span class="si">}</span> <span class="se">\
</span><span class="se"></span>  --kubeconfig<span class="o">=</span>/etc/kubernetes/<span class="si">${</span><span class="nv">KUBE_CONFIG</span><span class="si">}</span>

kubectl config use-context <span class="si">${</span><span class="nv">CLUSTER_NAME</span><span class="si">}</span> --kubeconfig<span class="o">=</span>/etc/kubernetes/<span class="si">${</span><span class="nv">KUBE_CONFIG</span><span class="si">}</span>
kubectl config view --kubeconfig<span class="o">=</span>/etc/kubernetes/<span class="si">${</span><span class="nv">KUBE_CONFIG</span><span class="si">}</span>
</code></pre></td></tr></table>
</div>
</div><p>在 k8s-m1 上将相关文件分发到所有 worker 节点</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="nb">cd</span> /home/k8s/k8s-manual-files/
<span class="k">for</span> NODE in <span class="s2">&#34;</span><span class="si">${</span><span class="p">!Other[@]</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">;</span> <span class="k">do</span>
    <span class="nb">echo</span> <span class="s2">&#34;--- </span><span class="nv">$NODE</span><span class="s2"> </span><span class="si">${</span><span class="nv">Other</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span><span class="s2"> ---&#34;</span>
    scp /etc/kubernetes/kube-proxy.kubeconfig <span class="si">${</span><span class="nv">Other</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span>:/etc/kubernetes/kube-proxy.kubeconfig
<span class="k">done</span>

<span class="k">for</span> NODE in <span class="s2">&#34;</span><span class="si">${</span><span class="p">!AllNode[@]</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">;</span> <span class="k">do</span>
    <span class="nb">echo</span> <span class="s2">&#34;--- </span><span class="nv">$NODE</span><span class="s2"> </span><span class="si">${</span><span class="nv">AllNode</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span><span class="s2"> ---&#34;</span>
    scp addons/kube-proxy/kube-proxy.conf <span class="si">${</span><span class="nv">AllNode</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span>:/etc/kubernetes/kube-proxy.conf
    scp addons/kube-proxy/kube-proxy.service <span class="si">${</span><span class="nv">AllNode</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span>:/usr/lib/systemd/system/kube-proxy.service
    ssh <span class="si">${</span><span class="nv">AllNode</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span> <span class="s2">&#34;sed -ri &#39;/0.0.0.0/s#\S+\$#</span><span class="si">${</span><span class="nv">MasterArray</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span><span class="s2">#&#39; /etc/kubernetes/kube-proxy.conf&#34;</span>
<span class="k">done</span>
</code></pre></td></tr></table>
</div>
</div><p>在 k8s-m1 上启动所有 worker 节点的 kube-proxy 服务</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="k">for</span> NODE in <span class="s2">&#34;</span><span class="si">${</span><span class="p">!AllNode[@]</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">;</span> <span class="k">do</span>
    <span class="nb">echo</span> <span class="s2">&#34;--- </span><span class="nv">$NODE</span><span class="s2"> </span><span class="si">${</span><span class="nv">AllNode</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span><span class="s2"> ---&#34;</span>
    ssh <span class="si">${</span><span class="nv">AllNode</span><span class="p">[</span><span class="nv">$NODE</span><span class="p">]</span><span class="si">}</span> <span class="s1">&#39;systemctl enable --now kube-proxy&#39;</span>
<span class="k">done</span>
</code></pre></td></tr></table>
</div>
</div><h4 id="daemonset-方式部署-kube-proxy">daemonset 方式部署 kube-proxy</h4>
<p>在 k8s-m1 上执行如下命令：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="nb">cd</span> /home/k8s/k8s-manual-files
<span class="nb">source</span> /home/k8s/env.sh
<span class="c1"># 注入变量</span>
sed -ri <span class="s2">&#34;/server:/s#(: ).+#\1</span><span class="si">${</span><span class="nv">KUBE_APISERVER</span><span class="si">}</span><span class="s2">#&#34;</span> addons/kube-proxy/kube-proxy.yml
sed -ri <span class="s2">&#34;/image:.+kube-proxy/s#:[^:]+\$#:</span><span class="nv">$KUBE_VERSION</span><span class="s2">#&#34;</span> addons/kube-proxy/kube-proxy.yml
kubectl apply -f addons/kube-proxy/kube-proxy.yml
</code></pre></td></tr></table>
</div>
</div><p>输出内容如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">serviceaccount <span class="s2">&#34;kube-proxy&#34;</span> created
clusterrolebinding.rbac.authorization.k8s.io <span class="s2">&#34;system:kube-proxy&#34;</span> created
configmap <span class="s2">&#34;kube-proxy&#34;</span> created
daemonset.apps <span class="s2">&#34;kube-proxy&#34;</span> created
</code></pre></td></tr></table>
</div>
</div><p>查看当前 kube-proxy pod 的状态</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="o">[</span>root@k8s-m1 k8s-manual-files<span class="o">]</span><span class="c1"># kubectl -n kube-system get po -l k8s-app=kube-proxy -o wide</span>
NAME               READY   STATUS    RESTARTS   AGE   IP               NODE     NOMINATED NODE   READINESS GATES
kube-proxy-52hkz   1/1     Running   <span class="m">0</span>          54s   192.168.15.161   c48-1    &lt;none&gt;           &lt;none&gt;
kube-proxy-54f8d   1/1     Running   <span class="m">0</span>          54s   192.168.15.143   c48-2    &lt;none&gt;           &lt;none&gt;
kube-proxy-757j7   1/1     Running   <span class="m">0</span>          54s   192.168.15.6     k8s-m2   &lt;none&gt;           &lt;none&gt;
kube-proxy-c2ht9   1/1     Running   <span class="m">0</span>          54s   192.168.15.7     k8s-m3   &lt;none&gt;           &lt;none&gt;
kube-proxy-fdx48   1/1     Running   <span class="m">0</span>          54s   192.168.15.5     k8s-m1   &lt;none&gt;           &lt;none&gt;
</code></pre></td></tr></table>
</div>
</div><p>通过 ipvsadm 查看当前 proxy 的规则</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="o">[</span>root@k8s-m1 k8s-manual-files<span class="o">]</span><span class="c1"># ipvsadm -ln</span>
IP Virtual Server version 1.2.1 <span class="o">(</span><span class="nv">size</span><span class="o">=</span>4096<span class="o">)</span>
Prot LocalAddress:Port Scheduler Flags
  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  10.96.0.1:443 rr
  -&gt; 192.168.15.5:6443            Masq    <span class="m">1</span>      <span class="m">0</span>          <span class="m">0</span>
  -&gt; 192.168.15.6:6443            Masq    <span class="m">1</span>      <span class="m">0</span>          <span class="m">0</span>
  -&gt; 192.168.15.7:6443            Masq    <span class="m">1</span>      <span class="m">0</span>          <span class="m">0</span>
</code></pre></td></tr></table>
</div>
</div><p>确认使用的是 ipvs 模式</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="o">[</span>root@k8s-m1 k8s-manual-files<span class="o">]</span><span class="c1"># curl localhost:10249/proxyMode</span>
ipvs
</code></pre></td></tr></table>
</div>
</div><h3 id="网络插件-flannel-或-calico">网络插件 flannel 或 calico</h3>
<p>Kubernetes 在默认情況下与 Docker 的网络有所不同。在 Kubernetes 中有四个问题是需要被解決的,分別为：</p>
<ul>
<li>高耦合的容器到容器通信：通过 Pods 内 localhost 的來解決。</li>
<li>Pod 到 Pod 的通信：通过实现网络模型来解决。</li>
<li>Pod 到 Service 通信：由 Service objects 结合 kube-proxy 解決。</li>
<li>外部到 Service 通信：一样由 Service objects 结合 kube-proxy 解決。</li>
</ul>
<p>而 Kubernetes 对于任何网络的实现都需要满足以下基本要求(除非是有意调整的网络分段策略)：</p>
<ul>
<li>所有容器能够在沒有 NAT 的情況下与其他容器通信。</li>
<li>所有节点能够在沒有 NAT 情況下与所有容器通信(反之亦然)。</li>
<li>容器看到的 IP 与其他人看到的 IP 是一样的。</li>
</ul>
<p>庆幸的是 Kubernetes 已经有非常多种的<a href="https://kubernetes.io/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-networking-model" target="_blank" rel="noopener noreffer">网络模型</a>作为<a href="https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/" target="_blank" rel="noopener noreffer">网络插件（Network Plugins）</a>方式被实现，因此可以选用满足自己需求的网络功能来使用。另外 Kubernetes 中的网络插件有以下两种形式：</p>
<ul>
<li>CNI plugins：以 appc/CNI 标准规范所实现的网络，详细可以阅读 <a href="https://github.com/containernetworking/cni/blob/master/SPEC.md" target="_blank" rel="noopener noreffer">CNI Specification</a>。</li>
<li>Kubenet plugin：使用 CNI plugins 的 bridge 与 host-local 来实现基本的 cbr0。这通常被用在公有云服务上的 Kubernetes 集群网络。</li>
</ul>
<p>如果想了解如何选择可以如阅读 Chris Love 的 <a href="https://chrislovecnm.com/kubernetes/cni/choosing-a-cni-provider/" target="_blank" rel="noopener noreffer">Choosing a CNI Network Provider for Kubernetes</a> 文章。</p>
<h4 id="flannel">flannel</h4>
<p>flannel 使用 vxlan 技术为各节点创建一个可以互通的 Pod 网络，使用的端口为 UDP 8472，需要开放该端口（如公有云 AWS 等）。</p>
<p>flannel 第一次启动时，从 etcd 获取 Pod 网段信息，为本节点分配一个未使用的 /24 段地址，然后创建 flannel.1（也可能是其它名称，如 flannel1 等） 接口。</p>
<p>查看使用的 flannel 容器</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">$ grep -Pom1 <span class="s1">&#39;image:\s+\K\S+&#39;</span> addons/flannel/kube-flannel.yml
quay.io/coreos/flannel:v0.11.0-amd64
</code></pre></td></tr></table>
</div>
</div><p>使用 daemonset 创建 flannel，yaml 来源于<a href="https://github.com/coreos/flannel/blob/master/Documentation/kube-flannel.yml" target="_blank" rel="noopener noreffer">官方</a>，删除了非  amd64 的 daemonset</p>
<p>将环境变量中的 interface 配置到 yml 中，然后安装 flannel</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">sed -ri <span class="s2">&#34;s#\{\{ interface \}\}#</span><span class="si">${</span><span class="nv">interface</span><span class="si">}</span><span class="s2">#&#34;</span> addons/flannel/kube-flannel.yml
kubectl apply -f addons/flannel/kube-flannel.yml
</code></pre></td></tr></table>
</div>
</div><p>检查 flannel 启动是否成功</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">$ kubectl -n kube-system get po -l k8s-app<span class="o">=</span>flannel
NAME                READY     STATUS    RESTARTS   AGE
kube-flannel-ds-27jwl   2/2       Running   <span class="m">0</span>          59s
kube-flannel-ds-4fgv6   2/2       Running   <span class="m">0</span>          59s
kube-flannel-ds-mvrt7   2/2       Running   <span class="m">0</span>          59s
kube-flannel-ds-p2q9g   2/2       Running   <span class="m">0</span>          59s
kube-flannel-ds-zchsz   2/2       Running   <span class="m">0</span>          59s
</code></pre></td></tr></table>
</div>
</div><h4 id="calico">calico</h4>
<p>Calico 是一款纯 Layer 3 的网络，其好处是它整合了各种云原生平台（Docker、Mesos 与 OpenStack 等），且 Calico 不采用 vSwitch，而是在每个 Kubernetes 节点使用 vRouter 功能，并通过 Linux Kernel 既有的 L3 forwarding 功能，而当资料中心复杂度增加时，Calico 也可以利用 BGP route reflector 来达成。</p>
<blockquote>
<ul>
<li>想了解 Calico 与传统 overlay networks 的差异，可以阅读 <a href="https://www.projectcalico.org/learn/" target="_blank" rel="noopener noreffer">Difficulties with traditional overlay networks</a> 文章</li>
<li><a href="https://docs.projectcalico.org/latest/getting-started/kubernetes/installation/" target="_blank" rel="noopener noreffer">calico 官方安装指南</a></li>
</ul>
</blockquote>
<p>calico 官方关于使用 calico  提供策略和网络的安装方式（Installing Calico for policy and networking）有三种，根据数据存储类型（datastore type）和节点数量（number of nodes）进行划分：</p>
<ul>
<li><a href="https://docs.projectcalico.org/v3.9/getting-started/kubernetes/installation/calico#installing-with-the-kubernetes-api-datastore50-nodes-or-less" target="_blank" rel="noopener noreffer">使用 kubernetes api 存储数据 且节点数量少于50</a></li>
<li><a href="https://docs.projectcalico.org/v3.9/getting-started/kubernetes/installation/calico#installing-with-the-kubernetes-api-datastoremore-than-50-nodes" target="_blank" rel="noopener noreffer">使用 kubernetes api 存储数据，且节点数量大于50</a></li>
<li><a href="https://docs.projectcalico.org/v3.9/getting-started/kubernetes/installation/calico#installing-with-the-etcd-datastore" target="_blank" rel="noopener noreffer">使用 etcd 存储数据</a></li>
</ul>
<p>本文使用 etcd 作为 calico 后端存储数据。</p>
<ol>
<li>
<p>下载 calico 适用于 etcd 后端存储的清单文件</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">curl https://docs.projectcalico.org/v3.9/manifests/calico-etcd.yaml -o calico-etcd.yaml
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>如果你使用的 pod 的 CIDR 为 <code>10.244.0.0/16</code> ，则跳过该步骤。如果你使用的 pod 的 CIDR 不同，使用如下命令设置一个名为 <code>POD_CIDR</code> 的环境变量，将清单文件中的 <code>10.244.0.0/16</code> 替换为你的 pod 的 CIDR。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="nv">POD_CIDR</span><span class="o">=</span><span class="s2">&#34;&lt;your-pod-cidr&gt;&#34;</span>
sed -i -e <span class="s2">&#34;s?10.244.0.0/16?</span><span class="nv">$POD_CIDR</span><span class="s2">?g&#34;</span> calico-etcd.yaml
</code></pre></td></tr></table>
</div>
</div><blockquote>
<p>注意：实际的清单文件中，pod cidr 为 192.168.0.0/16</p>
</blockquote>
<p>因为本文中 pod 的 CIDR 为 172.30.0.0/16，所以需要替换下。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="nv">POD_CIDR</span><span class="o">=</span><span class="s2">&#34;172.30.0.0/16&#34;</span>
sed -i -e <span class="s2">&#34;s?192.168.0.0/16?</span><span class="nv">$POD_CIDR</span><span class="s2">?g&#34;</span> calico-etcd.yaml
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>在清单文件（calico-etcd.yaml）中的名为 calico-config 的 ConfigMap中，设置你的 etcd 服务的 ip 地址和端口，可以通过逗号（英文逗号）作为分隔符指定多个 etcd 服务。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="nb">source</span> /home/k8s/env.sh
<span class="nv">etcd_servers</span><span class="o">=</span><span class="k">$(</span> xargs -n1<span class="o">&lt;&lt;&lt;</span><span class="si">${</span><span class="nv">MasterArray</span><span class="p">[@]</span><span class="si">}</span> <span class="p">|</span> sort <span class="p">|</span> sed <span class="s1">&#39;s#^#https://#;s#$#:2379#;$s#\n##&#39;</span> <span class="p">|</span> paste -d, -s - <span class="k">)</span>
sed -ri <span class="s2">&#34;s#http:\/\/&lt;ETCD_IP&gt;:&lt;ETCD_PORT&gt;#</span><span class="si">${</span><span class="nv">etcd_servers</span><span class="si">}</span><span class="s2">#&#34;</span> calico-etcd.yaml
</code></pre></td></tr></table>
</div>
</div><p>修改后如下所示</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="c1"># Source: calico/templates/calico-config.yaml</span>
<span class="c1"># This ConfigMap is used to configure a self-hosted Calico installation.</span>
kind: ConfigMap
apiVersion: v1
metadata:
  name: calico-config
  namespace: kube-system
data:
  <span class="c1"># Configure this with the location of your etcd cluster.</span>
  etcd_endpoints: <span class="s2">&#34;https://192.168.15.5:2379,https://192.168.15.6:2379,https://192.168.15.7:2379&#34;</span>
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>因为本文的 etcd 服务使用的是 https，所以需要在清单文件中配置下证书，这样 calico 才可以访问 etcd。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="nv">etcd_key</span><span class="o">=</span><span class="k">$(</span>cat /etc/kubernetes/pki/etcd/server.key <span class="p">|</span> base64 <span class="p">|</span> tr -d <span class="s1">&#39;\n&#39;</span><span class="k">)</span>
<span class="nv">etcd_cert</span><span class="o">=</span><span class="k">$(</span>cat /etc/kubernetes/pki/etcd/server.crt <span class="p">|</span> base64 <span class="p">|</span> tr -d <span class="s1">&#39;\n&#39;</span><span class="k">)</span>
<span class="nv">etcd_ca</span><span class="o">=</span><span class="k">$(</span>cat /etc/kubernetes/pki/etcd/ca.crt <span class="p">|</span> base64 <span class="p">|</span> tr -d <span class="s1">&#39;\n&#39;</span><span class="k">)</span>
sed -i <span class="s2">&#34;s/# etcd-key: null/etcd-key: </span><span class="si">${</span><span class="nv">etcd_key</span><span class="si">}</span><span class="s2">/&#34;</span> calico-etcd.yaml
sed -i <span class="s2">&#34;s/# etcd-cert: null/etcd-cert: </span><span class="si">${</span><span class="nv">etcd_cert</span><span class="si">}</span><span class="s2">/&#34;</span> calico-etcd.yaml
sed -i <span class="s2">&#34;s/# etcd-ca: null/etcd-ca: </span><span class="si">${</span><span class="nv">etcd_ca</span><span class="si">}</span><span class="s2">/&#34;</span> calico-etcd.yaml
sed -i <span class="s1">&#39;s?etcd_ca: &#34;&#34;?etcd_ca: &#34;/calico-secrets/etcd-ca&#34;?&#39;</span> calico-etcd.yaml
sed -i <span class="s1">&#39;s?etcd_cert: &#34;&#34;?etcd_cert: &#34;/calico-secrets/etcd-cert&#34;?&#39;</span> calico-etcd.yaml
sed -i <span class="s1">&#39;s?etcd_key: &#34;&#34;?etcd_key: &#34;/calico-secrets/etcd-key&#34;?&#39;</span> calico-etcd.yaml
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>若主机存在多个网卡设备，calico 可能绑定错误的网卡，需要在清单文件中添加 <code>IP_AUTODETECTION_METHOD</code> 环境变量，因为本文没有使用 ipv6 所以没有配置 <code>IP6_AUTODETECTION_METHOD</code>。</p>
<blockquote>
<p>calico 官网环境变量参考：<a href="https://docs.projectcalico.org/v3.9/reference/node/configuration#environment-variables">https://docs.projectcalico.org/v3.9/reference/node/configuration#environment-variables</a><br>
自动检测 ip 机制参考：<a href="https://docs.projectcalico.org/v3.9/reference/node/configuration#ip-autodetection-methods">https://docs.projectcalico.org/v3.9/reference/node/configuration#ip-autodetection-methods</a></p>
</blockquote>
<p><strong>IP_AUTODETECTION_METHOD</strong>: The method to use to autodetect the IPv4 address for this host. This is only used when the IPv4 address is being autodetected. See <a href="https://docs.projectcalico.org/v3.9/reference/node/configuration#ip-autodetection-methods" target="_blank" rel="noopener noreffer">IP Autodetection methods</a> for details of the valid methods. [Default: <code>first-found</code>]</p>
<p><strong>IP6_AUTODETECTION_METHOD</strong>: The method to use to autodetect the IPv6 address for this host. This is only used when the IPv6 address is being autodetected. See <a href="https://docs.projectcalico.org/v3.9/reference/node/configuration#ip-autodetection-methods" target="_blank" rel="noopener noreffer">IP Autodetection methods</a> for details of the valid methods. [Default: <code>first-found</code>]</p>
<p>因为本文的 k8s 集群中不同的主机网卡名称不一样，k8s-m1、k8s-m2、k8s-m3 中每台主机存在4个网卡，网卡名为eno1、eno2、eno3、eno4。而 c48-1、c48-2 中每台主机存在2个网卡，网卡名为 eth0、eth1。所以本文使用了 <a href="https://docs.projectcalico.org/v3.9/reference/node/configuration#ip-autodetection-methods" target="_blank" rel="noopener noreffer">IP Autodetection methods</a> 中的 <code>can-reach=DESTINATION</code> 方法，绑定能抵达 192.168.15.5 （k8s-m1 的 eno1） 的网卡。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">sed -ri <span class="s1">&#39;/&#34;autodetect&#34;/a\            - name: IP_AUTODETECTION_METHOD\n              value: &#34;can-reach=192.168.15.5&#34;&#39;</span> calico-etcd.yaml
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>修改清单文件中容器的时区，使用宿主机的时区（可选）。（容器中默认的时间与北京时间相差8个小时）</p>
<p>修改后对比于原文的差异：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="o">[</span>root@k8s-m1 k8s<span class="o">]</span><span class="c1"># diff calico.yaml calico-etcd.yaml</span>
17,19c17,19
&lt;   <span class="c1"># etcd-key: null</span>
&lt;   <span class="c1"># etcd-cert: null</span>
&lt;   <span class="c1"># etcd-ca: null</span>
---
&gt;   etcd-key: <span class="nv">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</span><span class="o">=</span>
&gt;   etcd-cert: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURGVENDQWYyZ0F3SUJBZ0lKQU1OcEpWVHJBNUNDTUEwR0NTcUdTSWIzRFFFQkN3VUFNQkl4RURBT0JnTlYKQkFNTUIyVjBZMlF0WTJFd0hoY05NVGt4TURFd01UUXhPRFExV2hjTk5EY3dNakkxTVRReE9EUTFXakFXTVJRdwpFZ1lEVlFRRERBdGxkR05rTFhObGNuWmxjakNDQVNJd0RRWUpLb1pJaHZjTkFRRUJCUUFEZ2dFUEFEQ0NBUW9DCmdnRUJBT0hvRkFhU3o3b3lVSXF4UEFIVW1aVFlYS0U3dmdzbm01ajllSHI5aFkxK3oxQmNUQnZDeExlMjd1QnIKckxES1Z0TzBwaU9QL0VrZnhYZWdGcit0TmRTalhIMXdVYVVSRXdtaEJvVW52MDBacTFMT1ZobEp1RG5LSE0wQgplSmZmczl0U2RiNXpiQml4K2gvd016TzhUL3cwOStpWWVhQmlmTVllbVRLV2FQak9RQ3RLcjRpQjBPYjBWaE8yCmtkMGxuTFZDUUFRbmNhRUg0OXY1cEw3SUdOVWJSK1diWXR5eXkzRFFDL3VsZzZsZ3YySEFRdVdNQmpqZyt0eVIKK2phM0VjL2NBM1c1WVYyZjdZTzcrNUUyOExzT1YvVFdxVDA1cGNIZ2t2MGp2RHpudi9lK2o4dFh2Q3lwR1VRNgozWXdZNExiSGdlNkpLamczTG1DcEZpK0Vod0VDQXdFQUFhTnFNR2d3Q1FZRFZSMFRCQUl3QURBT0JnTlZIUThCCkFmOEVCQU1DQmFBd0hRWURWUjBsQkJZd0ZBWUlLd1lCQlFVSEF3RUdDQ3NHQVFVRkJ3TUNNQ3dHQTFVZEVRUWwKTUNPQ0NXeHZZMkZzYUc5emRJY0Vmd0FBQVljRXdLZ1BCWWNFd0tnUEJvY0V3S2dQQnpBTkJna3Foa2lHOXcwQgpBUXNGQUFPQ0FRRUFaUU1PVVRCeDZoZlBaNTFsL3BTeGtoamE4NXUwalRGOHdaK0hTYkFNRDlDNElpN1NhVlRWCmhvZkg3ZjVNUlBvMjRKSHlLZWt1RXNNYVpkRWdJUDZxeE5IbE00SkNvN3hhUGN1Y2dCV05veVQxclVrOU9oYloKTXRQbnk3SEppV3hNUWxtbzJSVk56NVVIODFXdUxXejZZOXJYcXM1M0E4YWNqZGtKaEpxRml2RUN4bllCQ0svMApjcGtxZndjS3djcitONWtJbmZjUEhpeGlWQVAvTmgxOGkyMzRZSUFYUklqQXRsZGdGdlRmMmFIb01iUUZtcG1uCnkzUzVLOWtNM01GMkVIWmMwYm5jU05hMnM5bzNGUExzVVN1ckU4emtIMWNSOXBiNDFsQlk5a1RxejgxMURLTDkKWUNQczc4TEFMVEptSS8vOWxUbFhTMm42VEtqZGxvaC9Mdz09Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K
&gt;   etcd-ca: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5akNDQWJLZ0F3SUJBZ0lKQU53d240aFNJZ3BlTUEwR0NTcUdTSWIzRFFFQkN3VUFNQkl4RURBT0JnTlYKQkFNTUIyVjBZMlF0WTJFd0hoY05NVGt4TURFd01UUXhPREV6V2hjTk5EY3dNakkxTVRReE9ERXpXakFTTVJBdwpEZ1lEVlFRRERBZGxkR05rTFdOaE1JSUJJakFOQmdrcWhraUc5dzBCQVFFRkFBT0NBUThBTUlJQkNnS0NBUUVBCndlbnhncGVwRmRjU0kzdzg4UlpNT0QrRVVHSEtjVXVmZ1ZSTXlZalJadHBDdzByU2ViWFBKMndTY2NnNVRkL3QKQXVLc3dIQXRtRWI3SjZrR3pTNGVjdXF1ZzFrMC9xY21WQWpGODVyb1UvaGVYcXZjeUx4U05ZbFpNQURURDNwLwpROUZwU3FlT2lZcVIrQjMzMWpuUmR0MXV0YTJyc2VGQTRuOVp3VXV5U2xaWEVuNGtsRXpPVUQ2MG9sVUFHUnNjCjVMSUQ0SnZwQUlTckEwd0p1NWNqbjBwRmtiZlVzUk9ueU14cDlPODh6MzhrK1RpUEN2SFBWTExKbE50cjdBSTQKUFdGRmh2UVczVjlLZEk5YWUrTXZlSXlzRHFmTnlQZ3ZWTTRJMGNJQkJ2SlV4eXVLV2lIazFCbW5lbGw1MisxTwpsaDE2V2FNSjlLZWxKVjF0SDdxeVJ3SURBUUFCb3lNd0lUQVBCZ05WSFJNQkFmOEVCVEFEQVFIL01BNEdBMVVkCkR3RUIvd1FFQXdJQ3BEQU5CZ2txaGtpRzl3MEJBUXNGQUFPQ0FRRUFqcUZUL1NxdHVGdWJsa0o4SlgxYjhtSlEKNCtsTjJIY0VibXF2dFVKViszVGk3Rk1tTFJ4SiszNFd0SW5GNUJPUVlzdFYxdEVEcmVXQjR0Z2R3MXBIOERGUApWZFNnZXZLSkFhUzNMOVRaejBobUxhOW5CQ1d0T3VLeTd3TXlxS1k1NTA2K3Vsd010aTRzY0o3bXlFcnpicWg4ClN0c3VPTTFMbkhTcC94MnVGMkVaL1Q1NG05a2RkcENXaWhXQVpUMmNVOWVlK0FUQzFzbVNIaE41TlNUZ2FJVkQKMHYvclhFWS81UVZaZFdHYmN6M21rcklISXo2LzVjaHR2Yjh2NmlpYktTbWtRMHd5TFZEVnl1Zm9pWXFodEZsQgp6bEt3dnlPMTh0SWdhOVBKdVNDbGtrbWFUaThSMXk4WWtMZzdBYkZZYnZTbUUxLzY2OXY3TjE5a2ZvVzJVZz09Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K
30c30
&lt;   etcd_endpoints: <span class="s2">&#34;http://&lt;ETCD_IP&gt;:&lt;ETCD_PORT&gt;&#34;</span>
---
&gt;   etcd_endpoints: <span class="s2">&#34;https://192.168.15.5:2379,https://192.168.15.6:2379,https://192.168.15.7:2379&#34;</span>
33,35c33,35
&lt;   etcd_ca: <span class="s2">&#34;&#34;</span>   <span class="c1"># &#34;/calico-secrets/etcd-ca&#34;</span>
&lt;   etcd_cert: <span class="s2">&#34;&#34;</span> <span class="c1"># &#34;/calico-secrets/etcd-cert&#34;</span>
&lt;   etcd_key: <span class="s2">&#34;&#34;</span>  <span class="c1"># &#34;/calico-secrets/etcd-key&#34;</span>
---
&gt;   etcd_ca: <span class="s2">&#34;/calico-secrets/etcd-ca&#34;</span>   <span class="c1"># &#34;/calico-secrets/etcd-ca&#34;</span>
&gt;   etcd_cert: <span class="s2">&#34;/calico-secrets/etcd-cert&#34;</span> <span class="c1"># &#34;/calico-secrets/etcd-cert&#34;</span>
&gt;   etcd_key: <span class="s2">&#34;/calico-secrets/etcd-key&#34;</span>  <span class="c1"># &#34;/calico-secrets/etcd-key&#34;</span>
303a304,305
&gt;             - name: IP_AUTODETECTION_METHOD
&gt;               value: <span class="s2">&#34;can-reach=192.168.15.5&#34;</span>
317c319
&lt;               value: <span class="s2">&#34;192.168.0.0/16&#34;</span>
---
&gt;               value: <span class="s2">&#34;172.30.0.0/16&#34;</span>
368a371,372
&gt;             - name: host-time
&gt;               mountPath: /etc/localtime
406a411,414
&gt;         <span class="c1"># Used for time synchronization</span>
&gt;         - name: host-time
&gt;           hostPath:
&gt;             path: /etc/localtime
490a499,500
&gt;             - name: host-time
&gt;               mountPath: /etc/localtime
503c513,516
&lt;
---
&gt;         <span class="c1"># Used for time synchronization</span>
&gt;         - name: host-time
&gt;           hostPath:
&gt;             path: /etc/localtime
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>使用如下命令执行清单文件</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">kubectl apply -f calico-etcd.yaml
</code></pre></td></tr></table>
</div>
</div></li>
</ol>
<h3 id="coredns-dns-and-service-discovery">CoreDNS: DNS and Service Discovery</h3>
<p>1.11 后 CoreDNS 已取代 Kube DNS 作为集群服务发现元件，由于 Kubernetes 需要让 Pod 与 Pod 之间能够互相通信，要能够通信需要知道彼此的 IP，通常是通过 Kubernetes API 来获取，但是 Pod IP 会因为生命周期变化而改变，因此这种做法无法弹性使用，且还会增加 API Server 负担，基于此问题 Kubernetes 提供了 DNS 服务来作为查询，让 Pod 能够以 Service 名称作为域名来查询 IP 位址，因此使用者就再不需要关心实际 Pod IP，而且 DNS 也会根据 Pod 变化更新资源记录（Record resources）。</p>
<ol>
<li>
<p>执行如下命令，修改 coredns.yaml 文件中的 dns 信息和镜像名（k8s.gcr.io的镜像需要翻墙才能下载）。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="nb">source</span> /home/k8s/env.sh
sed -i -e <span class="s2">&#34;s/__PILLAR__DNS__DOMAIN__/</span><span class="si">${</span><span class="nv">CLUSTER_DNS_DOMAIN</span><span class="si">}</span><span class="s2">/&#34;</span> -e <span class="s2">&#34;s/__PILLAR__DNS__SERVER__/</span><span class="si">${</span><span class="nv">CLUSTER_DNS_SVC_IP</span><span class="si">}</span><span class="s2">/&#34;</span> -e <span class="s2">&#34;s/__PILLAR__DNS__MEMORY__LIMIT__/170Mi/&#34;</span> /home/k8s/k8s-manual-files/addons/coredns/coredns.yaml
sed -i <span class="s2">&#34;s#k8s.gcr.io/coredns:1.3.1#coredns/coredns:1.4.0#&#34;</span> /home/k8s/k8s-manual-files/addons/coredns/coredns.yaml
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>修改 coredns.yaml 中容器的时区，使用宿主机的时区，从而使时间和宿主机保持一致（可选）。修改后与原文对比差异如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="o">[</span>root@k8s-m1 coredns<span class="o">]</span><span class="c1"># diff coredns.yaml coredns.yaml.base</span>
67c67
&lt;         kubernetes cluster.local in-addr.arpa ip6.arpa <span class="o">{</span>
---
&gt;         kubernetes __PILLAR__DNS__DOMAIN__ in-addr.arpa ip6.arpa <span class="o">{</span>
119c119
&lt;         image: coredns/coredns:1.4.0
---
&gt;         image: k8s.gcr.io/coredns:1.3.1
123c123
&lt;             memory: 170Mi
---
&gt;             memory: __PILLAR__DNS__MEMORY__LIMIT__
132,134d131
&lt;         - name: host-time
&lt;           mountPath: /etc/localtime
&lt;           readOnly: <span class="nb">true</span>
175,177d171
&lt;         - name: host-time
&lt;           hostPath:
&lt;             path: /etc/localtime
195c189
&lt;   clusterIP: 10.96.0.10
---
&gt;   clusterIP: __PILLAR__DNS__SERVER__
</code></pre></td></tr></table>
</div>
</div><blockquote>
<p>coredns.yaml 原文（coredns.yaml.base）地址：<a href="https://github.com/kubernetes/kubernetes/blob/release-1.15/cluster/addons/dns/coredns/coredns.yaml.base">https://github.com/kubernetes/kubernetes/blob/release-1.15/cluster/addons/dns/coredns/coredns.yaml.base</a></p>
</blockquote>
</li>
<li>
<p>执行 coredns.yaml 清单文件，安装 coredns</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">kubectl apply -f /home/k8s/k8s-manual-files/addons/coredns/coredns.yaml
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>检查 coredns 是否启动成功</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="o">[</span>root@k8s-m1 coredns<span class="o">]</span><span class="c1"># kubectl get pods --all-namespaces</span>
NAMESPACE     NAME                                      READY   STATUS    RESTARTS   AGE
kube-system   calico-kube-controllers-c84b7d67c-hbb65   1/1     Running   <span class="m">0</span>          53m
kube-system   calico-node-8gxlw                         1/1     Running   <span class="m">0</span>          53m
kube-system   calico-node-95ltb                         1/1     Running   <span class="m">0</span>          53m
kube-system   calico-node-9s7jk                         1/1     Running   <span class="m">0</span>          53m
kube-system   calico-node-ntbnw                         1/1     Running   <span class="m">0</span>          52m
kube-system   calico-node-t2tfx                         1/1     Running   <span class="m">0</span>          53m
kube-system   coredns-677dfcd7d8-qcqfw                  1/1     Running   <span class="m">0</span>          2m42s
kube-system   kube-proxy-52hkz                          1/1     Running   <span class="m">0</span>          10h
kube-system   kube-proxy-54f8d                          1/1     Running   <span class="m">0</span>          10h
kube-system   kube-proxy-757j7                          1/1     Running   <span class="m">0</span>          10h
kube-system   kube-proxy-c2ht9                          1/1     Running   <span class="m">0</span>          10h
kube-system   kube-proxy-fdx48                          1/1     Running   <span class="m">0</span>          10h
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>测试 coredns 是否正常工作
新建一个 dnstool 的 pod，执行如下命令</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">cat<span class="s">&lt;&lt;EOF | kubectl apply -f -
</span><span class="s">apiVersion: v1
</span><span class="s">kind: Pod
</span><span class="s">metadata:
</span><span class="s">  name: busybox
</span><span class="s">  namespace: default
</span><span class="s">spec:
</span><span class="s">  containers:
</span><span class="s">  - name: busybox
</span><span class="s">    image: busybox:1.28
</span><span class="s">    command:
</span><span class="s">      - sleep
</span><span class="s">      - &#34;3600&#34;
</span><span class="s">    imagePullPolicy: IfNotPresent
</span><span class="s">  restartPolicy: Always
</span><span class="s">EOF</span>
</code></pre></td></tr></table>
</div>
</div><p>该 pod 启动成功后，执行如下命令，查看是否能解析到对应的 ip 地址</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="o">[</span>root@k8s-m1 coredns<span class="o">]</span><span class="c1"># kubectl get pods</span>
NAME      READY   STATUS    RESTARTS   AGE
busybox   1/1     Running   <span class="m">0</span>          2m7s
<span class="o">[</span>root@k8s-m1 coredns<span class="o">]</span><span class="c1"># kubectl exec -ti busybox -- nslookup kubernetes</span>
Server:    10.96.0.10
Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local

Name:      kubernetes
Address 1: 10.96.0.1 kubernetes.default.svc.cluster.local
</code></pre></td></tr></table>
</div>
</div></li>
</ol>
<h3 id="dns-horizontal-autoscaler">dns-horizontal-autoscaler</h3>
<p>部署 coredns 时未指定副本数，默认 1 个 coredns 的 pod 运行，可以安装 dns-horizontal-autoscaler 实现 dns 横向自动扩容。</p>
<ol>
<li>
<p>Get the name of your DNS Deployment
List the DNS deployments in your cluster in the kube-system namespace:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">kubectl get deployment -l k8s-app<span class="o">=</span>kube-dns --namespace<span class="o">=</span>kube-system
</code></pre></td></tr></table>
</div>
</div><p>The output is similar to this:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">NAME      READY   UP-TO-DATE   AVAILABLE   AGE
coredns   1/1     <span class="m">1</span>            <span class="m">1</span>           18m
</code></pre></td></tr></table>
</div>
</div><p>If you don’t see a Deployment for DNS services, you can also look for it by name:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">kubectl get deployment --namespace<span class="o">=</span>kube-system
</code></pre></td></tr></table>
</div>
</div><p>and look for a deployment named coredns or kube-dns.</p>
<p>Your scale target is</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">Deployment/&lt;your-deployment-name&gt;
</code></pre></td></tr></table>
</div>
</div><p>where <code>&lt;your-deployment-name&gt;</code> is the name of your DNS Deployment. For example, if the name of your Deployment for DNS is coredns, your scale target is Deployment/coredns.</p>
<blockquote>
<p>Note: CoreDNS is the default DNS service for Kubernetes. CoreDNS sets the label <code>k8s-app=kube-dns</code> so that it can work in clusters that originally used kube-dns.</p>
</blockquote>
<p>本文的 <strong>target</strong> 为 Deployment/coredns</p>
</li>
<li>
<p>Enable DNS horizontal autoscaling
In this section, you create a new Deployment. The Pods in the Deployment run a container based on the <code>cluster-proportional-autoscaler-amd64</code> image.</p>
<p>执行如下命令，将上一步获取的 <strong>target</strong> 更新到 dns 横向自动扩容的清单文件中，默认的镜像为 <code>k8s.gcr.io/cluster-proportional-autoscaler-amd64:1.6.0</code>，该镜像需要国内无法下载，改为<code>hekai/k8s.gcr.io_cluster-proportional-autoscaler-amd64_1.6.0</code></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">sed -ri <span class="s2">&#34;s#\{\{.Target\}\}#Deployment\/coredns#&#34;</span> /home/k8s/k8s-manual-files/addons/dns-horizontal-autoscaler/dns-horizontal-autoscaler.yaml
sed -ri <span class="s2">&#34;s#k8s.gcr.io/cluster-proportional-autoscaler-amd64:1.6.0#hekai/k8s.gcr.io_cluster-proportional-autoscaler-amd64_1.6.0#&#34;</span> /home/k8s/k8s-manual-files/addons/dns-horizontal-autoscaler/dns-horizontal-autoscaler.yaml
</code></pre></td></tr></table>
</div>
</div><p>使用 hostpath 将宿主机时区文件 /etc/localetime 挂载到容器中，使容器内的时间和宿主机时间保持同步（可选），修改后与原文件对比差异如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="o">[</span>root@k8s-m1 dns-horizontal-autoscaler<span class="o">]</span><span class="c1"># diff dns-horizontal-autoscaler.yaml dns-horizontal-autoscaler.yaml.orig</span>
88c88
&lt;         image: hekai/k8s.gcr.io_cluster-proportional-autoscaler-amd64_1.6.0
---
&gt;         image: k8s.gcr.io/cluster-proportional-autoscaler-amd64:1.6.0
98c98
&lt;           - --target<span class="o">=</span>Deployment/coredns
---
&gt;           - --target<span class="o">={{</span>.Target<span class="o">}}</span>
104,107d103
&lt;         volumeMounts:
&lt;         - name: host-time
&lt;           mountPath: /etc/localtime
&lt;           readOnly: <span class="nb">true</span>
112,115d107
&lt;       volumes:
&lt;       - name: host-time
&lt;         hostPath:
&lt;           path: /etc/localtime
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>执行如下命令新建 dns 横向自动扩容的 Deployment</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">kubectl apply -f /home/k8s/k8s-manual-files/addons/dns-horizontal-autoscaler/dns-horizontal-autoscaler.yaml
</code></pre></td></tr></table>
</div>
</div></li>
</ol>
<blockquote>
<p>更多操作请参考官方文档：<a href="https://kubernetes.io/docs/tasks/administer-cluster/dns-horizontal-autoscaling/">https://kubernetes.io/docs/tasks/administer-cluster/dns-horizontal-autoscaling/</a></p>
</blockquote>
<h3 id="metrics-server">metrics-server</h3>
<p><a href="https://github.com/kubernetes-incubator/metrics-server" target="_blank" rel="noopener noreffer">Metrics Server</a> 是实现了 Metrics API 的元件,其目标是取代 Heapster 作为 Pod 与 Node 提供资源的 Usage metrics，该元件会从每个 Kubernetes 节点上的 Kubelet 所公开的 Summary API 中收集 Metrics</p>
<ul>
<li>Horizontal Pod Autoscaler（HPA）控制器用于实现基于CPU使用率进行自动Pod伸缩的功能。</li>
<li>HPA 控制器基于 Master 的 kube-controller-manager 服务启动参数 –horizontal-pod-autoscaler-sync-period 定义是时长（默认30秒），周期性监控目标 Pod 的 CPU 使用率，并在满足条件时对 ReplicationController 或 Deployment 中的 Pod 副本数进行调整,以符合用户定义的平均 Pod CPU 使用率。</li>
<li>在新版本的 kubernetes 中 Pod CPU 使用率不在来源于 heapster，而是来自于 metrics-server，官网原话是 The –horizontal-pod-autoscaler-use-rest-clients is true or unset. Setting this to false switches to Heapster-based autoscaling, which is deprecated.</li>
<li>yml 文件来自于github <a href="https://github.com/kubernetes-incubator/metrics-server/tree/master/deploy/1.8+">https://github.com/kubernetes-incubator/metrics-server/tree/master/deploy/1.8+</a></li>
</ul>
<p>若启用 metrics-server，则 kube-apiserver 的 systemed unit 文件（kube-apiserver.service）需要配置如下参数（本文默认已配置）：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">  --requestheader-client-ca-file<span class="o">=</span>/etc/kubernetes/pki/front-proxy-ca.pem  <span class="se">\
</span><span class="se"></span>  --proxy-client-cert-file<span class="o">=</span>/etc/kubernetes/pki/front-proxy-client.pem  <span class="se">\
</span><span class="se"></span>  --proxy-client-key-file<span class="o">=</span>/etc/kubernetes/pki/front-proxy-client-key.pem  <span class="se">\
</span><span class="se"></span>  --requestheader-allowed-names<span class="o">=</span>aggregator  <span class="se">\
</span><span class="se"></span>  --requestheader-group-headers<span class="o">=</span>X-Remote-Group  <span class="se">\
</span><span class="se"></span>  --requestheader-extra-headers-prefix<span class="o">=</span>X-Remote-Extra-  <span class="se">\
</span><span class="se"></span>  --requestheader-username-headers<span class="o">=</span>X-Remote-User  <span class="se">\
</span></code></pre></td></tr></table>
</div>
</div><ol>
<li>
<p>官方 metric-server 使用的镜像为 <code>k8s.gcr.io/metrics-server-amd64:v0.3.5</code>，该镜像在国内无法下载，替换为 <code>hekai/k8s.gcr.io_metrics-server-amd64_v0.3.5</code></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">sed -ri <span class="s2">&#34;s#k8s.gcr.io/metrics-server-amd64:v0.3.5#hekai/k8s.gcr.io_metrics-server-amd64_v0.3.5#&#34;</span> /home/k8s/k8s-manual-files/addons/metric-server/metrics-server-deployment.yaml
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>使用 hostpath 将宿主机时区文件 /etc/localetime  挂载到容器中，使容器内的时间和宿主机时间保持同步（可选），修改后与原文件对比差异如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="o">[</span>root@k8s-m1 metric-server<span class="o">]</span><span class="c1"># diff metrics-server-deployment.yaml metrics-server-deployment.yaml.orig</span>
30,32d29
&lt;       - name: host-time
&lt;         hostPath:
&lt;           path: /etc/localtime
35c32
&lt;         image: hekai/k8s.gcr.io_metrics-server-amd64_v0.3.5
---
&gt;         image: k8s.gcr.io/metrics-server-amd64:v0.3.5
40,42d36
&lt;         - name: host-time
&lt;           mountPath: /etc/localtime
&lt;           readOnly: <span class="nb">true</span>
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>执行如下命令，部署 metric-server</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">kubectl apply -f /home/k8s/k8s-manual-files/addons/metric-server/
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>查看 pod 的状态</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">kubectl -n kube-system get po -l k8s-app<span class="o">=</span>metrics-server
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>过一段时间就可以查看 node 的 top 信息</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">kubectl top nodes
</code></pre></td></tr></table>
</div>
</div></li>
</ol>
<h2 id="kubernetes-extra-addons">Kubernetes Extra Addons</h2>
<h3 id="kubernetes-dashboard">kubernetes-dashboard</h3>
<h4 id="阅读-dashboard-官方文档">阅读 Dashboard 官方文档</h4>
<blockquote>
<p>参考：<a href="https://github.com/kubernetes/dashboard">https://github.com/kubernetes/dashboard</a></p>
</blockquote>
<h4 id="安装-dashboard">安装 Dashboard</h4>
<blockquote>
<p>注意： k8s 1.15.0 server 包中含有 dashboard 的清单文件，位置为 <code>kubernetes/cluster/addons/dashboard</code> ，其中，使用的是 dashboard 1.10.1 版本，因为 dashboard 发布页面上显示 dashboard 1.10.1 版本不支持 k8s 1.15.0，所以此次安装 dashboard 使用的是 dashboard v2.0.0-beta4</p>
</blockquote>
<ol>
<li>
<p>在安装新Beta之前，请通过删除其名称空间来删除以前的版本：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">kubectl delete ns kubernetes-dashboard
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>执行以下命令进行安装</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta4/aio/deploy/recommended.yaml
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>访问 dashboard</p>
<p>访问方式：</p>
<ol>
<li>使用 ingress 访问（看下文）</li>
<li>使用 nodePort 访问
<ol>
<li>
<p>查看 dashboard 的 Service</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">[root@k8s-m1 ~]# kubectl get svc -n kubernetes-dashboard
NAME                        TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
dashboard-metrics-scraper   ClusterIP   10.99.202.100   &lt;none&gt;        8000/TCP   30m
kubernetes-dashboard        ClusterIP   10.103.89.91    &lt;none&gt;        443/TCP    30m
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>修改 dashboard 的 Service 的 服务类型（type）</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">[root@k8s-m1 ~]# kubectl edit svc -n kubernetes-dashboard kubernetes-dashboard
</code></pre></td></tr></table>
</div>
</div><p>将 <code>type: ClusterIP</code> 修改为 <code>type: NodePort</code></p>
</li>
<li>
<p>查看该 Service 映射的端口</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">[root@k8s-m1 ~]# kubectl get svc -n kubernetes-dashboard
NAME                        TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)         AGE
dashboard-metrics-scraper   ClusterIP   10.99.202.100   &lt;none&gt;        8000/TCP        35m
kubernetes-dashboard        NodePort    10.103.89.91    &lt;none&gt;        443:30732/TCP   35m
</code></pre></td></tr></table>
</div>
</div><p>可以看到，映射到了节点的30732端口。</p>
</li>
<li>
<p>访问 dashboard
<a href="https://192.168.15.5:30732">https://192.168.15.5:30732</a></p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/images/dashboard-nodeport-20191023.png"
        data-srcset="/images/dashboard-nodeport-20191023.png, /images/dashboard-nodeport-20191023.png 1.5x, /images/dashboard-nodeport-20191023.png 2x"
        data-sizes="auto"
        alt="/images/dashboard-nodeport-20191023.png"
        title="IMAGES" /></p>
<p>chrome 浏览器无法访问，因为 dashboard 使用的根证书在本地浏览器不存在，所以不被信任。使用 firefox 浏览器可以访问。配置 https 请查看 ingress-nginx 一节。</p>
</li>
</ol>
</li>
</ol>
</li>
<li>
<p>登录 dashboard</p>
<ol>
<li>
<p>使用 token 登录 dashboard</p>
<ol>
<li>
<p>在 dashboard 的命名空间（本文为 kubernetes-dashboard）内新建一个 sa （Service Account）</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">kubectl create sa admin-user -n kubernetes-dashboard
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>将新建的名为 dashboard-admin 的 sa 与 名为 cluster-admin 的 clusterrole 绑定</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">kubectl create clusterrolebinding admin-user --clusterrole=cluster-admin --serviceaccount=kubernetes-dashboard:admin-user
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>输出可以用来登录的 token</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">DASHBOARD_LOGIN_TOKEN=$(kubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret | grep admin-user | awk &#39;{print $1}&#39;)  | grep -E &#39;^token&#39; | awk &#39;{print $2}&#39;)
echo ${DASHBOARD_LOGIN_TOKEN}
</code></pre></td></tr></table>
</div>
</div></li>
</ol>
</li>
<li>
<p>使用 kubeconfig 登录 dashboard，需要使用上面的 token</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">source /home/k8s/env.sh
# 设置集群参数
kubectl config set-cluster kubernetes \
--certificate-authority=/etc/kubernetes/pki/ca.crt \
--embed-certs=true \
--server=${KUBE_APISERVER} \
--kubeconfig=dashboard.kubeconfig

# 设置客户端认证参数，使用上面创建的 Token
kubectl config set-credentials admin_user \
--token=${DASHBOARD_LOGIN_TOKEN} \
--kubeconfig=dashboard.kubeconfig

# 设置上下文参数
kubectl config set-context default \
--cluster=kubernetes \
--user=admin_user \
--kubeconfig=dashboard.kubeconfig

# 设置默认上下文
kubectl config use-context default --kubeconfig=dashboard.kubeconfig
</code></pre></td></tr></table>
</div>
</div></li>
</ol>
</li>
</ol>
<blockquote>
<p>https证书是 dashboard 自动生成的，因为清单文件（recommended.yaml）中配置了 <code>--auto-generate-certificates</code></p>
</blockquote>
<h3 id="helm">Helm</h3>
<h4 id="阅读-helm-官方文档">阅读 Helm 官方文档</h4>
<p><a href="/images/helm-v2.14.3-%e5%ae%98%e6%96%b9%e6%96%87%e6%a1%a3%e9%98%85%e8%af%bb%e7%ac%94%e8%ae%b0/index.html" rel="">Helm v2.14.3 官方文档阅读笔记</a></p>
<h4 id="helm-介绍">Helm 介绍</h4>
<p>helm 是 kubernetes 的包管理器。<br>
helm 通过 kubernetes 的配置文件（通常是 $HOME/.kube/config ）来断定将 tiller （helm 服务端）安装到哪个 context 中。<br>
如果想弄清楚 tiller 将要安装到哪个 context 中，可以执行 <code>kubectl config current-context</code> 或 <code>kubectl cluster-info</code> 查看。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">[root@k8s-m1 ~]# kubectl config current-context
kubernetes-admin@kubernetes
[root@k8s-m1 ~]# kubectl cluster-info
Kubernetes master is running at https://127.0.0.1:8443
CoreDNS is running at https://127.0.0.1:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
Metrics-server is running at https://127.0.0.1:8443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy

To further debug and diagnose cluster problems, use &#39;kubectl cluster-info dump&#39;.
</code></pre></td></tr></table>
</div>
</div><p>本文是在拥有全部权限，完全控制的 k8s 集群中使用 helm，即在私有网络中使用，不用考虑额外的安全配置。如果 k8s 集群暴露在一个更大的网络中，或和其他人共享集群（正式环境属于这一类），必须采取额外的步骤保护已安装的 helm ，避免其他用户无意或恶意的操作损坏 helm 或它的数据，在正式环境或多租户环境中保护 helm 的部署，参考：<a href="https://helm.sh/docs/using_helm/#securing-your-helm-installation">https://helm.sh/docs/using_helm/#securing-your-helm-installation</a></p>
<p>如果 k8s 集群中启用了 RBAC ，则需要在安装 helm 前配置 service account 。</p>
<p>本次部署默认开启了 RBAC （见 kube-apiserver.service），安装之前，需要配置 service account</p>
<h4 id="安装-helm">安装 Helm</h4>
<h5 id="安装-helm-客户端">安装 helm 客户端</h5>
<ol>
<li>
<p>下载期望安装的 helm 版本，下载地址：<a href="https://github.com/helm/helm/releases">https://github.com/helm/helm/releases</a></p>
</li>
<li>
<p>将下载得到的 tar 包解压</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">tar -zxvf helm-v2.14.1-linux-amd64.tar.gz
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>将 helm 二进制文件放到可执行目录中。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">mv linux-amd64/helm /usr/local/bin/helm
</code></pre></td></tr></table>
</div>
</div></li>
</ol>
<h5 id="安装-helm-服务端tiller">安装 helm 服务端（tiller）</h5>
<ol>
<li>
<p>在 namespace 为 kube-system 中新建一个 sa （Service Account），名为 tiller</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">kubectl -n kube-system create sa tiller
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>将上一步新建的 sa 与名为 cluster-admin 的clusterrole 进行绑定（clusterrolebinding），该绑定（clusterrolebinding）名为 tiller</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller
</code></pre></td></tr></table>
</div>
</div><blockquote>
<p>Note: 名为 cluster-admin 的 clusterrole 是 kubernetes 集群默认创建的，不需要手动创建。</p>
</blockquote>
</li>
<li>
<p>使用 新建的 sa 安装 helm 服务端（tiller）</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">helm init --service-account tiller --history-max 200 --tiller-image hekai/gcr.io_kubernetes-helm_tiller_v2.14.1
</code></pre></td></tr></table>
</div>
</div><blockquote>
<p>Note: <code>tiller</code> 默认会被安装到 <code>kube-system</code> 命名空间内。默认情况下，<code>tiller</code> 会使用 <code>gcr.io/kubernetes-helm/tiller:v2.14.1</code> 镜像，由于国内墙的原因，该镜像无法下载，可以使用 <code>--tiller-image</code> 参数指定 <code>tiller</code> 的镜像，此处使用 <code>hekai/gcr.io_kubernetes-helm_tiller_v2.14.1</code>
官网推荐初始化 <code>helm</code> 时使用 <code>--history-max</code> 参数，因为在 helm 的历史记录中，configmaps 和其他对象如果在初始化时不指定 <code>--history-max</code> 就会无限地增大，不方便维护。</p>
</blockquote>
</li>
<li>
<p>查看 tiller 是否安装成功</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">$ kubectl get pods -n kube-system -l app=helm
NAME                            READY   STATUS    RESTARTS   AGE
tiller-deploy-799f6d8f7-dsv4r   1/1     Running   0          40s
$ helm version
Client: &amp;version.Version{SemVer:&#34;v2.14.1&#34;, GitCommit:&#34;5270352a09c7e8b6e8c9593002a73535276507c0&#34;, GitTreeState:&#34;clean&#34;}
Server: &amp;version.Version{SemVer:&#34;v2.14.1&#34;, GitCommit:&#34;5270352a09c7e8b6e8c9593002a73535276507c0&#34;, GitTreeState:&#34;clean&#34;}
</code></pre></td></tr></table>
</div>
</div></li>
</ol>
<h3 id="metallb">MetalLB</h3>
<h4 id="阅读-metallb官方文档">阅读 MetalLB官方文档</h4>
<p><a href="/images/metallb-v0.8.1-%e5%ae%98%e6%96%b9%e6%96%87%e6%a1%a3%e9%98%85%e8%af%bb%e7%ac%94%e8%ae%b0/index.html" rel="">MetalLB-v0.8.1-官方文档阅读笔记</a></p>
<h4 id="metallb-介绍">MetalLB 介绍</h4>
<p>MetalLB 是为裸机部署的 k8s 集群提供的使用标准路由协议的负载均衡器（load-balancer）的实现。</p>
<p>Kubernetes 没有为 k8s 的裸机集群提供网络负载均衡器（对应的 Service 的类型为 LoadBalancer）的实现，Kubernetes 实现的网络负载均衡器都是调用各种 Iaas 平台的粘合代码（代码耦合性很高），如果你没有运行在支持 load-balancer 的 Iaas 平台上，LoadBalancers 被创建后将会一直处于 pending 状态。</p>
<p>裸机部署的 k8s 集群操作者只剩下两种方式将用户的流量引入集群内部， NodePort 和 externalIPs 服务（还有 ingress 呢），这两种方式如果在生产环境使用都有很大的缺点，从而使裸机部署的 k8s 集群成为 k8s 生态系统的二等公民。</p>
<p>MetalLB 通过提供一个整合标准网络设备的网络负载均衡器的实现纠正了这种不平衡，因此在裸机部署的集群外的服务也能够像实现了网络负载均衡器的 Iaas 平台一样工作。</p>
<p>一旦 MetalLB 为服务分配了外部 IP 地址，它就需要使群集之外的网络意识到该 IP 在群集中“存在”。 MetalLB 使用标准路由协议来实现此目的：ARP，NDP或BGP。</p>
<p>2层协议模式（ARP/NDP）: 在 2 层协议模式中，集群中的一台机器拥有服务的所有权，并且使用标准地址发现协议（ipv4 使用 ARP，ipv6 使用 NDP）来让那些 ip 能够在本地网络中可达。从局域网的角度看，该机器上有多个 IP 地址。</p>
<p>优点：2层协议模式的优势是它的通用性，它可以在任何以太网网络上运行，不需要特殊的硬件，甚至不需要特殊的路由器。在2层协议模式中，所有外部服务的流量都会流向一个节点（机器），在该节点上，k8s 的 kube-proxy 组件将流量转发到对应服务的 pods 中。这就意味着，2层协议模式没有实现负载均衡器。相反地，它实现了一个故障转移的机制，以便当前节点如果因为某些原因发生故障时，其他节点可以接替工作。如果 leader 节点因为某些原因发生故障，故障转移会自动进行：旧 leader 的租约超过 10 秒的超时时间后，另一个节点会变成 leader 并且接管服务 ip 的所有权（和 keepalived 比较相似）。</p>
<p>缺点：2层协议模式有两个主要的限制需要被注意，单节点瓶颈和潜在的缓慢故障转移。就像上面描述的一样，2层协议模式中，被选举出的单节点的 leader 会接收该服务ip的所有的流量，这意味着服务的带宽被单节点的带宽限制，这个是使用 ARP 和 NDP 引导流量的基本限制。在当前的 MetalLB 实现中，节点间的故障转移依靠客户端的协作。当一个故障转移发生时， MetalLB 会发送一些2层协议的 gratuitou 数据包通知客户端，让客户端知道和服务 ip 关联的 mac 地址已经发生改变。大多数操作系统知道如何正确的处理gratuitou数据包并且正确地更新缓存，在这种情况下，故障转移可以在几秒内完成。然而，一些系统没有实现gratuitou数据包的处理机制，或者存在错误的操作导致更新缓存缓慢。所有的主流的操作系统(Windows, Mac, Linux）都正确地实现了二层故障转移，所以那些版本较老或使用较少的操作系统可能会存在问题。</p>
<p>BGP 模式：在 BGP 模式中，机器中的所有机器都会与您控制的附近的路由器建立 BGP 对等会话，并且告诉这些路由器如何转发服务的 ip。使用 BGP 模式能够实现真正的不同节点的负载均衡，并实现细粒度的流量控制。</p>
<p>优点：假设您的路由器配置为支持多路径，这将实现真正的负载平衡：MetalLB发布的路由彼此等效，除了它们的下一跳。 这意味着路由器将一起使用所有下一跳，并在它们之间进行负载平衡。数据包到达节点后，kube-proxy负责流量路由的最后一跳，以将数据包到达服务中的一个特定容器。负载平衡的确切实现取决于您的特定路由器模式和配置，但是常见的行为是基于数据包哈希值来平衡每个连接的流量。每个连接意味着一个单独的 tcp 或 udp 会话的所有的数据包都会定向到机器中的某一个单独机器上。流量分配仅在不同的连接进行，而不是在同一个连接的不同包。这个是一个很好的设计，因为在不同的集群节点中分发数据包将会导致在多个级别上产生不良的行为：1. 将单个连接分布在多个路径上会导致数据包在网络上重新排序，这将严重影响最终主机的性能。2. Kubernetes中单节点流量路由在各个节点之间不保证是一致的。 这意味着两个不同的节点可能决定将同一连接的数据包路由到不同的Pod，这将导致连接失败</p>
<p>缺点：将BGP用作负载平衡机制的优势在于，您可以使用标准路由器硬件，而不是定制的负载平衡器。 但是，这也有缺点。最大的问题是，基于BGP的负载平衡无法对地址后端集的更改做出优雅的响应。 这意味着，当群集节点出现故障时，您应该想到与你的服务的所有活动连接被断开（用户将看到“对等方重置连接”）。基于BGP的路由器实现无状态负载平衡。 他们通过散列数据包头中的某些字段，并使用该散列作为可用后端数组的索引，将给定的数据包分配给特定的下一跳。问题在于路由器中使用的哈希值通常不稳定，因此，只要后端集的大小发生变化（例如，当节点的BGP会话断开时），现有连接就会随机有效地进行哈希刷新，这意味着大多数现有连接会突然结束被转发到另一后端，该后端不知道所讨论的连接。这样做的结果是，每当服务的IP→节点映射发生更改时，您都应该会看到连接到服务的大多数活动的连接断开。 没有持续的数据包丢失或黑洞，只有一次干净的清理。BGP 模式无法与其他的 BGP 兼容（如 calico 的 bgp 模式）</p>
<h4 id="安装-metallb">安装 MetalLB</h4>
<ol>
<li>
<p>安装前检查是否满足要求</p>
<ul>
<li>一个版本最低为1.13.0的没有网络负载均衡器的 kubernetes 集群</li>
<li>一个能与 MetalLB 共存的<a href="https://metallb.universe.tf/installation/network-addons/" target="_blank" rel="noopener noreffer">集群网络配置</a></li>
<li>一些 ipv4 地址供 MetalLB 使用</li>
<li>根据操作模式，您可能需要一个或多个能够使用BGP的路由器。</li>
</ul>
<blockquote>
<p>本文中 calico 使用的是默认的 IPIP 模式</p>
</blockquote>
</li>
<li>
<p>执行清单文件，在 metallb-system 的命名空间内安装 MetalLB</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">kubectl apply -f https://raw.githubusercontent.com/google/metallb/master/manifests/metallb.yaml
</code></pre></td></tr></table>
</div>
</div><p>安装清单不包括配置文件。 MetalLB的组件仍将启动，但在定义和部署配置映射之前将保持 idle 状态。</p>
</li>
<li>
<p>定义配置文件，在配置文件中指定使用二层协议模式或者 BGP 模式，配置文件示例：<a href="https://raw.githubusercontent.com/google/metallb/master/manifests/example-config.yaml">https://raw.githubusercontent.com/google/metallb/master/manifests/example-config.yaml</a></p>
<p><strong>本文使用二层协议模式</strong></p>
<ul>
<li>
<p>二层协议模式是最简单的，适合大部分情况，不需要任何协议对应的配置，只需要 IP 即可。第2层模式不需要将IP绑定到工作节点的网络接口。 它的工作原理是直接响应本地网络上的ARP请求，从而将计算机的MAC地址提供给客户端。例如，以下配置使 MetalLB 控制从 192.168.15.144 到 192.168.15.148 的 IP ，并配置第2层模式：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">ConfigMap</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w"> </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">metallb-system</span><span class="w">
</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">config</span><span class="w">
</span><span class="w"></span><span class="nt">data</span><span class="p">:</span><span class="w">
</span><span class="w"> </span><span class="nt">config</span><span class="p">:</span><span class="w"> </span><span class="p">|</span><span class="sd">
</span><span class="sd">   address-pools:
</span><span class="sd">   - name: default
</span><span class="sd">     protocol: layer2
</span><span class="sd">     addresses:
</span><span class="sd">     - 192.168.15.144-192.168.15.148</span><span class="w">   
</span></code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>对于具有一个BGP路由器和一个IP地址范围的基本配置，您需要4条信息：</p>
<ul>
<li>MetalLB应该连接的路由器IP地址，</li>
<li>路由器的AS号，</li>
<li>MetalLB应该使用的AS号，</li>
<li>以CIDR前缀表示的IP地址范围。
例如，如果要给MetalLB提供192.168.10.0/24的范围和AS编号64500，并将其连接到AS编号为64501的地址为10.0.0.1的路由器，则配置如下所示：</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">ConfigMap</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">metallb-system</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">config</span><span class="w">
</span><span class="w"></span><span class="nt">data</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">config</span><span class="p">:</span><span class="w"> </span><span class="p">|</span><span class="sd">
</span><span class="sd">    peers:
</span><span class="sd">    - peer-address: 10.0.0.1
</span><span class="sd">      peer-asn: 64501
</span><span class="sd">      my-asn: 64500
</span><span class="sd">    address-pools:
</span><span class="sd">    - name: default
</span><span class="sd">      protocol: bgp
</span><span class="sd">      addresses:
</span><span class="sd">      - 192.168.10.0/24</span><span class="w">    
</span></code></pre></td></tr></table>
</div>
</div></li>
</ul>
</li>
<li>
<p>执行配置文件</p>
</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">kubectl apply -f config.yaml
</code></pre></td></tr></table>
</div>
</div><h4 id="测试-loadbalancer">测试 LoadBalancer</h4>
<p>新建清单文件，<code>nginx-test.yaml</code>，内容如下</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">apps/v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Deployment</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">nginx</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">selector</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">matchLabels</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">nginx</span><span class="w">
</span><span class="w">  </span><span class="nt">template</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">nginx</span><span class="w">
</span><span class="w">    </span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">      </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">nginx</span><span class="w">
</span><span class="w">        </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">nginx</span><span class="w">
</span><span class="w">        </span><span class="nt">ports</span><span class="p">:</span><span class="w">
</span><span class="w">        </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">http</span><span class="w">
</span><span class="w">          </span><span class="nt">containerPort</span><span class="p">:</span><span class="w"> </span><span class="m">80</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="nn">---</span><span class="w">
</span><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Service</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">nginx</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">ports</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">http</span><span class="w">
</span><span class="w">    </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="m">80</span><span class="w">
</span><span class="w">    </span><span class="nt">protocol</span><span class="p">:</span><span class="w"> </span><span class="l">TCP</span><span class="w">
</span><span class="w">    </span><span class="nt">targetPort</span><span class="p">:</span><span class="w"> </span><span class="m">80</span><span class="w">
</span><span class="w">  </span><span class="nt">selector</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">nginx</span><span class="w">
</span><span class="w">  </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">LoadBalancer</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>执行部署该清单文件</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">kubectl apply -f nginx-test.yaml
</code></pre></td></tr></table>
</div>
</div><p>查看是否分配了 EXTERNAL-IP</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">$ kubectl get svc
NAME         TYPE           CLUSTER-IP      EXTERNAL-IP      PORT(S)        AGE
kubernetes   ClusterIP      10.96.0.1       &lt;none&gt;           443/TCP        11d
nginx        LoadBalancer   10.109.245.49   192.168.15.144   80:32024/TCP   8m20s
</code></pre></td></tr></table>
</div>
</div><p>可以看到 Service 类型为 LoadBalancer ，也映射了宿主机的端口</p>
<p>集群内访问该地址</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">[root@c48-1 ~]# curl 192.168.15.144
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
&lt;style&gt;
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
&lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;
&lt;p&gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&lt;/p&gt;

&lt;p&gt;For online documentation and support please refer to
&lt;a href=&#34;http://nginx.org/&#34;&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;
Commercial support is available at
&lt;a href=&#34;http://nginx.com/&#34;&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;
[root@c48-1 ~]#
</code></pre></td></tr></table>
</div>
</div><p>集群外也可以直接通过 EXTERNAL-IP 访问。</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/images/metallb-test-20191022.png"
        data-srcset="/images/metallb-test-20191022.png, /images/metallb-test-20191022.png 1.5x, /images/metallb-test-20191022.png 2x"
        data-sizes="auto"
        alt="/images/metallb-test-20191022.png"
        title="images" /></p>
<p>因为 Service 也映射了端口，所以也可以通过 NodePort 访问</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/images/metallb-test2-20191022.png"
        data-srcset="/images/metallb-test2-20191022.png, /images/metallb-test2-20191022.png 1.5x, /images/metallb-test2-20191022.png 2x"
        data-sizes="auto"
        alt="/images/metallb-test2-20191022.png"
        title="images" /></p>
<p>删除测试的部署</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">kubectl delete -f nginx-test.yaml
</code></pre></td></tr></table>
</div>
</div><h3 id="ingress-nginx">ingress-nginx</h3>
<h4 id="阅读-ingress-nginx-官方文档">阅读 ingress-nginx 官方文档</h4>
<p><a href="/images/ingress-nginx-20191015-%e5%ae%98%e6%96%b9%e6%96%87%e6%a1%a3%e9%98%85%e8%af%bb%e7%ac%94%e8%ae%b0/index.html" rel="">Ingress-Nginx-20191015-官方文档阅读笔记</a></p>
<h4 id="ingress-nginx-介绍">ingress-nginx 介绍</h4>
<p>Ingress 将 k8s 集群内部的 Service 通过 HTTP 和 HTTPS 路由暴露到集群外部。 可以将Ingress配置为提供服务外部可访问的URL，负载平衡流量，终止SSL / TLS并提供基于名称的虚拟主机。 Ingress Controller 通常结合负载平衡器实现 Ingress，尽管它也可以配置边缘路由器或其他前端以帮助处理流量。</p>
<p>Ingress 不会公开任意端口或协议。 若将除 HTTP 和 HTTPS 以外的服务暴露到 Internet，通常使用 Service.Type = NodePort 或 Service.Type = LoadBalancer 类型的服务。</p>
<p>为了使 Ingress 资源正常工作，集群必须运行一个 Ingress Controller 。</p>
<p>本文使用 ingress-nginx 作为 k8s 集群的 Ingress Controller 。</p>
<p>ingress-nginx 是使用 nginx 实现的 Ingress Controller ，使用 nginx 的重点是 nginx 的配置文件（nginx.conf），若 nginx 的配置文件发生变化必然会涉及 nginx configuration reload，导致 ingress controller 短暂的不可用。需要注意的是，nginx 配置文件中，上游（upstream）的变化不会导致 nginx reload（如部署的服务的 Endpoints 发生变化，不会导致 reload）</p>
<p>以下情况会导致 nginx configuration reload</p>
<ul>
<li>创建新的 Ingress</li>
<li>现有 Ingress 中添加 TLS 部分</li>
<li>Ingress annotations 的更改影响的不仅仅是上游（upstream）配置。 例如，load-balance annotation不需要重新加载。</li>
<li>从 Ingress 中添加/删除路径。</li>
<li>删除 Ingress、Service 或 Secret</li>
<li>Ingress 中缺少的一些的引用对象变为可用，例如 Service 或 Secret。</li>
<li>Secret 被更新。</li>
</ul>
<h4 id="部署-ingress-controller">部署 Ingress Controller</h4>
<h5 id="load-balancer-方式部署">load-balancer 方式部署</h5>
<p>使用 metalLB 提供的 load-balancer 部署 ingress-nginx</p>
<p><em><strong>本文使用的是该方式部署的 Ingress Controller</strong></em></p>
<ol>
<li>
<p>安装ingress-nginx 清单文件</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/mandatory.yaml
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>下载 Service 文件，将文件中的 <code>type: LoadBalancer</code> 修改为 <code>type: NodePort</code>，然后执行安装</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/provider/baremetal/service-nodeport.yaml
</code></pre></td></tr></table>
</div>
</div><p>修改 type</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">kubectl create -f service-nodeport.yaml
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>检查是否分配了 EXTERNAL-IP</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">$ kubectl get svc -n ingress-nginx
NAME            TYPE           CLUSTER-IP      EXTERNAL-IP      PORT(S)                      AGE
ingress-nginx   LoadBalancer   10.107.168.72   192.168.15.144   80:32435/TCP,443:31885/TCP   4m41s
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>新建 Ingress 文件测试</p>
<blockquote>
<p>注意 Ingress 文件的 namespace 必须与对应 service 的 namespace 保持一致，否则会报错503</p>
</blockquote>
</li>
</ol>
<h5 id="hostnetwork-方式部署">hostNetwork 方式部署</h5>
<blockquote>
<p>注意： 默认配置从所有名称空间监视Ingress对象。要更改此行为，请使用标志 &ndash;watch-namespace 将范围限制为特定的名称空间。
如果多个 Ingress 为同一主机定义了不同的路径，则 Ingress Controller 将合并这些定义。</p>
</blockquote>
<p>使用 hostnetwork 的方式进行部署，这种方法的好处是，NGINX Ingress控制器可以将端口80和443直接绑定到Kubernetes节点的网络接口，而无需NodePort Services施加额外的网络转换。此部署方法的一个主要限制是，在每个群集节点上只能调度单个NGINX Ingress控制器Pod，因为从技术上讲，在同一网络接口上多次绑定同一端口是不可能的。</p>
<p>缺点是只能部署一个 ingress controller，并且存在单点问题，如新建 ingress ，域名为 example.com，在 dns中配置域名与ip 的映射后，如配置 192.168.15.5 example.com ，若对应的ip（192.168.15.5）宕机了，会导致该ingress 不可用。</p>
<ol>
<li>
<p>下载 ingress-nginx 清单文件</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/mandatory.yaml
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>修改清单文件</p>
<ul>
<li>将 Deployment 改为使用 DaemonSet 进行部署（保证一个节点上运行一个 ingress controller 容器）</li>
<li>去除清单文件中的副本数限制（replicas: 1）</li>
<li>添加 <code>hostNetwork: true</code> ，启用 hostNetwork</li>
<li>添加 <code>dnsPolicy: ClusterFirstWithHostNet</code> ，允许 pod 使用 k8s 集群内的 dns 服务</li>
<li>去除参数 <code>- --publish-service=$(POD_NAMESPACE)/ingress-nginx</code> ，因为使用 hostNetwork 模式不需要新建 Service，hostNetwork模式下此参数会导致 Ingress 对象的 status 为空</li>
<li>添加参数 <code>- --report-node-internal-ip-address</code> ，将所有Ingress对象的状态设置为运行NGINX Ingress控制器的所有节点的内部IP地址。</li>
</ul>
<p>修改后和原清单内容对比如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="o">[</span>root@k8s-m1 ingress-nginx<span class="o">]</span><span class="c1"># diff mandatory.yaml mandatory.yaml.orig </span>
191c191
&lt; kind: DaemonSet
---
&gt; kind: Deployment
199c199
&lt;   <span class="c1">#replicas: 1</span>
---
&gt;   replicas: <span class="m">1</span>
216,217d215
&lt;       hostNetwork: <span class="nb">true</span>
&lt;       dnsPolicy: ClusterFirstWithHostNet
228c226
&lt;             <span class="c1">#- --publish-service=$(POD_NAMESPACE)/ingress-nginx</span>
---
&gt;             - --publish-service<span class="o">=</span><span class="k">$(</span>POD_NAMESPACE<span class="k">)</span>/ingress-nginx
230d227
&lt;             - --report-node-internal-ip-address
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>执行修改后的清单文件，完成 nginx-ingress 的部署</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">kubectl apply -f mandatory.yaml
</code></pre></td></tr></table>
</div>
</div></li>
</ol>
<h4 id="测试-ingress">测试 Ingress</h4>
<p>将上文中的 nginx-test.yaml 修改 Service 的 tpye ，改为 ClusterIP， ClusterIP 模式只能在集群内访问，集群外无法访问。修改后如下所示：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">apps/v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Deployment</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">nginx</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">selector</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">matchLabels</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">nginx</span><span class="w">
</span><span class="w">  </span><span class="nt">template</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">nginx</span><span class="w">
</span><span class="w">    </span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">      </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">nginx</span><span class="w">
</span><span class="w">        </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">nginx</span><span class="w">
</span><span class="w">        </span><span class="nt">imagePullPolicy</span><span class="p">:</span><span class="w"> </span><span class="l">IfNotPresent</span><span class="w">
</span><span class="w">        </span><span class="nt">ports</span><span class="p">:</span><span class="w">
</span><span class="w">        </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">http</span><span class="w">
</span><span class="w">          </span><span class="nt">containerPort</span><span class="p">:</span><span class="w"> </span><span class="m">80</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="nn">---</span><span class="w">
</span><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Service</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">nginx</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">ports</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">http</span><span class="w">
</span><span class="w">    </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="m">80</span><span class="w">
</span><span class="w">    </span><span class="nt">protocol</span><span class="p">:</span><span class="w"> </span><span class="l">TCP</span><span class="w">
</span><span class="w">    </span><span class="nt">targetPort</span><span class="p">:</span><span class="w"> </span><span class="m">80</span><span class="w">
</span><span class="w">  </span><span class="nt">selector</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">nginx</span><span class="w">
</span><span class="w">  </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">ClusterIP</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>部署该清单文件</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">kubectl create -f nginx-test.yaml
</code></pre></td></tr></table>
</div>
</div><p>查看部署是否成功</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">[root@k8s-m1 test-nginx]# kubectl get pods
NAME                     READY   STATUS    RESTARTS   AGE
nginx-6877889877-jcsrj   1/1     Running   0          56s
[root@k8s-m1 test-nginx]# kubectl get svc
NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1      &lt;none&gt;        443/TCP   12d
nginx        ClusterIP   10.96.121.19   &lt;none&gt;        80/TCP    58s
</code></pre></td></tr></table>
</div>
</div><p>新建 Ingress ，将 Service 以 http 方式暴露到集群外部，使集群外部可以访问 nginx</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">extensions/v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Ingress</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">nginx-ingress</span><span class="w">
</span><span class="w"> </span><span class="c"># namespace: kube-system</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">rules</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">host</span><span class="p">:</span><span class="w"> </span><span class="l">nginx.k8s.abc</span><span class="w">
</span><span class="w">    </span><span class="nt">http</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">paths</span><span class="p">:</span><span class="w">
</span><span class="w">      </span>- <span class="nt">backend</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">serviceName</span><span class="p">:</span><span class="w"> </span><span class="l">nginx</span><span class="w">
</span><span class="w">          </span><span class="nt">servicePort</span><span class="p">:</span><span class="w"> </span><span class="m">80</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>执行如下命令，部署该 Ingress</p>
<blockquote>
<p>注意： Ingress 的命名空间需要和 Deployment 、 Service 的命名空间保持一致。</p>
</blockquote>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">kubectl create -f test-nginx-ing.yaml
</code></pre></td></tr></table>
</div>
</div><p>过一段时间，查看 ingress 是否新建成功</p>
<blockquote>
<p>若一段时间后，该 ingress 的 ADDRESS 还是显示为空，则查看 ingress-nginx 的日志排查问题，
<code>kubectl logs -n ingress-nginx -f nginx-ingress-controller-799dbf6fbd-f5fkb</code>，nginx-ingress-controller-799dbf6fbd-f5fkb 替换为自己的 nginx-ingress-controller 对应 pod 的名字</p>
</blockquote>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">$ kubectl get ing
NAME            HOSTS              ADDRESS          PORTS   AGE
nginx-ingress   nginx.k8s.abc   192.168.15.144   80      2m
</code></pre></td></tr></table>
</div>
</div><blockquote>
<p>因为本文 ingress-nginx 使用的是 metallb 实现的 loadbalancer 部署的，所以 ADDRESS 显示的是 metallb 为 ingress-nginx 分配的 IP。</p>
</blockquote>
<p>在集群外的主机上配置 dns 映射，如 win10 系统，在 <code>C:\Windows\System32\drivers\etc\hosts</code> 文件中添加 <code>192.168.15.144  nginx.k8s.abc</code>，然后就可以在 win10 的浏览器中直接通过域名访问集群内的 nginx 服务了。</p>
<p>在浏览器中输入 <a href="http://nginx.k8s.abc">http://nginx.k8s.abc</a> ，访问成功。</p>
<h4 id="ingress-配置-https">Ingress 配置 https</h4>
<blockquote>
<p>参考：<a href="https://github.com/kubernetes/ingress-nginx/tree/master/docs/examples/tls-termination">https://github.com/kubernetes/ingress-nginx/tree/master/docs/examples/tls-termination</a></p>
</blockquote>
<p>本文使用 openssl 生成自签名证书演示。</p>
<ol>
<li>
<p>新增 openssl.cnf 配置文件，文件内容如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-txt" data-lang="txt">$ cat openssl.cnf
[ req ]
default_bits = 2048
default_md = sha256
distinguished_name = req_distinguished_name

[req_distinguished_name]

[ v3_ca ]
basicConstraints = critical, CA:TRUE
keyUsage = critical, digitalSignature, keyEncipherment, keyCertSign

[ v3_req_ingress ]
basicConstraints = CA:FALSE
keyUsage = critical, digitalSignature, keyEncipherment
extendedKeyUsage = serverAuth, clientAuth
subjectAltName = @alt_names_ingress

[ alt_names_ingress ]
DNS.1 = *.k8s.abc
DNS.2 = localhost
IP.1 = 192.168.15.5
IP.2 = 192.168.15.6
IP.3 = 192.168.15.7
IP.4 = 192.168.15.143
IP.5 = 192.168.15.161
IP.6 = 127.0.0.1
</code></pre></td></tr></table>
</div>
</div><blockquote>
<p>其中 alt_names_ingress 中指定了使用该证书的域名和 ip，若不加的话，通过该 域名或 IP 访问的时候浏览器中证书没有问题，但是还会出现警告。
域名使用的是通配符指定的，所以生成的证书适合格式为 *.k8s.abc 的域名，且能使用 ip 进行访问。</p>
</blockquote>
</li>
<li>
<p>生成CA私钥</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">openssl genrsa -out ca.key 2048
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>使用CA私钥生成CA自签名证书</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">openssl req -x509 -new -nodes -key ca.key -config openssl.cnf -subj &#34;/CN=Ingress-CA&#34; -extensions v3_ca -out ca.crt -days 10000
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>生成 ingress 私钥</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">openssl genrsa -out ingress.key 2048
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>使用 ingress 私钥生成证书签名请求</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">openssl req -new -key ingress.key -subj &#34;/CN=ingress&#34; -out ingress.csr
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>使用 CA 证书根据证书签名请求签发 ingress 证书</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">openssl x509 -in ingress.csr -req -CA ca.crt -CAkey ca.key -CAcreateserial -extensions v3_req_ingress -extfile openssl.cnf -out ingress.crt -days 10000
</code></pre></td></tr></table>
</div>
</div></li>
</ol>
<p>至此， 适合 *.k8s.abc 的 ingress 证书生成好了，下面来试用下</p>
<p>将上文中以 http 方式暴露的 nginx 服务的 ingress 修改下，使其以 https 方式暴露 nginx 服务。</p>
<p>删除上文中新建的 nginx ingress 对象</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">kubectl delete -f test-nginx-ing.yaml
</code></pre></td></tr></table>
</div>
</div><p>修改 test-nginx-ing.yaml 文件，修改后的内容如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">$ cat test-nginx-ing.yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: nginx-ingress
 # namespace: kube-system
spec:
  rules:
  - host: nginx.k8s.abc
    http:
      paths:
      - backend:
          serviceName: nginx
          servicePort: 80
  tls:
  - hosts:
    - nginx.k8s.abc
    secretName: nginx-ingress-secret
</code></pre></td></tr></table>
</div>
</div><p>创建 ingress 使用的 secret</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">kubectl create secret generic nginx-ingress-secret --from-file=tls.crt=ingress.crt --from-file=tls.key=ingress.key --from-file=ca.crt=ca.crt
</code></pre></td></tr></table>
</div>
</div><blockquote>
<p>Note: The CA Certificate must contain the trusted certificate authority chain to verify client certificates.
该 secret 的 namespace （命名空间）必须和 ingress 一致。</p>
</blockquote>
<p>创建修改后 ingress 对象</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">kubectl create -f test-nginx-ing.yaml
</code></pre></td></tr></table>
</div>
</div><p>查看生成的 ingress</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">$ kubectl get ing
NAME            HOSTS              ADDRESS          PORTS     AGE
nginx-ingress   nginx.k8s.abc   192.168.15.144   80, 443   5m42s
</code></pre></td></tr></table>
</div>
</div><p>在集群外的主机上通过 <a href="https://nginx.k8s.abc">https://nginx.k8s.abc</a> 访问集群内的 nginx 服务。</p>
<p>浏览器显示不安全，如下图</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/images/ingress-nginx-https-1-20191023.png"
        data-srcset="/images/ingress-nginx-https-1-20191023.png, /images/ingress-nginx-https-1-20191023.png 1.5x, /images/ingress-nginx-https-1-20191023.png 2x"
        data-sizes="auto"
        alt="/images/ingress-nginx-https-1-20191023.png"
        title="IMAGE" /></p>
<p>查看该网页的证书，如下图</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/images/ingress-nginx-https-2-20191023.png"
        data-srcset="/images/ingress-nginx-https-2-20191023.png, /images/ingress-nginx-https-2-20191023.png 1.5x, /images/ingress-nginx-https-2-20191023.png 2x"
        data-sizes="auto"
        alt="/images/ingress-nginx-https-2-20191023.png"
        title="IMAGE" /></p>
<p>原因是本文使用的是自签名证书配置的 https，CA证书不被浏览器信任。解决办法为将生成的CA证书（ca.crt）导入到 win10 的<code>受信任的根证书办法机构</code>中。双击 ca.crt 导入即可。</p>
<p>导入ca.crt成功后，重新打开浏览器，访问 <a href="https://nginx.k8s.abc">https://nginx.k8s.abc</a> ，就可以看到 https 证书被浏览器信任了。</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/images/ingress-nginx-https-3-20191023.png"
        data-srcset="/images/ingress-nginx-https-3-20191023.png, /images/ingress-nginx-https-3-20191023.png 1.5x, /images/ingress-nginx-https-3-20191023.png 2x"
        data-sizes="auto"
        alt="/images/ingress-nginx-https-3-20191023.png"
        title="IMAGE" /></p>
<blockquote>
<p>注意：如果生成证书时，对应的域名或 ip 不在 openssl.cnf 中配置，会出现浏览器提示 https 网站不安全，但是证书没问题的情况。</p>
</blockquote>
<p>下面介绍下 win10 系统如何删除导入的证书</p>
<p>在win10左下角的搜索框中搜索并打开 <code>mmc</code>，依次点击 <code>文件</code> -&gt; <code>添加/删除管理单元</code>，在可用的管理单元中双击证书，添加到<code>我的用户账户</code>中，点击<code>确定</code>，之后就可以对证书进行删除等操作了。</p>
<h4 id="使用-ingress-为-kubernetes-dashboard-暴露服务">使用 ingress 为 kubernetes dashboard 暴露服务</h4>
<p>上文中，kubernetes dashboard 是使用 nodeport 向集群外暴露的服务。此处介绍下使用 ingress 向集群外暴露 dashboard</p>
<p>dashboard 默认使用 https 暴露服务，它的清单文件中包含了启动参数<code>auto-generate-certificates</code>，表示使用 https 提供服务，并且自动生成证书，生成的证书存放在名为 <code>kubernetes-dashboard-certs</code> 的 secret 中。2.0版本的 dashboard 的命名空间为 kubernetes-dashboard，低版本的命名空间为 kube-system。</p>
<p>下面我们将 dashboard 自动生成的证书替换为自己的证书，简单起见，使用上文生成的 ingress 的证书。</p>
<ol>
<li>
<p>将 <code>kubernetes-dashboard</code> 命名空间中的名为<code>kubernetes-dashboard-certs</code> 的 <code>secret</code> 删掉</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">kubectl delete secret -n kubernetes-dashboard kubernetes-dashboard-certs
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>生成新的 <code>kubernetes-dashboard-certs</code></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">kubectl create secret generic kubernetes-dashboard-certs --from-file=dashboard.crt=ingress.crt --from-file=dashboard.key=ingress.key -n kubernetes-dashboard
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>重启 dashboard 的 pod ，是 dashboard 挂载新的证书</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">kubectl delete pods -n kubernetes-dashboard kubernetes-dashboard-d9fdb86c9-2w2zz
</code></pre></td></tr></table>
</div>
</div><blockquote>
<p><code>kubernetes-dashboard-d9fdb86c9-2w2zz</code> 替换为自己的 <code>dashboard</code> 的 <code>pod</code> 名。</p>
</blockquote>
</li>
<li>
<p>通过 nodeport 访问 dashboard ，就是受浏览器信任的了。</p>
<blockquote>
<p>上文中已经将 dashboard 使用的证书的 ca证书导入到win10中，所以 https 可以被信任。</p>
</blockquote>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/images/dashboard-nodeport-https-20191023.png"
        data-srcset="/images/dashboard-nodeport-https-20191023.png, /images/dashboard-nodeport-https-20191023.png 1.5x, /images/dashboard-nodeport-https-20191023.png 2x"
        data-sizes="auto"
        alt="/images/dashboard-nodeport-https-20191023.png"
        title="IMAGE" /></p>
</li>
<li>
<p>使用 ingress 暴露 dashboard 服务，首先在 kubernetes-dashboard 命名空间内生成 ingress 使用的证书</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">kubectl create secret generic dashboard-ingress-secret --from-file=tls.crt=ingress.crt --from-file=tls.key=ingress.key --from-file=ca.crt=ca.crt -n kubernetes-dashboard
</code></pre></td></tr></table>
</div>
</div><blockquote>
<p>注意： 本文中 dashboard 的 ingress 中使用的证书（在 secret 中）和 dashboard 的容器中使用的证书是同一个，如果不想使用同一个的话，需要使用同一个 CA 签发的证书。</p>
</blockquote>
</li>
<li>
<p>新建 dashboard 的 ingress 文件，内容如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">$ cat dashboard-ingress.yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: dashboard-ingress
  namespace: kubernetes-dashboard
  annotations:
    nginx.ingress.kubernetes.io/ingress.class: &#34;nginx&#34;
    nginx.ingress.kubernetes.io/backend-protocol: &#34;HTTPS&#34;
spec:
  rules:
  - host: dashboard.k8s.abc
    http:
      paths:
      - backend:
          serviceName: kubernetes-dashboard
          servicePort: 443
  tls:
  - hosts:
    - dashboard.k8s.abc
    secretName: dashboard-ingress-secret
</code></pre></td></tr></table>
</div>
</div><blockquote>
<p>注意：<br>
必须要加上 <code>annotations: nginx.ingress.kubernetes.io/backend-protocol: &quot;HTTPS&quot;</code><br>
Using <code>backend-protocol</code> annotations is possible to indicate how NGINX should communicate with the backend service. (Replaces <code>secure-backends</code> in older versions), Valid Values: HTTP, HTTPS, GRPC, GRPCS and AJP, By default NGINX uses <code>HTTP</code>.</p>
</blockquote>
</li>
<li>
<p>部署 ingress 文件</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">kubectl apply -f dashboard-ingress.yaml
</code></pre></td></tr></table>
</div>
</div><p>查看 ingress</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback"># kubectl get ing -n kubernetes-dashboard 
NAME                HOSTS                  ADDRESS          PORTS     AGE
dashboard-ingress   dashboard.k8s.abc   192.168.15.144   80, 443   25m
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>在 win10 的 hosts 文件中配置 域名与ip 的映射，然后通过域名访问dashboard</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/images/dashboard-ingress-20191023.png"
        data-srcset="/images/dashboard-ingress-20191023.png, /images/dashboard-ingress-20191023.png 1.5x, /images/dashboard-ingress-20191023.png 2x"
        data-sizes="auto"
        alt="/images/dashboard-ingress-20191023.png"
        title="IMAGES" /></p>
<p>这样手动配置 ingress 的证书还是比较麻烦的，每新增一个 ingress，都要手动在 ingress 的命名空间内新建一个包含证书的 secret，后面了解了 cert-manager 后，可以让 cert-manager 自动为我们生成证书，续约证书等。</p>
</li>
</ol>
<h3 id="cert-manager">cert-manager</h3>
<h4 id="阅读-cert-manager官方文档">阅读 cert-manager官方文档</h4>
<p><a href="/images/cert-manager-v0.11-%e5%ae%98%e6%96%b9%e6%96%87%e6%a1%a3%e9%98%85%e8%af%bb%e7%ac%94%e8%ae%b0/index.html" rel="">Cert-Manager-v0.11-官方文档阅读笔记</a></p>
<h4 id="cert-manager-介绍">cert-manager 介绍</h4>
<p>cert-manager 是一个基于 kubernetes 的证书管理控制器。它可以帮助你从不同的来源签发证书，如 Let&rsquo;s Encrypt ， HashiCorp Vault ， Venafi ，简单的签名密钥对或自签名。</p>
<p>它将确保证书有效并且是最新的，并在到期前尝试在配置的时间续订证书。</p>
<p>cert-manager 支持运行在 kubernetes 或 openshif 上，本文只介绍基于 kubernetes 的安装。</p>
<p>cert-manager 可以使用 清单文件安装 或使用 helm 安装。它利用自定义资源对象（CustomResourceDefinitions）来配置证书颁发机构和请求证书。cert-manager 部署完成后，需要你配置代表证书颁发机构的 Issuer 或 ClusterIssuer 。</p>
<p>在你可以使用 cert-manager 签发证书之前，必须配置至少一个 Issuer 或 ClusterIssuer。 Issuer 与 ClusterIssuer 的区别是 Issuer 的作用域为单个命名空间，并且只能在其命名空间内签发证书，这在多租户环境中非常有用。而 ClusterIssuer 是 Issuer 的集群范围版本，证书资源可以在任何命名空间中引用它。</p>
<p>cert-manager 支持的 Issuer 类型有：</p>
<ul>
<li>CA-颁发由X509签名密钥对签名的证书，该证书存储在Kubernetes API server 的 secret 中。</li>
<li>自签名-颁发自签名证书。</li>
<li>ACME-颁发通过对诸如 Let&rsquo;s Encrypt 之类的 ACME 服务器执行挑战验证而获得的证书。</li>
<li>Vault-颁发从配置了 Vault PKI 后端的 Vault 实例中的获得的证书。</li>
<li>Venafi-颁发从 Venafi Cloud 或 Trust Protection Platform 实例获得的证书。</li>
</ul>
<p>概念理解：</p>
<ul>
<li><a href="https://cert-manager.readthedocs.io/en/latest/reference/issuers.html" target="_blank" rel="noopener noreffer">Issuer</a>（证书颁发者）</li>
<li><a href="https://cert-manager.readthedocs.io/en/latest/reference/clusterissuers.html" target="_blank" rel="noopener noreffer">ClusterIssuers</a>（集群范围内的证书颁发者）</li>
<li><a href="https://cert-manager.readthedocs.io/en/latest/reference/certificates.html" target="_blank" rel="noopener noreffer">Certificates</a>（证书）</li>
</ul>
<p>本文只介绍 CA 和 自签名 两种 Issuer。</p>
<h5 id="配置-ca-issuers">配置 CA Issuers</h5>
<blockquote>
<p>参考：<a href="https://cert-manager.readthedocs.io/en/latest/tasks/issuers/setup-ca.html">https://cert-manager.readthedocs.io/en/latest/tasks/issuers/setup-ca.html</a></p>
</blockquote>
<p>cert-manager 可用于使用存储在 Kubernetes Secret 资源中的任意签名密钥对来获取证书。</p>
<p>本指南将向您展示如何基于存储在 secret 资源中的签名密钥对来配置和创建基于 CA 的 issuer。</p>
<ol>
<li>
<p>生成一个签名密钥对（如果你已有签名密钥对，可不执行此步骤）</p>
<p>CA Issuer不会自动为您创建和管理签名密钥对。 所以，您将需要提供自己的 CA 或使用诸如 openssl 或 cfssl 之类的工具生成自签名的 CA 。</p>
<p>本步骤将说明如何生成新的签名密钥对，但是如果你的签名密钥对是 CA ，你就可以使用自己的代替，跳过这一步。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback"># Generate a CA private key
$ openssl genrsa -out ca.key 2048

# Create a self signed Certificate, valid for 10yrs with the &#39;signing&#39; option set
$ openssl req -x509 -new -nodes -key ca.key -subj &#34;/CN=${COMMON_NAME}&#34; -days 3650 -reqexts v3_req -extensions v3_ca -out ca.crt
</code></pre></td></tr></table>
</div>
</div><p>上述命令会生成两个文件， ca.key 和 ca.crt ，分别为你的签名证书对的密钥和证书。如果你已经有了自己的签名密钥对，你应该分别命名私钥和证书为 ca.key 和 ca.crt 。</p>
</li>
<li>
<p>将签名密钥对以 secret 形式保存</p>
<p>我们将创建一个 Issuer ，它将使用此密钥对生成签名证书。 您可以在<a href="https://cert-manager.readthedocs.io/en/latest/reference/issuers.html" target="_blank" rel="noopener noreffer">Issuer参考文档</a>中阅读有关 Issuer 资源的更多信息。 为了允许 Issuer 引用签名密钥对，我们将其存储在Kubernetes Secret资源中。</p>
<p>Issuer 是命名空间范围的资源，因此它们只能在自己的命名空间中引用 Secrets。 因此，我们将密钥对放入与 Issuer 相同的名称空间中。 我们也可以创建一个ClusterIssuer，它是Issuer的群集范围版本。 有关ClusterIssuers的更多信息，请阅读<a href="https://cert-manager.readthedocs.io/en/latest/reference/clusterissuers.html" target="_blank" rel="noopener noreffer">ClusterIssuer参考文档</a>。</p>
<p>以下命令将在默认名称空间中创建一个包含签名密钥对的Secret：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">kubectl create secret tls ca-key-pair \
   --cert=ca.crt \
   --key=ca.key \
   --namespace=default
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>引用 secret 创建一个 Issuer</p>
<p>现在，我们可以创建一个引用刚刚创建的 Secret 资源的 Issuer ：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">cert-manager.io/v1alpha2</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Issuer</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">ca-issuer</span><span class="w">
</span><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">default</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">ca</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">secretName</span><span class="p">:</span><span class="w"> </span><span class="l">ca-key-pair</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>我们现在准备获得证书！</p>
</li>
<li>
<p>获取签名证书</p>
<p>现在，我们可以创建以下证书资源，以指定所需的证书。 您可以在<a href="https://cert-manager.readthedocs.io/en/latest/reference/certificates.html" target="_blank" rel="noopener noreffer">参考文档</a>中阅读有关证书资源的更多信息。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">cert-manager.io/v1alpha2</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Certificate</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">example-com</span><span class="w">
</span><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">default</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">secretName</span><span class="p">:</span><span class="w"> </span><span class="l">example-com-tls</span><span class="w">
</span><span class="w">  </span><span class="nt">issuerRef</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">ca-issuer</span><span class="w">
</span><span class="w">    </span><span class="c"># We can reference ClusterIssuers by changing the kind here.</span><span class="w">
</span><span class="w">    </span><span class="c"># The default value is Issuer (i.e. a locally namespaced Issuer)</span><span class="w">
</span><span class="w">    </span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Issuer</span><span class="w">
</span><span class="w">  </span><span class="nt">commonName</span><span class="p">:</span><span class="w"> </span><span class="l">example.com</span><span class="w">
</span><span class="w">  </span><span class="nt">organization</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="l">Example CA</span><span class="w">
</span><span class="w">  </span><span class="nt">dnsNames</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="l">example.com</span><span class="w">
</span><span class="w">  </span>- <span class="l">www.example.com</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>为了使用 Issuer 获得证书，我们必须在与 Issuer 相同的名称空间中创建证书资源，因为 Issuer 是命名空间范围的资源。 如果我们想在多个命名空间之间重用签名密钥对，则可以选择创建 ClusterIssuer 。</p>
<p>一旦我们创建了 Certificate 资源，cert-manager 将尝试使用 Issuer <code>ca-issuer</code>获取证书。 如果成功，则证书将存储在名为<code>example-com-tls</code> 的 Secret 资源中，位于与 Certificate 资源相同的名称空间中（default）。</p>
<p>上面的示例将 commonName 字段显式设置为 <code>example.com</code> 。 如果 dnsNames 字段中尚未包含 commonName 字段，则 cert-manager 会自动将 commonName 字段添加为<a href="https://en.wikipedia.org/wiki/Subject_Alternative_Name" target="_blank" rel="noopener noreffer">DNS SAN</a>。</p>
<p>如果我们未指定 commonName 字段，则将使用第一个指定的DNS SAN（在 dnsNames 下）作为证书的通用名称。</p>
<p>创建上述证书后，我们可以检查是否已成功获得证书，如下所示：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">$ kubectl describe certificate example-com
Events:
  Type     Reason                 Age              From                     Message
  ----     ------                 ----             ----                     -------
  Warning  ErrorCheckCertificate  26s              cert-manager-controller  Error checking existing TLS certificate: secret &#34;example-com-tls&#34; not found
  Normal   PrepareCertificate     26s              cert-manager-controller  Preparing certificate with issuer
  Normal   IssueCertificate       26s              cert-manager-controller  Issuing certificate...
  Normal   CertificateIssued      25s              cert-manager-controller  Certificate issued successfully
</code></pre></td></tr></table>
</div>
</div><p>你还可以通过执行 <code>kubectl get secret example-com-tls -o yaml</code> 检查 issuer 生成的证书是否成功，你应该看到一个base64编码的签名TLS密钥对。</p>
<p>一旦获得证书，cert-manager 将继续检查其有效性，并在证书即将到期时尝试对其进行更新。 当证书上的 “Not After” 字段小于当前时间加上30天时，cert-manager 认为证书即将到期。 对于基于 CA 的颁发者，cert-manager 将颁发 “Not After” 字段设置为当前时间加上365天的证书。</p>
</li>
</ol>
<h5 id="配置自签名-issuer">配置自签名 Issuer</h5>
<p>自签名 Issuers 将发行自签名证书。</p>
<p>在 Kubernetes 中构建 PKI 时，或生成与 <a href="https://cert-manager.readthedocs.io/en/latest/tasks/issuers/setup-ca.html" target="_blank" rel="noopener noreffer">CA Issuer</a> 一起使用的根 CA 的方式时，这很有用。</p>
<p>自签名颁发者不包含其他配置字段，并且可以使用如下资源创建：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">cert-manager.io/v1alpha2</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">ClusterIssuer</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">selfsigning-issuer</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">selfSigned</span><span class="p">:</span><span class="w"> </span>{}<span class="w">
</span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p><code>selfSigned: {}</code> 一行的存在足以表明该 Issuer 的类型为 “selfsigned”。</p>
</blockquote>
<p>创建后，你可以像平常一样通过在 <code>issuerRef</code> 中引用上面新建的 Issuer 来颁发证书：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">cert-manager.io/v1alpha2</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Certificate</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">example-crt</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">secretName</span><span class="p">:</span><span class="w"> </span><span class="l">my-selfsigned-cert</span><span class="w">
</span><span class="w">  </span><span class="nt">commonName</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;my-selfsigned-root-ca&#34;</span><span class="w">
</span><span class="w">  </span><span class="nt">isCA</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">  </span><span class="nt">issuerRef</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">selfsigning-issuer</span><span class="w">
</span><span class="w">    </span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">ClusterIssuer</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><h5 id="为-ingress-资源自动创建证书">为 ingress 资源自动创建证书</h5>
<p>cert-manager 可以配置为通过 Ingress 上的 annotations 自动为 Ingress 资源设置 TLS 证书。</p>
<p>cert-manager的一个子组件 ingress-shim 负责这项工作。</p>
<p>ingress-shim 监视整个集群中的 Ingress 资源。如果它观察到具有特定 annotations 的Ingress，它将确保和 ingress 中secret相同名称的并且 配置和 ingress 中描述一致的证书资源存在。例如：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">extensions/v1beta1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Ingress</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">annotations</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="c"># add an annotation indicating the issuer to use.</span><span class="w">
</span><span class="w">    </span><span class="nt">cert-manager.io/cluster-issuer</span><span class="p">:</span><span class="w"> </span><span class="l">nameOfClusterIssuer</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">myIngress</span><span class="w">
</span><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">myIngress</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">rules</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">host</span><span class="p">:</span><span class="w"> </span><span class="l">myingress.com</span><span class="w">
</span><span class="w">    </span><span class="nt">http</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">paths</span><span class="p">:</span><span class="w">
</span><span class="w">      </span>- <span class="nt">backend</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">serviceName</span><span class="p">:</span><span class="w"> </span><span class="l">myservice</span><span class="w">
</span><span class="w">          </span><span class="nt">servicePort</span><span class="p">:</span><span class="w"> </span><span class="m">80</span><span class="w">
</span><span class="w">        </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l">/</span><span class="w">
</span><span class="w">  </span><span class="nt">tls</span><span class="p">:</span><span class="w"> </span><span class="c"># &lt; placing a host in the TLS config will indicate a cert should be created</span><span class="w">
</span><span class="w">  </span>- <span class="nt">hosts</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="l">myingress.com</span><span class="w">
</span><span class="w">    </span><span class="nt">secretName</span><span class="p">:</span><span class="w"> </span><span class="l">myingress-cert</span><span class="w"> </span><span class="c"># &lt; cert-manager will store the created certificate in this secret.</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p><em><strong>Configuration</strong></em>
Since cert-manager v0.2.2, ingress-shim is deployed automatically as part of a Helm chart installation.</p>
<p>If you would also like to use the old <a href="https://github.com/jetstack/kube-lego" target="_blank" rel="noopener noreffer">kube-lego</a> <code>kubernetes.io/tls-acme: &quot;true&quot;</code> annotation for fully automated TLS, you will need to configure a default Issuer when deploying cert-manager. This can be done by adding the following <code>--set</code> when deploying using Helm:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">--set ingressShim.defaultIssuerName=letsencrypt-prod \
--set ingressShim.defaultIssuerKind=ClusterIssuer
</code></pre></td></tr></table>
</div>
</div><p>In the above example, cert-manager will create Certificate resources that reference the ClusterIssuer letsencrypt-prod for all Ingresses that have a <code>kubernetes.io/tls-acme: &quot;true&quot;</code> annotation.</p>
<p>For more information on deploying cert-manager, read the <a href="https://cert-manager.readthedocs.io/en/latest/getting-started/index.html" target="_blank" rel="noopener noreffer">deployment guide</a>.</p>
<p>kube-lego 是 cert-manager 的上一个版本，现在已经停止更新了。</p>
<p><em><strong>支持的 annotations</strong></em>
你可以在 ingress 中执行如下的 annotations 从而能够触发证书（Certificate ）资源能够被自动创建。</p>
<ul>
<li><code>cert-manager.io/issuer</code> - the name of an Issuer to acquire the certificate required for this ingress from. The Issuer <strong>must</strong> be in the same namespace as the Ingress resource.</li>
<li><code>cert-manager.io/cluster-issuer</code> - the name of a ClusterIssuer to acquire the certificate required for this ingress from. It does not matter which namespace your Ingress resides, as ClusterIssuers are non-namespaced resources.</li>
<li><code>kubernetes.io/tls-acme: &quot;true&quot;</code> - this annotation requires additional configuration of the ingress-shim (see above). Namely, a default issuer must be specified as arguments to the ingress-shim container.</li>
<li><code>acme.cert-manager.io/http01-ingress-class</code> - this annotation allows you to configure ingress class that will be used to solve challenges for this ingress. Customising this is useful when you are trying to secure internal services, and need to solve challenges using different ingress class to that of the ingress. If not specified and the ‘acme-http01-edit-in-place’ annotation is not set, this defaults to the ingress class of the ingress resource.</li>
<li><code>acme.cert-manager.io/http01-edit-in-place</code>: &ldquo;true&rdquo; - this controls whether the ingress is modified ‘in-place’, or a new one created specifically for the http01 challenge. If present, and set to “true” the existing ingress will be modified. Any other value, or the absence of the annotation assumes “false”.</li>
</ul>
<h4 id="安装-cert-manager">安装 cert-manager</h4>
<p>本文使用清单文件安装，若想使用 helm 安装，请参考<a href="https://cert-manager.readthedocs.io/en/latest/getting-started/install/kubernetes.html" target="_blank" rel="noopener noreffer">官网安装</a></p>
<ol>
<li>
<p>新建 cert-manager 命名空间，cert-manager 默认安装在名为 cert-manager 的命名空间内</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">kubectl create namespace cert-manager
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>执行清单文件，所有关于 cert-manager 的资源都定义在了清单文件中</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v0.11.0/cert-manager.yaml
</code></pre></td></tr></table>
</div>
</div><blockquote>
<p>注意：如果你运行的 k8s 版本小于或等于 v1.15，你需要在执行命令时添加 <code>--validate=false</code> ，因为本文的 k8s 版本为 v1.15.0，所以需要添加该参数<br>
cert-manager 会部署一个 webhook 作为 apiservice ，当 webhook 不在运行时但这个 apiservice 还存在的时候进行卸载 cert-manager 就会出现问题，所以卸载 cert-manager 的时候确保按照<a href="https://cert-manager.readthedocs.io/en/latest/tasks/uninstall/kubernetes.html" target="_blank" rel="noopener noreffer">卸载文档</a>进行。</p>
</blockquote>
</li>
</ol>
<h5 id="配置-ca-issuers-1">配置 CA Issuers</h5>
<p>本文使用上面[Ingress 配置 https][#Ingress 配置 https] 创建的 CA 证书来配置 ClusterIssuer 。</p>
<ol>
<li>
<p>生成签名密钥对</p>
<p>跳过，使用 [Ingress 配置 https] 中 创建的 ca.key 和 ca.crt 。</p>
</li>
<li>
<p>将签名密钥对保存为 secret</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">kubectl create secret tls ca-key-pair \
   --cert=ca.crt \
   --key=ca.key \
   --namespace=cert-manager
</code></pre></td></tr></table>
</div>
</div><blockquote>
<p>注意： 因为本次使用的是 ClusterIssuer，所以这个 secret 需要放在 cert-manager 命名空间内。若使用 Issuer，则 secret 需要放在与 Issuer 相同的命名空间内。</p>
</blockquote>
</li>
<li>
<p>新建一个 ClusterIssuer 引用上面的 secret</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">$ cat cluster-issuer.yaml
apiVersion: cert-manager.io/v1alpha2
kind: ClusterIssuer
metadata:
  name: cluster-issuer
spec:
  ca:
    secretName: ca-key-pair
</code></pre></td></tr></table>
</div>
</div><blockquote>
<p>ClusterIssuer 不用指定命名空间，Issuer 需要指定命名空间。ClusterIssuer 引用的 secret 需要在 cert-manager 命名空间内，如果 secret 在 default 命名空间内，则 ClusterIssuer 找不到 secret : <code>secret &quot;ca-key-pair&quot; not found</code></p>
</blockquote>
<p>通过如下命令查看 ClusterIssuer 的状态</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">kubectl describe ClusterIssuer cluster-issuer
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>利用 annotations ，让 cert-manager 的 ingress-shim 组件自动生成 ingress 的证书。</p>
<p>使用 dashboard 的 ingress 进行测试</p>
<ol>
<li>
<p>首先删除 dashboard 的 ingress</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">kubectl delete ing -n kubernetes-dashboard dashboard-ingress
</code></pre></td></tr></table>
</div>
</div><blockquote>
<p>dashboard-ingress 为 dashboard 的 ingress 中的 name<br>
也可以通过 dashboard 的 ingress 的 yaml 文件删除 ingress，命令为：<code>kubectl delete -f dashboard-ingress.yaml</code></p>
</blockquote>
</li>
<li>
<p>删除 dashboard ingress 的 secret</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">kubectl delete secret -n kubernetes-dashboard dashboard-ingress-secret
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>添加 annotations
修改 dashboard 的 ingress 文件，dashboard-ingress.yaml ， 添加 annotations cert-manager.io/cluster-issuer，修改后内容如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">$ cat dashboard-ingress.yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: dashboard-ingress
  namespace: kubernetes-dashboard
  annotations:
    nginx.ingress.kubernetes.io/ingress.class: &#34;nginx&#34;
    nginx.ingress.kubernetes.io/backend-protocol: &#34;HTTPS&#34;
    cert-manager.io/cluster-issuer: &#34;cluster-issuer&#34;
spec:
  rules:
  - host: dashboard.k8s.abc
    http:
      paths:
      - backend:
          serviceName: kubernetes-dashboard
          servicePort: 443
  tls:
  - hosts:
    - dashboard.k8s.abc
    secretName: dashboard-ingress-secret
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>安装 ingress</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">kubectl create -f dashboard-ingress.yaml
</code></pre></td></tr></table>
</div>
</div><p>可以看到执行完之后， kubernetes-dashboard 中新建了一个名为<code>dashboard-ingress-secret</code>的secret，通过浏览器也能够正常访问。</p>
</li>
</ol>
</li>
</ol>
<p>使用 cert-manager 自动生成 ingress 证书的好处还有，可以使用任意的域名，如果手动配置 ingress 证书，需要在证书上配置域名或 IP，如果域名不对的话，访问还是会提示不安全。使用 cert-manager 解决了这个问题，它只使用 CA 根证书，自动生成 https 的证书，可用在 ingress 里配置任意的域名。</p>
<p>以后在创建 ingress 的时候，就可以加上 annotations: <code>cert-manager.io/cluster-issuer: &quot;cluster-issuer&quot;</code>，这样 cert-manager 就能自动为我们创建 https 证书了。</p>
<h3 id="nfs-storageclass">nfs-storageclass</h3>
<h4 id="安装-nfs-服务器">安装 NFS 服务器</h4>
<p>略</p>
<h4 id="安装-nfs-客户端">安装 NFS 客户端</h4>
<ol>
<li>
<p>获取 NFS 服务器的连接信息</p>
</li>
<li>
<p>在<a href="https://github.com/kubernetes-incubator/external-storage/releases" target="_blank" rel="noopener noreffer">发布版本页面</a>下载最新的发布包</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">wget https://github.com/kubernetes-incubator/external-storage/archive/v5.5.0.tar.gz
</code></pre></td></tr></table>
</div>
</div><p>下载后解压</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">tar -zxvf external-storage-5.5.0.tar.gz
</code></pre></td></tr></table>
</div>
</div><p>进入 nfs-client 目录</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">cd external-storage-5.5.0/nfs-client/
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>设置权限和修改命名空间</p>
<p>新建 nfs-storageclass 命名空间</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">kubectl create ns nfs-storageclass
</code></pre></td></tr></table>
</div>
</div><p>修改 deploy/rbac.yaml 中的ServiceAccount，Role 等的命名空间，修改后内容如下</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">ServiceAccount</span><span class="w">
</span><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">nfs-client-provisioner</span><span class="w">
</span><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">nfs-storageclass</span><span class="w">
</span><span class="w"></span><span class="nn">---</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">ClusterRole</span><span class="w">
</span><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">rbac.authorization.k8s.io/v1</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">nfs-client-provisioner-runner</span><span class="w">
</span><span class="w"></span><span class="nt">rules</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">apiGroups</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">&#34;&#34;</span><span class="p">]</span><span class="w">
</span><span class="w">    </span><span class="nt">resources</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">&#34;persistentvolumes&#34;</span><span class="p">]</span><span class="w">
</span><span class="w">    </span><span class="nt">verbs</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">&#34;get&#34;</span><span class="p">,</span><span class="w"> </span><span class="s2">&#34;list&#34;</span><span class="p">,</span><span class="w"> </span><span class="s2">&#34;watch&#34;</span><span class="p">,</span><span class="w"> </span><span class="s2">&#34;create&#34;</span><span class="p">,</span><span class="w"> </span><span class="s2">&#34;delete&#34;</span><span class="p">]</span><span class="w">
</span><span class="w">  </span>- <span class="nt">apiGroups</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">&#34;&#34;</span><span class="p">]</span><span class="w">
</span><span class="w">    </span><span class="nt">resources</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">&#34;persistentvolumeclaims&#34;</span><span class="p">]</span><span class="w">
</span><span class="w">    </span><span class="nt">verbs</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">&#34;get&#34;</span><span class="p">,</span><span class="w"> </span><span class="s2">&#34;list&#34;</span><span class="p">,</span><span class="w"> </span><span class="s2">&#34;watch&#34;</span><span class="p">,</span><span class="w"> </span><span class="s2">&#34;update&#34;</span><span class="p">]</span><span class="w">
</span><span class="w">  </span>- <span class="nt">apiGroups</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">&#34;storage.k8s.io&#34;</span><span class="p">]</span><span class="w">
</span><span class="w">    </span><span class="nt">resources</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">&#34;storageclasses&#34;</span><span class="p">]</span><span class="w">
</span><span class="w">    </span><span class="nt">verbs</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">&#34;get&#34;</span><span class="p">,</span><span class="w"> </span><span class="s2">&#34;list&#34;</span><span class="p">,</span><span class="w"> </span><span class="s2">&#34;watch&#34;</span><span class="p">]</span><span class="w">
</span><span class="w">  </span>- <span class="nt">apiGroups</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">&#34;&#34;</span><span class="p">]</span><span class="w">
</span><span class="w">    </span><span class="nt">resources</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">&#34;events&#34;</span><span class="p">]</span><span class="w">
</span><span class="w">    </span><span class="nt">verbs</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">&#34;create&#34;</span><span class="p">,</span><span class="w"> </span><span class="s2">&#34;update&#34;</span><span class="p">,</span><span class="w"> </span><span class="s2">&#34;patch&#34;</span><span class="p">]</span><span class="w">
</span><span class="w"></span><span class="nn">---</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">ClusterRoleBinding</span><span class="w">
</span><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">rbac.authorization.k8s.io/v1</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">run-nfs-client-provisioner</span><span class="w">
</span><span class="w"></span><span class="nt">subjects</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">ServiceAccount</span><span class="w">
</span><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">nfs-client-provisioner</span><span class="w">
</span><span class="w">    </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">nfs-storageclass</span><span class="w">
</span><span class="w"></span><span class="nt">roleRef</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">ClusterRole</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">nfs-client-provisioner-runner</span><span class="w">
</span><span class="w">  </span><span class="nt">apiGroup</span><span class="p">:</span><span class="w"> </span><span class="l">rbac.authorization.k8s.io</span><span class="w">
</span><span class="w"></span><span class="nn">---</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Role</span><span class="w">
</span><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">rbac.authorization.k8s.io/v1</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">leader-locking-nfs-client-provisioner</span><span class="w">
</span><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">nfs-storageclass</span><span class="w">
</span><span class="w"></span><span class="nt">rules</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">apiGroups</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">&#34;&#34;</span><span class="p">]</span><span class="w">
</span><span class="w">    </span><span class="nt">resources</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">&#34;endpoints&#34;</span><span class="p">]</span><span class="w">
</span><span class="w">    </span><span class="nt">verbs</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">&#34;get&#34;</span><span class="p">,</span><span class="w"> </span><span class="s2">&#34;list&#34;</span><span class="p">,</span><span class="w"> </span><span class="s2">&#34;watch&#34;</span><span class="p">,</span><span class="w"> </span><span class="s2">&#34;create&#34;</span><span class="p">,</span><span class="w"> </span><span class="s2">&#34;update&#34;</span><span class="p">,</span><span class="w"> </span><span class="s2">&#34;patch&#34;</span><span class="p">]</span><span class="w">
</span><span class="w"></span><span class="nn">---</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">RoleBinding</span><span class="w">
</span><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">rbac.authorization.k8s.io/v1</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">leader-locking-nfs-client-provisioner</span><span class="w">
</span><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">nfs-storageclass</span><span class="w">
</span><span class="w"></span><span class="nt">subjects</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">ServiceAccount</span><span class="w">
</span><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">nfs-client-provisioner</span><span class="w">
</span><span class="w">    </span><span class="c"># replace with namespace where provisioner is deployed</span><span class="w">
</span><span class="w">    </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">nfs-storageclass</span><span class="w">
</span><span class="w"></span><span class="nt">roleRef</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Role</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">leader-locking-nfs-client-provisioner</span><span class="w">
</span><span class="w">  </span><span class="nt">apiGroup</span><span class="p">:</span><span class="w"> </span><span class="l">rbac.authorization.k8s.io</span><span class="w">
</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>执行如下命令部署 rbac.yaml</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">kubectl create -f deploy/rbac.yaml
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>修改 deployment.yaml 配置 NFS server 连接信息
将 deploy/deployment.yaml 中的 <code>&lt;YOUR NFS SERVER HOSTNAME&gt;</code> 修改为 nfs server 的 hostname 或 ip，同时也修改下<code>NFS_PATH</code> 和 <code>PROVISIONER_NAME</code> 对应的值。若修改 <code>deploy/deployment.yaml</code> 中 <code>PROVISIONER_NAME</code> 对应的值，必须同时修改下 <code>deploy/class.yaml</code> 中 <code>provisioner</code> 的值，使两个值保持一致。</p>
<p>因为修改了默认的命名空间，默认命名空间为 default，改为了 nfs-storageclass，所以，也需要修改下 deploy/deployment.yaml 中 Deployment 和 ServiceAccount 的命名空间。</p>
<p>deploy/class.yaml 这个 StorageClass 定义文件中，添加注解 <code>storageclass.kubernetes.io/is-default-class: &quot;true&quot;</code>， 该注解将该 storageclass 配置为默认的 StorageClass。参考：<a href="https://kubernetes.io/docs/tasks/administer-cluster/change-default-storage-class/">https://kubernetes.io/docs/tasks/administer-cluster/change-default-storage-class/</a></p>
<p>nfs server 地址为 <code>192.168.15.143</code>， 路径为 <code>/home/data/nfs/data</code></p>
<p>修改后的 deploy/deployment.yaml 内容如下</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="c">#apiVersion: v1</span><span class="w">
</span><span class="w"></span><span class="c">#kind: ServiceAccount</span><span class="w">
</span><span class="w"></span><span class="c">#metadata:</span><span class="w">
</span><span class="w"></span><span class="c">#  name: nfs-client-provisioner</span><span class="w">
</span><span class="w"></span><span class="c">#  namespace: nfs-storageclass</span><span class="w">
</span><span class="w"></span><span class="nn">---</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Deployment</span><span class="w">
</span><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">extensions/v1beta1</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">nfs-client-provisioner</span><span class="w">
</span><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">nfs-storageclass</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">replicas</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">  </span><span class="nt">strategy</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">Recreate</span><span class="w">
</span><span class="w">  </span><span class="nt">template</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">nfs-client-provisioner</span><span class="w">
</span><span class="w">    </span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">serviceAccountName</span><span class="p">:</span><span class="w"> </span><span class="l">nfs-client-provisioner</span><span class="w">
</span><span class="w">      </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">        </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">nfs-client-provisioner</span><span class="w">
</span><span class="w">          </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">quay.io/external_storage/nfs-client-provisioner:latest</span><span class="w">
</span><span class="w">          </span><span class="nt">volumeMounts</span><span class="p">:</span><span class="w">
</span><span class="w">            </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">nfs-client-root</span><span class="w">
</span><span class="w">              </span><span class="nt">mountPath</span><span class="p">:</span><span class="w"> </span><span class="l">/persistentvolumes</span><span class="w">
</span><span class="w">          </span><span class="nt">env</span><span class="p">:</span><span class="w">
</span><span class="w">            </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">PROVISIONER_NAME</span><span class="w">
</span><span class="w">              </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="l">nfs-storage</span><span class="w">
</span><span class="w">            </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">NFS_SERVER</span><span class="w">
</span><span class="w">              </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="m">192.168.15.143</span><span class="w">
</span><span class="w">            </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">NFS_PATH</span><span class="w">
</span><span class="w">              </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="l">/home/data/nfs/data</span><span class="w">
</span><span class="w">      </span><span class="nt">volumes</span><span class="p">:</span><span class="w">
</span><span class="w">        </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">nfs-client-root</span><span class="w">
</span><span class="w">          </span><span class="nt">nfs</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="nt">server</span><span class="p">:</span><span class="w"> </span><span class="m">192.168.15.143</span><span class="w">
</span><span class="w">            </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l">/home/data/nfs/data</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>注释掉了 ServiceAccount，因为在 deploy/rbac.yaml 中已经定义了该 ServiceAccount</p>
<p>修改后的 deploy/class.yaml 内容如下</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">StorageClass</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">managed-nfs-storage</span><span class="w">
</span><span class="w">  </span><span class="nt">annotations</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">storageclass.kubernetes.io/is-default-class</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;true&#34;</span><span class="w"> </span><span class="c"># 将该sc配置为默认的sc</span><span class="w">
</span><span class="w"></span><span class="nt">provisioner</span><span class="p">:</span><span class="w"> </span><span class="l">nfs-storage</span><span class="w"> </span><span class="c"># or choose another name, must match deployment&#39;s env PROVISIONER_NAME&#39;</span><span class="w">
</span><span class="w"></span><span class="nt">parameters</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">archiveOnDelete</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;false&#34;</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>部署</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">kubectl create -f deploy/deployment.yaml -f deploy/class.yaml
</code></pre></td></tr></table>
</div>
</div><p>查看安装是否成功</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">$ kubectl get pods -n nfs-storageclass
NAME                                      READY   STATUS    RESTARTS   AGE
nfs-client-provisioner-699948c999-45p56   1/1     Running   0          108s
$ kubectl get sc
NAME                            PROVISIONER   AGE
managed-nfs-storage (default)   nfs-storage   2m37s
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>测试</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">kubectl create -f deploy/test-claim.yaml -f deploy/test-pod.yaml
</code></pre></td></tr></table>
</div>
</div><p>查看 pvc 是否挂载成功</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">[root@k8s-m1 nfs-client]# kubectl get pvc
NAME         STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS          AGE
test-claim   Bound    pvc-bf3135ac-a04d-418a-9a87-675bef49c022   1Mi        RWX            managed-nfs-storage   4m53s
</code></pre></td></tr></table>
</div>
</div><p>挂载成功，删除测试部署</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">kubectl delete -f deploy/test-claim.yaml -f deploy/test-pod.yaml
</code></pre></td></tr></table>
</div>
</div></li>
</ol>
<h3 id="harbor">Harbor</h3>
<h4 id="阅读-harbor-官方文档">阅读 Harbor 官方文档</h4>
<p><a href="/images/harbor-v1.9.1-%e5%ae%98%e6%96%b9%e6%96%87%e6%a1%a3%e9%98%85%e8%af%bb%e7%ac%94%e8%ae%b0/index.html" rel="">Harbor-v1.9.1-官方文档阅读笔记</a></p>
<h4 id="安装-harbor">安装 Harbor</h4>
<p>本文使用 helm 安装 Harbor，使用 nfs-storageclass 作为 harbor 的后端存储。</p>
<p>参考：<a href="https://github.com/goharbor/harbor-helm/tree/1.2.0">https://github.com/goharbor/harbor-helm/tree/1.2.0</a></p>
<ol>
<li>
<p>在 <a href="https://github.com/goharbor/harbor-helm/releases" target="_blank" rel="noopener noreffer">harbor-helm 发布页面</a>下载发布包</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">wget https://github.com/goharbor/harbor-helm/archive/v1.2.1.tar.gz
</code></pre></td></tr></table>
</div>
</div><p>解压发布包</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">tar -zxvf v1.2.1.tar.gz
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>编辑 <code>values.yaml</code> ，修改 harbor-helm 的配置</p>
<ul>
<li>配置这么暴露 harboar 的服务，默认是 Ingress，本文使用 Ingress 暴漏服务，所以不用修改</li>
<li>tls 默认是开启的。如果使用 ingress 暴漏服务的时候，tls是关闭的，则需要在执行 pull/push 命令的时候加上端口号。[见issues](Refer to <a href="https://github.com/goharbor/harbor/issues/5291">https://github.com/goharbor/harbor/issues/5291</a>)</li>
<li>配置 tls secret，对应 values.yaml 中的secretName 和 notarySecretName，若 notarySecretName不指定的时候，会使用secretName 的证书和key，secretName 指定 harbor 管理界面域名使用的的https证书，notarySecretName 中指定的 notary 页面使用的 https 证书。secretName 中指定的 secret 需要包含 tls.crt 、tls.key 和 ca.crt， secret 中证书的内容和 ingress secret 的内容相同，所以可以使用 ingress 的证书的手动生成方式或使用 cert-manager 进行生成。secretName和notarySecretName不要配置成一样的，否则其中一个页面无法 https。</li>
<li>配置 expose.ingress.hosts，将 core 和 notary 分别配置为期望的域名。</li>
<li>ingress annotatiaons 中添加 cert-manager.io/cluster-issuer: &ldquo;cluster-issuer&rdquo;，使用 cert-manager 自动生成包含证书的 secret。</li>
<li>配置 externalURL，因为本文使用的是 ingress 暴漏服务，所以此处的域名应该和 <code>expose.ingress.hosts.core</code> 配置的域名相同</li>
<li>配置如何持久化数据，默认使用 storageclass 持久化，上面安装了 nfs-storage ，并设为了默认的 sc ，可以使用它来动态提供 pv，满足 pvc 的要求，所以不用修改</li>
</ul>
<blockquote>
<p>helm install 的方式</p>
<ul>
<li>从 chart 仓库中安装 <code>helm install stable/mysql</code></li>
<li>从本地的 chart 包安装 <code>helm install foo-0.1.1.tgz</code></li>
<li>从 chart 包解压的目录进行安装 <code>helm install path/to/foo</code></li>
<li>通过完整 URL 安装 <code>helm install https://example.com/charts/foo-1.2.3.tgz</code></li>
</ul>
</blockquote>
<p>修改后的 values.yaml 和原文对比如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">[root@k8s-m1 harbor-helm-1.2.1]# diff values.yaml values.yaml.orig
19c19
&lt;     secretName: &#34;core.harbor.ingress.secret&#34;
---
&gt;     secretName: &#34;&#34;
23c23
&lt;     notarySecretName: &#34;notary.harbor.ingress.secret&#34;
---
&gt;     notarySecretName: &#34;&#34;
29,30c29,30
&lt;       core: core.harbor.k8s.abc
&lt;       notary: notary.harbor.k8s.abc
---
&gt;       core: core.harbor.domain
&gt;       notary: notary.harbor.domain
41d40
&lt;       cert-manager.io/cluster-issuer: &#34;cluster-issuer&#34;
102c101
&lt; externalURL: https://core.harbor.k8s.abc
---
&gt; externalURL: https://core.harbor.domain
</code></pre></td></tr></table>
</div>
</div><p>annotations 添加 <code>cert-manager.io/cluster-issuer: cluster-issuer</code>，cert-manager 监听到后会新建 ingress tls 的 secret。</p>
</li>
<li>
<p>安装 harbor-helm</p>
<p>新建 harbor-helm 安装的命名空间</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">kubectl create ns harbor
</code></pre></td></tr></table>
</div>
</div><p>在 <code>values.yaml</code> 所在的目录进行安装，所以 install 后面是一个点，表示当前目录。<code>--namespace</code> 指定安装的命名空间，<code>--name</code> 指定安装后的名字</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">[root@k8s-m1 harbor-helm-1.2.1]# helm install . --namespace harbor --name harbor-helm --values values.yaml
</code></pre></td></tr></table>
</div>
</div><p>helm v3 使用如下命令安装</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">[root@k8s-m1 harbor-helm-1.2.2]# helm install . --namespace harbor --name-template harbor-helm --values values.yaml
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>登录管理界面</p>
<p>查看生成的 ingress</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">$ kubectl get ing -n harbor
NAME                         HOSTS                                             ADDRESS          PORTS     AGE
harbor-helm-harbor-ingress   core.harbor.k8s.abc,notary.harbor.k8s.abc   192.168.15.144   80, 443   7m51s
</code></pre></td></tr></table>
</div>
</div><p>在 win10 hosts文件中，添加 <code>192.168.15.144  *.harbor.k8s.abc</code>，然后通过浏览器访问 <code>https://core.harbor.k8s.abc</code> 和 <code>https://notary.harbor.k8s.abc</code> ，https 是安全的。</p>
<p>harbor 管理界面 <code>https://core.harbor.k8s.abc</code> 的默认用户名密码为 <code>admin/Harbor12345</code></p>
</li>
<li>
<p>通过 docker client 推送或拉取镜像</p>
<p>docker client 默认使用 https 连接镜像仓库，可以通过<a href="https://docs.docker.com/registry/insecure/" target="_blank" rel="noopener noreffer">这个文档</a>了解如何配置使用 http 连接镜像仓库。</p>
<p>如果一个私有镜像仓库使用http 或使用未知ca的https 提供服务，需要将 <code>--insecure-registry myregistrydomain.com</code> 添加到 docker client 的守护进程启动参数中。</p>
<p>对于使用HTTPS的镜像仓库，如果您有权访问镜像仓库的CA证书，只需将CA证书放在/etc/docker/certs.d/myregistrydomain.com/ca.crt。</p>
<p><em><strong>下载镜像</strong></em>
如果镜像属于私有的项目，你首先需要登录</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">docker login core.harbor.k8s.abc
</code></pre></td></tr></table>
</div>
</div><p>然后才可以下载镜像</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">docker pull core.harbor.k8s.abc/library/centos:latest
</code></pre></td></tr></table>
</div>
</div><p><em><strong>推送镜像</strong></em><br>
推送镜像之前，必须通过 harbor UI 新建一个对应的项目，如新建一个 demo 项目</p>
<p>然后通过 docker client 登录 harbor</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">docker login core.harbor.k8s.abc
</code></pre></td></tr></table>
</div>
</div><p>为 镜像 打标签</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">docker tag centos:latest core.harbor.k8s.abc/demo/centos:latest
</code></pre></td></tr></table>
</div>
</div><p>推送镜像到 harbor 仓库</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">docker push core.harbor.k8s.abc/demo/centos:latest
</code></pre></td></tr></table>
</div>
</div></li>
</ol>
<h3 id="tidb">tidb</h3>
<h4 id="官方文档">官方文档</h4>
<p><a href="https://pingcap.com/docs-cn/v3.0/tidb-in-kubernetes/tidb-operator-overview/">https://pingcap.com/docs-cn/v3.0/tidb-in-kubernetes/tidb-operator-overview/</a></p>
<h4 id="安装">安装</h4>
<p><a href="/images/tidb-v3.0-%e5%ae%98%e6%96%b9%e6%96%87%e6%a1%a3%e9%98%85%e8%af%bb%e7%ac%94%e8%ae%b0/index.html" rel="">TiDB-v3.0-安装记录</a></p>
<h3 id="jenkins">jenkins</h3>
<h4 id="jenkins-master">jenkins master</h4>
<p>jenkins master 镜像基于 <code>jenkins/jenkins:lts</code> 进行构建，<code>jinkins/jenkins:lts</code> 的介绍详见： <a href="https://github.com/jenkinsci/docker">https://github.com/jenkinsci/docker</a></p>
<p>因为 oracle jdk 无法直接下载（官网下载方式需要登录），所以无法直接在 <code>Dockerfile</code> 里指定下载地址，此处通过在 build 时使用 <code>--build-arg</code> 参数指定 jdk 的下载地址，jdk 的下载地址可以通过 onedriver 生成，生成的地址虽然只有一个小时有效期，但是足够进行构建镜像了。为了加速构建镜像，此处使用 dockerhub 进行镜像的构建，所以需要在 Dockerfile 所在的目录新建一个 hooks 目录，里面放入构建文件 build。</p>
<p>jenkins master 的Dockerfile 等构建资料详见：<a href="https://github.com/msdemt/dockerfiles/tree/master/jenkins/jenkins-master">https://github.com/msdemt/dockerfiles/tree/master/jenkins/jenkins-master</a></p>
<p>关于为什么不使用 COPY 命令将 jdk 包传入镜像进行构建的原因，是因为 jdk 的 tar 包有 185mb，如果使用 COPY 命令，则生成的镜像会多出一层，这层有185mb ，即使在后面的 RUN 命令中删除了 tar 包，这层还是存在的，从而导致生成的镜像比较大，所以将下载 jdk，删除多余文件都放在了一个 RUN 命令中执行。</p>
<p>关于使用 <code>dockerhub</code> 构建镜像如何传入 <code>ARG</code> 参数的方法详见：<a href="https://github.com/docker/hub-feedback/issues/508">https://github.com/docker/hub-feedback/issues/508</a></p>
<p>使用 dockerhub 生成镜像，名为 hekai/jenkins-slaver-oraclejdk8-debian:20191029</p>
<p>下载镜像后，为镜像重新打 tag ，使镜像名符合 harbor 仓库的要求</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">docker tag hekai/jenkins-slaver-oraclejdk8-debian:20191029 core.harbor.k8s.abc/library/jenkins-slaver-oraclejdk8-debian:20191029
</code></pre></td></tr></table>
</div>
</div><p>将生成后的镜像推送到 harbor 仓库</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">docker push core.harbor.k8s.abc/library/jenkins-slaver-oraclejdk8-debian:20191029
</code></pre></td></tr></table>
</div>
</div><p>修改 <code>jenkins.yaml</code> 中的镜像名为 <code>core.harbor.k8s.abc/library/jenkins-slaver-oraclejdk8-debian:20191029</code>，执行清单文件，生成 jenkins 服务</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">kubectl apply -f jenkins.yaml
kubectl apply -f jenkins-ing.yml
</code></pre></td></tr></table>
</div>
</div><p>查看 jenkins master 的 pod，可以看到 jenkins 服务第一次启动生成的默认密码</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">$ kubectl get pods -n jenkins-ns
NAME                              READY   STATUS    RESTARTS   AGE
jenkins-deploy-7dbcdd9848-d8b27   0/1     Running   0          42s
$ kubectl logs -n jenkins-ns jenkins-deploy-7dbcdd9848-d8b27
... ...
*************************************************************

Jenkins initial setup is required. An admin user has been created and a password generated.
Please use the following password to proceed to installation:

a9ad8fbef3e448c0b208c54fd5313a24

This may also be found at: /var/jenkins_home/secrets/initialAdminPassword

*************************************************************
... ...
2019-10-25 07:18:54.943+0000 [id=39]  INFO  hudson.util.Retrier#start: The attempt #1 to do the action check updates server failed with an allowed exception:
java.net.SocketTimeoutException: connect timed out
  at java.net.PlainSocketImpl.socketConnect(Native Method)
... ...
</code></pre></td></tr></table>
</div>
</div><p>jenkins pod 启动后，会访问 <code>https://updates.jenkins.io/update-center.json</code> ，下载 updates 资源，生成 updates 目录，如果访问不通，则 jenkins 的 pod 会处于 0/1 的状态。</p>
<p>如下</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">$ kubectl get pods -n jenkins-ns
NAME                              READY   STATUS    RESTARTS   AGE
jenkins-deploy-7dbcdd9848-rrgct   0/1     Running   0          77s
</code></pre></td></tr></table>
</div>
</div><p>查看该 pod 的日志，会发现有个 <code>SocketTimeoutException</code> 的错误。</p>
<p>需要修改 /var/jenkins_home/ 中的 <code>hudson.model.UpdateCenter.xml</code> 文件中指定的更新地址。</p>
<p>因为本文使用的使 nfs 挂载的 jenkins，在 jenkins.yaml 中找到jenkins 容器的 <code>/var/jenkins_home</code> 对应的 pvc 的名字，本文为 <code>jenkins-home-claim</code>，然后在 nfs server 挂载目录中找到对应的挂载文件夹，本文为 <code>/home/data/nfs/data/jenkins-ns-jenkins-home-claim-pvc-4662aeda-4628-4425-a9f1-5a6461425ca5</code></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">[root@c48-2 jenkins-ns-jenkins-home-claim-pvc-4662aeda-4628-4425-a9f1-5a6461425ca5]# ls
config.xml               hudson.model.UpdateCenter.xml  jenkins.install.UpgradeWizard.state  jobs  nodeMonitors.xml  plugins     secret.key.not-so-secret  userContent  war
copy_reference_file.log  identity.key.enc               jenkins.telemetry.Correlator.xml     logs  nodes             secret.key  secrets                   users
[root@c48-2 jenkins-ns-jenkins-home-claim-pvc-4662aeda-4628-4425-a9f1-5a6461425ca5]# cat hudson.model.UpdateCenter.xml
&lt;?xml version=&#39;1.1&#39; encoding=&#39;UTF-8&#39;?&gt;
&lt;sites&gt;
  &lt;site&gt;
    &lt;id&gt;default&lt;/id&gt;
    &lt;url&gt;https://updates.jenkins.io/update-center.json&lt;/url&gt;
  &lt;/site&gt;
&lt;/sites&gt;
</code></pre></td></tr></table>
</div>
</div><p>jenkins 第一次启动会访问 <code>hudson.model.UpdateCenter.xml</code> 中指定的地址，生成 <code>updates</code> 目录及其内容，因为默认的地址为 <code>https://updates.jenkins.io/update-center.json</code> ，该地址国内可能无法访问，所以 jenkins 没有启动成功。 将该地址修改为清华大学 jenkins 镜像源的地址 <code>https://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.json</code>，手工修改或执行如下命令修改</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">sed -i &#39;s/updates.jenkins.io/mirrors.tuna.tsinghua.edu.cn\/jenkins\/updates/g&#39; hudson.model.UpdateCenter.xml
</code></pre></td></tr></table>
</div>
</div><p>修改后，重启 jenkins 对应的 pod</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">kubectl delete pods -n jenkins-ns jenkins-deploy-7dbcdd9848-d8b27
</code></pre></td></tr></table>
</div>
</div><p>jenkins 服务启动成功，生成了 updates 文件夹</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">[root@c48-2 jenkins-ns-jenkins-home-claim-pvc-4662aeda-4628-4425-a9f1-5a6461425ca5]# ls
config.xml               hudson.model.UpdateCenter.xml  jenkins.install.UpgradeWizard.state  jobs  nodeMonitors.xml  plugins        secret.key                secrets  userContent  war
copy_reference_file.log  identity.key.enc               jenkins.telemetry.Correlator.xml     logs  nodes             queue.xml.bak  secret.key.not-so-secret  updates  users
</code></pre></td></tr></table>
</div>
</div><p>此时若进行插件下载，会出现很多失败的情况，因为 jenkins 通过 updates/default.json 中的地址进行对应插件的下载，默认的地址为 <code>http://updates.jenkins-ci.org/download/plugins</code> ，这个地址国内也是无法访问或访问特别慢，将该地址修改为清华大学的 jenkins 镜像对应的地址，同时修改测试连接的地址为<code>www.baidu.com</code> ，使用如下命令修改</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">sed -i &#39;s/http:\/\/updates.jenkins-ci.org\/download/https:\/\/mirrors.tuna.tsinghua.edu.cn\/jenkins/g&#39; updates/default.json &amp;&amp; sed -i &#39;s/http:\/\/www.google.com/https:\/\/www.baidu.com/g&#39; updates/default.json
</code></pre></td></tr></table>
</div>
</div><p>再次重启 jenkins 对应的 pod</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">kubectl delete pods -n jenkins-ns jenkins-deploy-7dbcdd9848-9brbx
</code></pre></td></tr></table>
</div>
</div><p>jenkins 服务启动成功</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">$ kubectl get pods -n jenkins-ns
NAME                              READY   STATUS    RESTARTS   AGE
jenkins-deploy-7dbcdd9848-rrgct   1/1     Running   0          77s
</code></pre></td></tr></table>
</div>
</div><p>查看 jenkins 的初始密码，记录初始密码的文件位于 <code>secrets/initialAdminPassword</code></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">[root@c48-2 jenkins-ns-jenkins-home-claim-pvc-4662aeda-4628-4425-a9f1-5a6461425ca5]# cat secrets/initialAdminPassword
a9ad8fbef3e448c0b208c54fd5313a24
</code></pre></td></tr></table>
</div>
</div><p>访问 jenkins 网页 <code>https://jenkins.k8s.abc</code>，输入初始密码，安装推荐的插件，配置管理员用户，若首次登录页面空白。再次重启下 jenkins 的 pod 。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">kubectl delete pods -n jenkins-ns jenkins-deploy-7dbcdd9848-rrgct
</code></pre></td></tr></table>
</div>
</div><p>万能的重启，如果 jenkins 有个启动参数能够指定插件源就不用这么麻烦了。。。</p>
<p>jenkins master 安装成功。</p>
<p>登录后，修改 admin 用户默认密码。</p>
<h4 id="jenkins-slaver">jenkins slaver</h4>
<p>jenkins slaver 的镜像基于 <code>jenkins/jnlp-slave</code> ， 详见：<a href="https://github.com/jenkinsci/docker-jnlp-slave">https://github.com/jenkinsci/docker-jnlp-slave</a></p>
<p>jenkins slaver 的 Dockerfile 等构建资料详见：<a href="https://github.com/msdemt/dockerfiles/tree/master/jenkins/jenkins-slaver">https://github.com/msdemt/dockerfiles/tree/master/jenkins/jenkins-slaver</a></p>
<h4 id="jdk-images">jdk images</h4>
<p>由于要使用 jenkins 运行 Java 项目，所以需要包含 oracle jdk 和 tomcat 的镜像。</p>
<p>基于 debian 的 oracle jdk 镜像的Dockerfile等构建资料详见：<a href="https://github.com/msdemt/dockerfiles/tree/master/jdk/oraclejdk-with-debian">https://github.com/msdemt/dockerfiles/tree/master/jdk/oraclejdk-with-debian</a></p>
<p>基于 debian 和 oracle jdk 的 tomcat 镜像详见：<a href="https://github.com/msdemt/dockerfiles/tree/master/tomcat/tomcat9-oraclejdk8-debian">https://github.com/msdemt/dockerfiles/tree/master/tomcat/tomcat9-oraclejdk8-debian</a></p>
<h4 id="jenkins-pipeline">jenkins pipeline</h4>
<p>jenkins 安装插件：Kubernetes Cli 、Kubernetes、Kubernetes Continuous Deploy</p>
<p>kubernetes 插件介绍：<a href="https://github.com/jenkinsci/kubernetes-plugin">https://github.com/jenkinsci/kubernetes-plugin</a></p>
<ol>
<li>
<p>配置 kubernetes</p>
<p>依次选择 Manage Jenkins - Configure System - 云 - 新增一个云</p>
<p>名称为 <code>kubernetes</code> ， kubernetes 地址为 <code>https://kubernetes.default</code> ，点击连接测试，测试成功后，保存。</p>
</li>
<li>
<p>生成 access token
点击用户列表，选择 当前的用户（admin）- 设置 - 添加新的Token 输入 Token的名称，点击生成，如生成的Token 为：11fa1a1b89200f30dbf4df6275e949de65</p>
</li>
<li>
<p>将用户名，token 和 jenkins 的地址 配置到 svn post-commit 脚本，用于 svn 提交代码后，向 jenkins 发消息触发构建</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="ch">#!/usr/bin/env python</span>
<span class="c1"># -*- coding: utf-8 -*-</span>
<span class="kn">import</span> <span class="nn">urllib2</span><span class="o">,</span> <span class="nn">sys</span><span class="o">,</span> <span class="nn">os</span><span class="o">,</span> <span class="nn">base64</span><span class="o">,</span> <span class="nn">json</span>
<span class="c1"># 本仓库的目录路径</span>
<span class="n">repos_path</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="c1"># 版本修订号</span>
<span class="n">revision</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="c1"># svnlook命令所在路径</span>
<span class="n">svnlook</span> <span class="o">=</span> <span class="s1">&#39;/usr/bin/svnlook&#39;</span>
<span class="c1"># Jenkins的地址</span>
<span class="n">baseurl</span> <span class="o">=</span> <span class="s1">&#39;http://192.168.15.143:30000&#39;</span>
<span class="c1"># Jenkins用户和用户的api token</span>
<span class="n">user_id</span> <span class="o">=</span> <span class="s1">&#39;admin&#39;</span>
<span class="n">api_token</span> <span class="o">=</span> <span class="s1">&#39;111ec492afd7a821259bbfd9d405047cac&#39;</span>

<span class="k">def</span> <span class="nf">my_urlopen</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="p">{</span> <span class="p">}):</span>
   <span class="n">request</span> <span class="o">=</span> <span class="n">urllib2</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
   <span class="n">base64string</span> <span class="o">=</span> <span class="n">base64</span><span class="o">.</span><span class="n">encodestring</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">%s</span><span class="s1">:</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">user_id</span><span class="p">,</span> <span class="n">api_token</span><span class="p">))</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span>
   <span class="n">header</span><span class="p">[</span><span class="s1">&#39;Authorization&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;Basic </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">base64string</span>
   <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">header</span><span class="p">:</span>
       <span class="n">request</span><span class="o">.</span><span class="n">add_header</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">header</span><span class="p">[</span><span class="n">key</span><span class="p">])</span>
   <span class="k">try</span><span class="p">:</span>
       <span class="n">response</span> <span class="o">=</span> <span class="n">urllib2</span><span class="o">.</span><span class="n">urlopen</span><span class="p">(</span><span class="n">request</span><span class="p">)</span>
   <span class="k">except</span> <span class="n">urllib2</span><span class="o">.</span><span class="n">URLError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
       <span class="k">print</span> <span class="s1">&#39;[Exception URLError]:&#39;</span><span class="p">,</span> <span class="n">e</span>
       <span class="n">sys</span><span class="o">.</span><span class="n">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
   <span class="k">else</span><span class="p">:</span>
       <span class="n">result</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">read</span><span class="p">(</span> <span class="p">)</span>
   <span class="k">finally</span><span class="p">:</span>
       <span class="k">if</span> <span class="s1">&#39;response&#39;</span> <span class="ow">in</span> <span class="nb">vars</span><span class="p">(</span> <span class="p">):</span>
           <span class="n">response</span><span class="o">.</span><span class="n">close</span><span class="p">(</span> <span class="p">)</span>
   <span class="k">return</span> <span class="n">result</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">(</span> <span class="p">):</span>
   <span class="c1"># 获取uuid值</span>
   <span class="n">command</span> <span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;</span><span class="si">%s</span><span class="s1"> uuid </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">svnlook</span><span class="p">,</span> <span class="n">repos_path</span><span class="p">)</span>
   <span class="k">with</span> <span class="n">os</span><span class="o">.</span><span class="n">popen</span><span class="p">(</span><span class="n">command</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
       <span class="n">uuid</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">(</span> <span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">(</span> <span class="p">)</span>
   <span class="c1"># 获取仓库变更信息</span>
   <span class="n">command</span> <span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;</span><span class="si">%s</span><span class="s1"> changed --revision </span><span class="si">%s</span><span class="s1"> </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">svnlook</span><span class="p">,</span> <span class="n">revision</span><span class="p">,</span> <span class="n">repos_path</span><span class="p">)</span>
   <span class="k">with</span> <span class="n">os</span><span class="o">.</span><span class="n">popen</span><span class="p">(</span><span class="n">command</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
       <span class="n">data</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">(</span> <span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">(</span> <span class="p">)</span>
   <span class="c1"># 获取crumb</span>
   <span class="n">url</span> <span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;</span><span class="si">%s</span><span class="s1">/crumbIssuer/api/json&#39;</span> <span class="o">%</span> <span class="n">baseurl</span>
   <span class="n">crumb_dict</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">my_urlopen</span><span class="p">(</span><span class="n">url</span><span class="p">))</span>
   <span class="c1"># 触发Jenkins</span>
   <span class="n">url</span> <span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;</span><span class="si">%s</span><span class="s1">/subversion/</span><span class="si">%s</span><span class="s1">/notifyCommit?rev=</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">baseurl</span><span class="p">,</span> <span class="n">uuid</span><span class="p">,</span> <span class="n">revision</span><span class="p">)</span>
   <span class="n">header</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;Content-Type&#39;</span><span class="p">:</span><span class="s1">&#39;text/plain&#39;</span><span class="p">,</span> <span class="s1">&#39;charset&#39;</span><span class="p">:</span><span class="s1">&#39;UTF-8&#39;</span><span class="p">,</span> <span class="n">crumb_dict</span><span class="p">[</span><span class="s1">&#39;crumbRequestField&#39;</span><span class="p">]:</span><span class="n">crumb_dict</span><span class="p">[</span><span class="s1">&#39;crumb&#39;</span><span class="p">]}</span>
   <span class="n">my_urlopen</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">header</span><span class="p">)</span>

<span class="n">main</span><span class="p">(</span> <span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>生成 svn 检出代码对应的流水线语句</p>
<p>进入 jenkins 首页，选择 <code>凭据</code> ，点击 <code>全局</code>， <code>添加凭据</code>，输入 svn 的用户名、密码，id 填入 svn 标记该凭据为 svn ，输入自定义的描述，然后点击确定。</p>
<p>进入 test 项目中，点击<code>流水线语法</code>，在<code>片段生成器</code> - <code>步骤</code> - <code>示例步骤</code> 中选择 <code>checkout: Check out from version control</code>， <code>SCM</code> 选择 <code>Subversion</code>，在 <code>Repository URL</code> 输入 svn 的项目地址，在 <code>Credentials</code> 选择记录有svn用户名和密码的凭据，然后点击生成流水线脚本，将脚本保存，下一步在 pipeline 中的 <code>CheckOut</code> 阶段使用。</p>
</li>
<li>
<p>新建流水线，用于自动构建 svn 上的项目</p>
<p>依次点击 新建Item - 输入任务名称（如 test） - 选择流水线 - 点击确定</p>
<p>在<code>构建触发器</code>中选择 <code>Poll SCM</code>，表示 jenkins 收到 svn 的消息后，触发该流程。</p>
<p>在<code>流水线</code>中输入流水线脚本，然后点击保存。</p>
<p>test 项目的流水线脚本如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span><span class="lnt">108
</span><span class="lnt">109
</span><span class="lnt">110
</span><span class="lnt">111
</span><span class="lnt">112
</span><span class="lnt">113
</span><span class="lnt">114
</span><span class="lnt">115
</span><span class="lnt">116
</span><span class="lnt">117
</span><span class="lnt">118
</span><span class="lnt">119
</span><span class="lnt">120
</span><span class="lnt">121
</span><span class="lnt">122
</span><span class="lnt">123
</span><span class="lnt">124
</span><span class="lnt">125
</span><span class="lnt">126
</span><span class="lnt">127
</span><span class="lnt">128
</span><span class="lnt">129
</span><span class="lnt">130
</span><span class="lnt">131
</span><span class="lnt">132
</span><span class="lnt">133
</span><span class="lnt">134
</span><span class="lnt">135
</span><span class="lnt">136
</span><span class="lnt">137
</span><span class="lnt">138
</span><span class="lnt">139
</span><span class="lnt">140
</span><span class="lnt">141
</span><span class="lnt">142
</span><span class="lnt">143
</span><span class="lnt">144
</span><span class="lnt">145
</span><span class="lnt">146
</span><span class="lnt">147
</span><span class="lnt">148
</span><span class="lnt">149
</span><span class="lnt">150
</span><span class="lnt">151
</span><span class="lnt">152
</span><span class="lnt">153
</span><span class="lnt">154
</span><span class="lnt">155
</span><span class="lnt">156
</span><span class="lnt">157
</span><span class="lnt">158
</span><span class="lnt">159
</span><span class="lnt">160
</span><span class="lnt">161
</span><span class="lnt">162
</span><span class="lnt">163
</span><span class="lnt">164
</span><span class="lnt">165
</span><span class="lnt">166
</span><span class="lnt">167
</span><span class="lnt">168
</span><span class="lnt">169
</span><span class="lnt">170
</span><span class="lnt">171
</span><span class="lnt">172
</span><span class="lnt">173
</span><span class="lnt">174
</span><span class="lnt">175
</span><span class="lnt">176
</span><span class="lnt">177
</span><span class="lnt">178
</span><span class="lnt">179
</span><span class="lnt">180
</span><span class="lnt">181
</span><span class="lnt">182
</span><span class="lnt">183
</span><span class="lnt">184
</span><span class="lnt">185
</span><span class="lnt">186
</span><span class="lnt">187
</span><span class="lnt">188
</span><span class="lnt">189
</span><span class="lnt">190
</span><span class="lnt">191
</span><span class="lnt">192
</span><span class="lnt">193
</span><span class="lnt">194
</span><span class="lnt">195
</span><span class="lnt">196
</span><span class="lnt">197
</span><span class="lnt">198
</span><span class="lnt">199
</span><span class="lnt">200
</span><span class="lnt">201
</span><span class="lnt">202
</span><span class="lnt">203
</span><span class="lnt">204
</span><span class="lnt">205
</span><span class="lnt">206
</span><span class="lnt">207
</span><span class="lnt">208
</span><span class="lnt">209
</span><span class="lnt">210
</span><span class="lnt">211
</span><span class="lnt">212
</span><span class="lnt">213
</span><span class="lnt">214
</span><span class="lnt">215
</span><span class="lnt">216
</span><span class="lnt">217
</span><span class="lnt">218
</span><span class="lnt">219
</span><span class="lnt">220
</span><span class="lnt">221
</span><span class="lnt">222
</span><span class="lnt">223
</span><span class="lnt">224
</span><span class="lnt">225
</span><span class="lnt">226
</span><span class="lnt">227
</span><span class="lnt">228
</span><span class="lnt">229
</span><span class="lnt">230
</span><span class="lnt">231
</span><span class="lnt">232
</span><span class="lnt">233
</span><span class="lnt">234
</span><span class="lnt">235
</span><span class="lnt">236
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">pipeline {
  agent {
    kubernetes {
      //cloud &#39;kubernetes&#39;
      //defaultContainer &#39;&#39;
      yaml  &#34;&#34;&#34;
            kind: Pod
            metadata:
              namespace: jenkins-ns
              labels:
                label: jenkins-agent
            spec:
              serviceAccountName: jenkins-sa
              containers:
              - name: jnlp
                image: core.harbor.k8s.abc/library/jenkins-slaver-oraclejdk8-debian:20191029
                imagePullPolicy: IfNotPresent
                env:
                - name: &#34;JENKINS_DIRECT_CONNECTION&#34;
                  value: &#34;192.168.15.5:50000&#34;
                ports:
                - name: http
                  containerPort: 8080
                - name: agent
                  containerPort: 50000
                volumeMounts:
                - name: hosts
                  mountPath: /etc/hosts
                - name: docker-config
                  mountPath: /etc/docker
                - name: docker-bin
                  mountPath: /usr/bin/docker
                - name: docker-sock
                  mountPath: /var/run/docker.sock
                - name: maven-repo
                  mountPath: /maven/repo
              volumes:
              - name: hosts
                hostPath:
                  path: /etc/hosts
              - name: docker-config
                hostPath:
                  path: /etc/docker
              - name: docker-bin
                hostPath:
                  path: /usr/bin/docker
              - name: docker-sock
                hostPath:
                  path: /var/run/docker.sock
              - name: maven-repo
                persistentVolumeClaim:
                  claimName: maven-repo-claim
            &#34;&#34;&#34;
        }
    }
    // 对应Do not allow concurrent builds
    options {
        disableConcurrentBuilds()
    }
    //triggers { pollSCM(&#39;&#39;) }
    environment {
        // ------ 以下内容，每个项目可能均有不同，按需修改 ------
        // branch: 分支，一般是test、 master，对应从哪个分支拉取代码，也对应究竟执行_deploy文件夹下的test配置还是master配置
        branch = &#34;master&#34;
        // namespace:
        namespace = &#34;default&#34;
        // appname：对应deployment中的app
        app = &#34;demo&#34;
        // port：对应deployment中的port
        port= &#34;38082&#34;
        privileged = &#34;true&#34;
        // replicas：对应deployment中的replicas
        replicas = 1
        //svn repo的地址
        //svn=&#34;http://192.168.15.143:8888/svn/code/abc&#34;
        //log：对应deployment中的log
        log=&#34;/usr/local/tomcat/logs&#34;
        // ------ 以下内容，一般所有的项目都一样，不经常修改 ------
        // harbor inner address
        repoHost = &#34;core.harbor.k8s.abc&#34;
        // harbor的账号密码信息，在jenkins中配置用户名/密码形式的认证信息，命名成harbor即可
        harborCreds = credentials(&#39;harbor&#39;)
        //harborCreds_USR = &#34;admin&#34;
        //harborCreds_PSW = &#34;Harbor12345&#34;
    }
    // ------ 以下内容无需修改 ------
    stages {
        // 开始构建前清空工作目录
        stage (&#34;CleanWS&#34;){
            steps {
                script {
                    try{
                        deleteDir()
                    }catch(err){
                        echo &#34;${err}&#34;
                        sh &#39;exit 1&#39;
                    }
                }
            }
        }
        // 拉取
        stage (&#34;CheckOut&#34;){
            steps {
                script {
                    try{
                        checkout([$class: &#39;SubversionSCM&#39;, additionalCredentials: [], excludedCommitMessages: &#39;&#39;, excludedRegions: &#39;&#39;, excludedRevprop: &#39;&#39;, excludedUsers: &#39;&#39;, filterChangelog: false, ignoreDirPropChanges: false, includedRegions: &#39;&#39;, locations: [[cancelProcessOnExternalsFail: true, credentialsId: &#39;test&#39;, depthOption: &#39;infinity&#39;, ignoreExternalsOption: true, local: &#39;.&#39;, remote: &#39;http://192.168.15.143:8888/svn/code/abc/trunk/demo&#39;]], quietOperation: true, workspaceUpdater: [$class: &#39;UpdateUpdater&#39;]])
                    }catch(err){
                        echo &#34;${err}&#34;
                        sh &#39;exit 1&#39;
                    }
                }
            }
        }
        // 编译
        stage (&#34;Compile&#34;){
            steps {
                script {
                    try{
                        sh &#34;mvn clean &amp;&amp; mvn package&#34;
                        //sh &#34;cp target/*.war &#34;
                    }catch(err){
                        echo &#34;${err}&#34;
                        sh &#39;exit 1&#39;
                    }
                }
            }
        }
        // 构建
        stage (&#34;Build&#34;){
            steps {
                script {
                    try{
                        // 镜像tag用时间戳代表
                        sh &#34;date +%Y%m%d%H%m%S &gt; timestamp&#34;
                        tag = readFile(&#39;timestamp&#39;).replace(&#34;\n&#34;, &#34;&#34;).replace(&#34;\r&#34;, &#34;&#34;)
                        repoPath = &#34;${repoHost}/${namespace}/${app}:${tag}&#34;
                        // 根据分支，进入_deploy下对应的不同文件夹，通过dockerfile打包镜像
                        sh &#34;cp _deploy/${branch}/* ./&#34;
                        withDockerRegistry(credentialsId: &#39;harbor&#39;, url: &#39;http://core.harbor.k8s.abc&#39;) {
                            // some block
                            //sh &#34;docker login -u ${harborCreds_USR} -p ${harborCreds_PSW} ${repoHost}&#34;
                            sh &#34;docker build -t ${repoPath}  .&#34;
                        }
                        //sh &#34;docker login -u ${harborCreds_USR} -p ${harborCreds_PSW} ${repoHost}&#34;
                        //sh &#34;docker build -t ${repoPath}  .&#34;
                    }catch(err){
                        echo &#34;${err}&#34;
                        sh &#39;exit 1&#39;
                    }
                }
            }
        }
        // 镜像推送到harbor
        stage (&#34;Push&#34;){
            steps {
                script {
                    try{
                        withDockerRegistry(credentialsId: &#39;harbor&#39;, url: &#39;https://core.harbor.k8s.abc&#39;) {

                            sh &#34;docker push ${repoPath}&#34;
                        }

                    }catch(err){
                        echo &#34;${err}&#34;
                        sh &#39;exit 1&#39;
                    }
                }
            }
        }
        // 使用pipeline script中复制的变量替换deployment.yaml中的占位变量，执行deployment.yaml进行部署
        stage (&#34;Deploy&#34;){
            steps {
                script {
                    try{
                        sh &#34;sed -i &#39;s|#namespace#|${namespace}|g&#39; deployment.yaml&#34;
                        sh &#34;sed -i &#39;s|#app#|${app}|g&#39; deployment.yaml&#34;
                        sh &#34;sed -i &#39;s|#image#|${repoPath}|g&#39; deployment.yaml&#34;
                        sh &#34;sed -i &#39;s|#port#|${port}|g&#39; deployment.yaml&#34;
                        sh &#34;sed -i &#39;s|#privileged#|${privileged}|g&#39; deployment.yaml&#34;
                        sh &#34;sed -i &#39;s|#replicas#|${replicas}|g&#39; deployment.yaml&#34;
                        sh &#34;sed -i &#39;s|#log#|${log}|g&#39; deployment.yaml&#34;
                        sh &#34;cat deployment.yaml&#34;
                        sh &#34;kubectl apply -f deployment.yaml&#34;
                    }catch(err){
                        echo &#34;${err}&#34;
                        sh &#39;exit 1&#39;
                    }
                }
            }
        }
    }
    post{
        failure {
            script {
                echo &#39;项目构建失败，发送邮件！&#39;
                send_email_results(&#34;失败&#34;,&#34;test&#34;,&#34;hekai@abc.com,276516848@qq.com&#34;)
            }
        }
        success {
            script {
                echo &#39;项目构建成功，发送邮件！&#39;
                send_email_results(&#34;成功&#34;,&#34;test&#34;,&#34;hekai@abc.com&#34;)
            }
        }
    }
}
def send_email_results(status,SVNBranch,to_email_address_list) {
    def subject = &#34;Jenkins 自动构建 : &#34; + env.JOB_NAME + &#34;/&#34; + env.BUILD_ID + &#34; 项目 &#34; +  status
    def result_url = env.BUILD_URL + &#34;console&#34;
    def text =
            &#34;&#34;&#34;
      &lt;html&gt;
      &lt;style type=&#34;text/css&#34;&gt;
      &lt;/style&gt;
      &lt;body&gt;
      &lt;div id=&#34;content&#34;&gt;
      &lt;h1&gt;Summary&lt;/h1&gt;
      &lt;div id=&#34;sum2&#34;&gt;
          &lt;h2&gt;Jenkins 自动构建&lt;/h2&gt;
          &lt;ul&gt;
          &lt;li&gt;构建任务URL : &lt;a href=&#39;${env.BUILD_URL}&#39;&gt;${env.BUILD_URL}&lt;/a&gt;&lt;/li&gt;
           &lt;li&gt;构建结果URL : &lt;a href=&#39;${result_url}&#39;&gt;${result_url}&lt;/a&gt;&lt;/li&gt;
          &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div id=&#34;sum0&#34;&gt;
      &lt;h2&gt;SVN Branch&lt;/h2&gt;
      &lt;ul&gt;
      &lt;li&gt;${SVNBranch}&lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;/div&gt;&lt;/body&gt;&lt;/html&gt;
    &#34;&#34;&#34;

    mail body: text, subject: subject,  mimeType: &#39;text/html&#39;, to: to_email_address_list
}

</code></pre></td></tr></table>
</div>
</div><p>对如上的 pipeline 脚本进行解释：首先定义了 jenkins slaver 的模板，jenkins slaver 使用的镜像为 <code>core.harbor.k8s.abc/library/jenkins-slaver-oraclejdk8-debian:20191029</code> ，并且将一些目录挂载到了 jenkins slaver 的容器中，从而使 jenkins slaver 可以执行 docker 的命令。下面定义了一些环境变量，这些环境变量针对不同的项目需要有不同的配置，用于生成部署服务时的 yaml 文件。再往下就是 jenkins slaver 进行项目构建的不同阶段，分别为清空当前工作目录，拉取代码（第 4 步生成），编译、打包代码，生成该服务的镜像，将镜像推送到 Harbor 仓库，使用变量替换 deployment.yaml 中的对应值，生成部署用的 yaml，并执行 deployment.yaml 生成服务。在下面就是对构建成功或失败进行发送邮件，当前使用的是 jenkins 自带的邮件工具，需要配置下才能正常发送邮件，配置方法见下一步。</p>
</li>
<li>
<p>配置邮件</p>
<p>进入 jenkins 首页，点击 <code>Manage Jenkins</code> - <code>Config System</code> - <code>邮件通知</code>， 输入 <code>SMTP服务器</code>，如 <code>smtp.abc.com</code>，输入<code>用户默认邮件后缀</code>，如 <code>@abc.com</code>, 选中 <code>使用SMTP认证</code>，输入邮箱的用户名和对应的密码，选中<code>使用SSL协议</code>， <code>SMTP端口</code>输入<code>465</code>。这样就配置好了。</p>
<p>测试下邮件通不通，在Reply-To Address 中输入某个邮箱，选中 <code>通过发送测试邮件测试配置</code>， jenkins 就会向该邮箱发送测试邮件，收到测试邮件，就表示邮件配置好了。</p>
</li>
<li>
<p>至此，jenkins 上的项目就配置好了，进入项目，点击 <code>Build Now</code>，查看是否能构建成功，没问题的话就可以通过提交代码触发项目的自动构建和部署了</p>
</li>
</ol>
<p>k8s 部署的 jenkins 与普通部署的 jenkins 的区别：
在该流程中，使用docker 部署 jenkins master 不是必须的，也可以使用普通部署的 jenkins，见：<a href="https://github.com/jenkinsci/kubernetes-plugin">https://github.com/jenkinsci/kubernetes-plugin</a>，其中指出 <code>It is not required to run the Jenkins master inside Kubernetes.</code>，所以，普通部署的 jenkins 也可以通过安装 kubernetes 插件实现上述的功能。有点不同的是若是普通部署的jenkins，在配置 kubernetes 时（第一步），因为当前的 jenkins master 并不在 k8s 的集群中，所以无法通过 <code>https://kubernetes.default</code> 访问到 k8s，需要配为 k8s master 的 api server 的地址，如：<code>https://192.168.15.5:6443</code>，还需要配置 <code>Kubernetes 服务证书 key</code></p>
<p>k8s 部署的 jenkins master 可以保证当该 jenkins master 因为意外的原因服务停止的时候，docker 可以自动重启 jenkins master。</p>
<h3 id="rook-ceph">rook-ceph</h3>
<h3 id="istio">istio</h3>
<h3 id="knative">knative</h3>
<h3 id="wayne">wayne</h3>
<h3 id="prometheus">prometheus</h3>
<h3 id="efk">efk</h3>
<h3 id="gitlab">gitlab</h3>
<p><a href="/images/post/gitlab%e5%ae%89%e8%a3%85%e8%ae%b0%e5%bd%95.md/index.html" rel="">gitlab 安装记录</a></p>
<p>使用 k8s 安装 gitlab <a href="https://docs.gitlab.com/charts/">https://docs.gitlab.com/charts/</a></p>
<p>gitlab 官方的 helm chart 地址：https://gitlab.com/gitlab-org/charts/gitlab</p>
<p>下载 master 分支的 chart</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">mkdir /home/k8s/gitlab &amp;&amp; cd /home/k8s/gitlab
wget https://gitlab.com/gitlab-org/charts/gitlab/-/archive/master/gitlab-master.tar.gz
tar -zxf gitlab-master.tar.gz
</code></pre></td></tr></table>
</div>
</div><p>下载依赖的子chart</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">helm dep up ./gitlab-master
</code></pre></td></tr></table>
</div>
</div><ol>
<li>coredns 挂载宿主机 /etc/hosts 并配置 hosts 插件</li>
</ol>
<p>export release=mygitlab</p>
<p>kubectl create ns gitlab</p>
<ol>
<li>使用 CA 证书生成 gitlab-runner 连接 gitlab server 认证用的 secret
kubectl create secret generic gitlab.k8s.abc.crt &ndash;from-file=gitlab.k8s.abc.crt=ca.crt -n gitlab</li>
<li>生成 gitlab server 与 registry 交互使用的证书
生成私钥和证书签名请求
openssl req -new -newkey rsa:4096 -subj &ldquo;/CN=gitlab-issuer&rdquo; -nodes -keyout registry-k8s-abc.key -out registry-k8s-abc.csr
使用CA证书根据证书签名请求签发证书
openssl x509 -req -sha256 -days 365 -in registry-k8s-abc.csr -CA ca.crt -CAkey ca.key -set_serial 01 -out registry-k8s-abc.crt -n gitlab</li>
</ol>
<blockquote>
<p>原文是自签名生成一个证书和私钥
openssl req -new -newkey rsa:4096 -subj &ldquo;/CN=gitlab-issuer&rdquo; -nodes -x509 -keyout certs/registry-example-com.key -out certs/registry-example-com.crt</p>
</blockquote>
<p>根据生成的证书和私钥建一个secret
kubectl create secret generic ${release}-registry-secret &ndash;from-file=registry-auth.key=registry-k8s-abc.key &ndash;from-file=registry-auth.crt=registry-k8s-abc.crt -n gitlab</p>
<p>然后生成yaml时可以通过 global.registry.certificate.secret 引用该 secret</p>
<p>生成 incoming secret所需的</p>
<p>kubectl create secret generic gitlab-incoming-imap-pwd-secret &ndash;from-file=./gitlab-incoming-imap-pwd-key -n gitlab
kubectl create secret generic gitlab-outgoing-smtp-pwd-secret &ndash;from-file=gitlab-outgoing-smtp-pwd-key=gitlab-incoming-imap-pwd-key -n gitlab</p>
<p>生成 gitlab.yaml</p>
<p>helm template ./gitlab-master &ndash;name ${release} &ndash;set global.edition=ce &ndash;set global.time_zone=Beijing &ndash;set global.hosts.domain=k8s.abc &ndash;set nginx-ingress.enabled=false &ndash;set global.ingress.class=nginx &ndash;set prometheus.install=false &ndash;set certmanager.install=false &ndash;set global.ingress.configureCertmanager=false  &ndash;set gitlab.unicorn.ingress.tls.secretName=release-gitlab-tls &ndash;set registry.ingress.tls.secretName=release-registry-tls &ndash;set minio.ingress.tls.secretName=release-minio-tls &ndash;set global.ingress.annotations.&ldquo;cert-manager.io/cluster-issuer&rdquo;=cluster-issuer &ndash;set global.registry.certificate.secret=${release}-registry-secret &ndash;namespace gitlab &gt; gitlab.yaml</p>
<p>检查 gitlab.yaml，少 namespace的加上 namespace，imageTag 为 latest 的imagePullPolicy配置为Alaways,其他的配置为IfNotPresent</p>
<p>将 gitlab.k8s.abc.crt 挂载到 gitlab-runner pod 的/home/gitlab-runner/.gitlab-runner/certs/</p>
<p>然后就可以通过 gitlab.yaml 安装gitlab了。</p>
<p>获取gitlab root 用户的密码</p>
<p>kubectl get secret -n gitlab mygitlab-gitlab-initial-root-password -ojsonpath='{.data.password}' | base64 &ndash;decode ; echo</p>
<p>gitlab 配置smtp，用来新建用户时为用户发送邮件</p>
<p>根据文档里的描述配置 smtp 参数后，新建用户时，查看容器日志
kubectl logs -n gitlab -f mygitlab-sidekiq-all-in-1-b8b4d49cd-947nb</p>
<p>邮件未成功发送，日志中出现错误</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">[ActiveJob] [ActionMailer::DeliveryJob] [962a7c24-d6a8-4450-a8ff-5356779d2f4a] Sent mail to 276516848@qq.com (17488.3ms)
[ActiveJob] [ActionMailer::DeliveryJob] [962a7c24-d6a8-4450-a8ff-5356779d2f4a] Error performing ActionMailer::DeliveryJob (Job ID: 962a7c24-d6a8-4450-a8ff-5356779d2f4a) from Sidekiq(mailers) in 17550.92ms: EOFError (end of file reached):
</code></pre></td></tr></table>
</div>
</div><p>百度下错误：<code>EOFError (end of file reached)</code></p>
<p>见：<a href="https://www.jianshu.com/p/af142a66d781">https://www.jianshu.com/p/af142a66d781</a></p>
<p>如果 smtp 使用 25 端口，则不要配置 ssl 相关的项目</p>
<p>如果 smtp 使用 465 或其他端口，则需要配置如下参数</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">gitlab_rails[&#39;smtp_enable_starttls_auto&#39;] = true
gitlab_rails[&#39;smtp_tls&#39;] = true
gitlab_rails[&#39;smtp_openssl_verify_mode&#39;] = &#39;none&#39;
</code></pre></td></tr></table>
</div>
</div><p>所以文件配置如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">helm template . \
  --name ${release} \
  --set global.edition=ce \
  --set global.time_zone=Beijing \
  --set global.hosts.domain=k8s.abc \
  --set global.grafana.enabled=false \
  --set global.ingress.class=nginx \
  --set global.ingress.configureCertmanager=false \
  --set gitlab.unicorn.ingress.tls.secretName=release-gitlab-tls \
  --set registry.ingress.tls.secretName=release-registry-tls \
  --set minio.ingress.tls.secretName=release-minio-tls \
  --set global.ingress.annotations.&#34;cert-manager\.io\/cluster-issuer&#34;=cluster-issuer \
  --set global.registry.certificate.secret=${release}-registry-secret \
  --set global.email.display_name=hekai \
  --set global.email.from=hekai@abc.com \
  --set global.email.reply_to=hekai@abc.com \
  --set global.email.subject_suffix=@abc.com \
  --set global.smtp.enabled=true \
  --set global.smtp.address=smtp.abc.com \
  --set global.smtp.port=465 \
  --set global.smtp.authentication=login \
  --set global.smtp.openssl_verify_mode=none \
  --set global.smtp.starttls_auto=true \
  --set global.smtp.tls=true \
  --set global.smtp.password.key=gitlab-outgoing-smtp-pwd-key \
  --set global.smtp.password.secret=gitlab-outgoing-smtp-pwd-secret \
  --set global.smtp.user_name=hekai@abc.com \
  --set certmanager.install=false \
  --set nginx-ingress.enabled=false \
  --set prometheus.install=false \
  --namespace gitlab \
  &gt; gitlab.yaml
</code></pre></td></tr></table>
</div>
</div><p>再配置下接收邮件，接收邮件的作用如下：
<a href="https://docs.gitlab.com/ee/administration/incoming_email.html">https://docs.gitlab.com/ee/administration/incoming_email.html</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">GitLab has several features based on receiving incoming emails:

    Reply by Email: allow GitLab users to comment on issues and merge requests by replying to notification emails.
    New issue by email: allow GitLab users to create a new issue by sending an email to a user-specific email address.
    New merge request by email: allow GitLab users to create a new merge request by sending an email to a user-specific email address.
    Service Desk: provide e-mail support to your customers through GitLab.
</code></pre></td></tr></table>
</div>
</div><p>日常用的应该不多。</p>
<p>incomingEmail 使用的端口为 143，不使用ssl 的，测试过程中，若使用993端口，找不到证书。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">[root@k8s-m1 ~]# kubectl logs -n gitlab -f mygitlab-mailroom-7b9d9c9cc-c9g2p
+ /scripts/set-config /etc /etc
Begin parsing .erb files from /etc
+ exec /bin/sh -c &#39;/usr/bin/mail_room -c /var/opt/gitlab/mail_room.yml&#39;
/usr/lib/ruby/2.6.0/net/protocol.rb:44:in `connect_nonblock&#39;: SSL_connect returned=1 errno=0 state=error: certificate verify failed (error number 1) (OpenSSL::SSL::SSLError)
	from /usr/lib/ruby/2.6.0/net/protocol.rb:44:in `ssl_socket_connect&#39;
	from /usr/lib/ruby/2.6.0/net/imap.rb:1534:in `start_tls_session&#39;
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">helm template . \
  --name ${release} \
  --set global.edition=ce \
  --set global.time_zone=Beijing \
  --set global.hosts.domain=k8s.abc \
  --set global.grafana.enabled=false \
  --set global.ingress.class=nginx \
  --set global.ingress.configureCertmanager=false \
  --set gitlab.unicorn.ingress.tls.secretName=release-gitlab-tls \
  --set registry.ingress.tls.secretName=release-registry-tls \
  --set minio.ingress.tls.secretName=release-minio-tls \
  --set global.ingress.annotations.&#34;cert-manager\.io\/cluster-issuer&#34;=cluster-issuer \
  --set global.registry.certificate.secret=${release}-registry-secret \
  --set global.email.display_name=hekai \
  --set global.email.from=hekai@abc.com \
  --set global.email.reply_to=hekai@abc.com \
  --set global.email.subject_suffix=@abc.com \
  --set global.smtp.enabled=true \
  --set global.smtp.address=smtp.abc.com \
  --set global.smtp.port=465 \
  --set global.smtp.authentication=login \
  --set global.smtp.openssl_verify_mode=none \
  --set global.smtp.starttls_auto=true \
  --set global.smtp.tls=true \
  --set global.smtp.password.key=gitlab-outgoing-smtp-pwd-key \
  --set global.smtp.password.secret=gitlab-outgoing-smtp-pwd-secret \
  --set global.smtp.user_name=hekai@abc.com \
  --set global.appConfig.incomingEmail.enabled=true \
  --set global.appConfig.incomingEmail.host=imap.abc.com \
  --set global.appConfig.incomingEmail.address=hekai@abc.com \
  --set global.appConfig.incomingEmail.port=143 \
  --set global.appConfig.incomingEmail.password.key=gitlab-incoming-imap-pwd-key \
  --set global.appConfig.incomingEmail.password.secret=gitlab-incoming-imap-pwd-secret \
  --set global.appConfig.incomingEmail.ssl=false \
  --set global.appConfig.incomingEmail.startTls=false \
  --set global.appConfig.incomingEmail.user=hekai@abc.com \
  --set certmanager.install=false \
  --set nginx-ingress.enabled=false \
  --set prometheus.install=false \
  --namespace gitlab \
  &gt; gitlab.yaml
</code></pre></td></tr></table>
</div>
</div><p>修改 postgresql 和 gitlab-runner 相关资源对应的命名空间</p>
<p>不加 incoming email配置了，作用并不大
helm v3 将 <code>--name</code> 改为了 <code>--name-template</code></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">helm template . \
  --name-template ${release} \
  --set global.edition=ce \
  --set global.time_zone=Beijing \
  --set global.hosts.domain=k8s.abc \
  --set global.grafana.enabled=false \
  --set global.ingress.class=nginx \
  --set global.ingress.configureCertmanager=false \
  --set gitlab.unicorn.ingress.tls.secretName=release-gitlab-tls \
  --set registry.ingress.tls.secretName=release-registry-tls \
  --set minio.ingress.tls.secretName=release-minio-tls \
  --set global.ingress.annotations.&#34;cert-manager\.io\/cluster-issuer&#34;=cluster-issuer \
  --set global.registry.certificate.secret=${release}-registry-secret \
  --set global.email.display_name=hekai \
  --set global.email.from=hekai@abc.com \
  --set global.email.reply_to=hekai@abc.com \
  --set global.email.subject_suffix=@abc.com \
  --set global.smtp.enabled=true \
  --set global.smtp.address=smtp.abc.com \
  --set global.smtp.port=465 \
  --set global.smtp.authentication=login \
  --set global.smtp.openssl_verify_mode=none \
  --set global.smtp.starttls_auto=true \
  --set global.smtp.tls=true \
  --set global.smtp.password.key=gitlab-outgoing-smtp-pwd-key \
  --set global.smtp.password.secret=gitlab-outgoing-smtp-pwd-secret \
  --set global.smtp.user_name=hekai@abc.com \
  --set certmanager.install=false \
  --set nginx-ingress.enabled=false \
  --set prometheus.install=false \
  --namespace gitlab \
  &gt; gitlab.yaml
</code></pre></td></tr></table>
</div>
</div><p>根据 doc/installation/storage.md 中描述，建议 storageclass 的 reclaimPolicy 使用 retain 模式</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">The default storage class should:

- Use fast SSD storage when available
- Set `reclaimPolicy` to `Retain`

&gt; Uninstalling GitLab without the `reclaimPolicy` set to `Retain` allows automated jobs to completely delete the volume, disk and data.
&gt; Some platforms set the default `reclaimPolicy` to `Delete`. The `gitaly` persistent volume claims do not follow this rule because
&gt; they belong to a [StatefulSet][].
</code></pre></td></tr></table>
</div>
</div><p>由于 上文创建的 managed-nfs-storage 的默认 reclaimPolicy 使用的是 Delete 模式，并且已经有 一些 容器使用 这个 storageclass 了，所以可以新建一个 nfs storage class 来支持 retain 模式，并把该 storage class 置为默认。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">[root@k8s-m1 deploy]# cat class.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: managed-nfs-storage
provisioner: nfs-storage # or choose another name, must match deployment&#39;s env PROVISIONER_NAME&#39;
parameters:
  archiveOnDelete: &#34;false&#34;
[root@k8s-m1 deploy]# cat class-retain.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs-storage-retain
  annotations:
    storageclass.kubernetes.io/is-default-class: &#34;true&#34; # 将该sc配置为默认的sc
provisioner: nfs-storage # or choose another name, must match deployment&#39;s env PROVISIONER_NAME&#39;
parameters:
  archiveOnDelete: &#34;false&#34;
# Supported policies: Delete, Retain
reclaimPolicy: Retain
[root@k8s-m1 deploy]# kubectl apply -f class.yaml -f class-retain.yaml
</code></pre></td></tr></table>
</div>
</div><p>安装 helm v3</p>
<ol>
<li>
<p>下载 helm v3 版本，下载地址：<a href="https://github.com/helm/helm/releases">https://github.com/helm/helm/releases</a></p>
</li>
<li>
<p>将下载得到的 tar 包解压</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">tar -zxvf helm-v3.0.0-linux-amd64.tar.gz
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>将 helm 二进制文件放到可执行目录中。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">mv linux-amd64/helm /usr/local/bin/helm
</code></pre></td></tr></table>
</div>
</div></li>
</ol>
<p>helm v3 取消了 tiller，所以不需要安装 服务端的tiller， helm 默认从 <code>~/.kube/config</code> 读取与k8s的连接信息。</p>
<p><a href="https://helm.sh/docs/intro/quickstart/">https://helm.sh/docs/intro/quickstart/</a></p>
<p>添加仓库，官方仓库需要翻墙，可以使用微软提供的镜像
参考：https://github.com/BurdenBear/kube-charts-mirror</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">[root@k8s-m1 ~]# helm repo add stable https://kubernetes-charts.storage.googleapis.com/
&#34;stable&#34; has been added to your repositories
[root@k8s-m1 ~]# helm repo remove stable
&#34;stable&#34; has been removed from your repositories
[root@k8s-m1 ~]# helm repo add stable http://mirror.azure.cn/kubernetes/charts/
&#34;stable&#34; has been added to your repositories
[root@k8s-m1 ~]# helm repo add incubator http://mirror.azure.cn/kubernetes/charts-incubator/
&#34;incubator&#34; has been added to your repositories
[root@k8s-m1 ~]# 
</code></pre></td></tr></table>
</div>
</div><h3 id="external-dns">external-dns</h3>
<p>debian中使用 ip 命令
apt install iproute2</p>
<p>容器内获取物理机ip
<a href="https://blog.csdn.net/kozazyh/article/details/79463688">https://blog.csdn.net/kozazyh/article/details/79463688</a>
<a href="https://github.com/kubernetes/kubernetes/issues/24657">https://github.com/kubernetes/kubernetes/issues/24657</a>
<a href="https://kubernetes.io/docs/tasks/inject-data-application/environment-variable-expose-pod-information/">https://kubernetes.io/docs/tasks/inject-data-application/environment-variable-expose-pod-information/</a></p>
<ol>
<li>使用 hostNetwork</li>
<li>使用环境变量的方式</li>
</ol>
<p>coredns 的 hosts 插件无法动态更新 /etc/hosts里的地址</p>
</div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>更新于 2019-10-13</span>
            </div>
            <div class="post-info-license"></div>
        </div>
        <div class="post-info-line">
            <div class="post-info-md"><span>
                            <a class="link-to-markdown" href="/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/index.md" target="_blank">阅读原始文档</a>
                        </span></div>
            <div class="post-info-share">
                <span><a href="javascript:void(0);" title="分享到 Twitter" data-sharer="twitter" data-url="https://msdemt.github.io/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/" data-title="k8s v1.15.0 二进制方式部署" data-hashtags="docker,k8s,devops"><i class="fab fa-twitter fa-fw"></i></a><a href="javascript:void(0);" title="分享到 Facebook" data-sharer="facebook" data-url="https://msdemt.github.io/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/" data-hashtag="docker"><i class="fab fa-facebook-square fa-fw"></i></a><a href="javascript:void(0);" title="分享到 Hacker News" data-sharer="hackernews" data-url="https://msdemt.github.io/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/" data-title="k8s v1.15.0 二进制方式部署"><i class="fab fa-hacker-news fa-fw"></i></a><a href="javascript:void(0);" title="分享到 Line" data-sharer="line" data-url="https://msdemt.github.io/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/" data-title="k8s v1.15.0 二进制方式部署"><i data-svg-src="https://cdn.jsdelivr.net/npm/simple-icons@2.14.0/icons/line.svg"></i></a><a href="javascript:void(0);" title="分享到 微博" data-sharer="weibo" data-url="https://msdemt.github.io/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/" data-title="k8s v1.15.0 二进制方式部署" data-ralateuid="xxxx"><i class="fab fa-weibo fa-fw"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw"></i>&nbsp;<a href="/tags/docker/">docker</a>,&nbsp;<a href="/tags/k8s/">k8s</a>,&nbsp;<a href="/tags/devops/">devops</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">返回</a></span>&nbsp;|&nbsp;<span><a href="/">主页</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" class="prev" rel="prev" title="Helm v2.14.3 官方文档阅读笔记"><i class="fas fa-angle-left fa-fw"></i>Helm v2.14.3 官方文档阅读笔记</a>
            <a href="/posts/k8s/metallb-v0.8.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" class="next" rel="next" title="MetalLB-v0.8.1-官方文档阅读笔记">MetalLB-v0.8.1-官方文档阅读笔记<i class="fas fa-angle-right fa-fw"></i></a></div>
</div>
</article></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line">由 <a href="https://gohugo.io/" target="_blank" rel="noopener noreffer" title="Hugo 0.80.0">Hugo</a> 强力驱动 | 主题 - <a href="https://github.com/dillonzq/LoveIt" target="_blank" rel="noopener noreffer" title="LoveIt 0.2.10"><i class="far fa-kiss-wink-heart fa-fw"></i> LoveIt</a>
                </div><div class="footer-line"><i class="far fa-copyright fa-fw"></i><span itemprop="copyrightYear">2019 - 2021</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="/" target="_blank">xxxx</a></span>&nbsp;|&nbsp;<span class="license"><a rel="license external nofollow noopener noreffer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="回到顶部">
                <i class="fas fa-arrow-up fa-fw"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="查看评论">
                <i class="fas fa-comment fa-fw"></i>
            </a>
        </div><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/smooth-scroll@16.1.3/dist/smooth-scroll.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/autocomplete.js@0.37.1/dist/autocomplete.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/algoliasearch@4.2.0/dist/algoliasearch-lite.umd.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lazysizes@5.2.2/lazysizes.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/clipboard@2.0.6/dist/clipboard.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/sharer.js@0.4.0/sharer.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"复制到剪贴板","maxShownLines":10},"comment":{},"search":{"algoliaAppID":"PASDMWALPK","algoliaIndex":"index.zh-cn","algoliaSearchKey":"b42948e51daaa93df92381c8e2ac0f93","highlightTag":"em","maxResultLength":10,"noResultsFound":"没有找到结果","snippetLength":50,"type":"algolia"}};</script><script type="text/javascript" src="/js/theme.min.js"></script></body>
</html>
