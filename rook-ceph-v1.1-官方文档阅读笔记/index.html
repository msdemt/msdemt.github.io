<!DOCTYPE html>
<html lang="zh-CN">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1">
        <title>Rook-Ceph-v1.1-官方文档阅读笔记 - MyBlog</title><meta name="Description" content="我的博客"><meta property="og:title" content="Rook-Ceph-v1.1-官方文档阅读笔记" />
<meta property="og:description" content="参考：https://rook.io/docs/rook/v1.1/ Rook Rook is an open source cloud-native storage orchestrator, providing the platform, framework, and support for a diverse set of storage solutions to natively integrate with cloud-native environments. Rook turns storage software into self-managing, self-scaling, and" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://msdemt.github.io/rook-ceph-v1.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" />
<meta property="og:image" content="https://msdemt.github.io/logo.png"/>
<meta property="article:published_time" content="2019-10-20T07:38:43+08:00" />
<meta property="article:modified_time" content="2019-10-20T07:38:43+08:00" />
<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://msdemt.github.io/logo.png"/>

<meta name="twitter:title" content="Rook-Ceph-v1.1-官方文档阅读笔记"/>
<meta name="twitter:description" content="参考：https://rook.io/docs/rook/v1.1/ Rook Rook is an open source cloud-native storage orchestrator, providing the platform, framework, and support for a diverse set of storage solutions to natively integrate with cloud-native environments. Rook turns storage software into self-managing, self-scaling, and"/>
<meta name="application-name" content="MyBlog">
<meta name="apple-mobile-web-app-title" content="MyBlog"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://msdemt.github.io/rook-ceph-v1.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" /><link rel="prev" href="https://msdemt.github.io/centos7%E6%90%AD%E5%BB%BAnfs-server-20191019/" /><link rel="next" href="https://msdemt.github.io/kubernetes-dashboard-v1.10.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" /><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/normalize.css@8.0.1/normalize.min.css"><link rel="stylesheet" href="/css/style.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.13.0/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.7.2/animate.min.css"><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "Rook-Ceph-v1.1-官方文档阅读笔记",
        "inLanguage": "zh-CN",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/msdemt.github.io\/rook-ceph-v1.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0\/"
        },"image": ["https:\/\/msdemt.github.io\/images\/Apple-Devices-Preview.png"],"genre": "posts","wordcount":  11380 ,
        "url": "https:\/\/msdemt.github.io\/rook-ceph-v1.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0\/","datePublished": "2019-10-20T07:38:43+08:00","dateModified": "2019-10-20T07:38:43+08:00","license": "This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher": {
            "@type": "Organization",
            "name": "xxxx","logo": "https:\/\/msdemt.github.io\/images\/avatar.png"},"author": {
                "@type": "Person",
                "name": "hekai"
            },"description": ""
    }
    </script></head>
    <body header-desktop="fixed" header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="MyBlog"><span class="header-title-pre"><i class='far fa-kiss-wink-heart fa-fw'></i></span>MyBlog</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> 所有文章 </a><a class="menu-item" href="/tags/"> 标签 </a><a class="menu-item" href="/categories/"> 分类 </a><a class="menu-item" href="/categories/documentation/"> 文档 </a><a class="menu-item" href="/about/"> 关于 </a><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="搜索">
                            <i class="fas fa-search fa-fw"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="清空">
                            <i class="fas fa-times-circle fa-fw"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                    <i class="fas fa-adjust fa-fw"></i>
                </a>
            </div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="MyBlog"><span class="header-title-pre"><i class='far fa-kiss-wink-heart fa-fw'></i></span>MyBlog</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="搜索">
                            <i class="fas fa-search fa-fw"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="清空">
                            <i class="fas fa-times-circle fa-fw"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        取消
                    </a>
                </div><a class="menu-item" href="/posts/" title="">所有文章</a><a class="menu-item" href="/tags/" title="">标签</a><a class="menu-item" href="/categories/" title="">分类</a><a class="menu-item" href="/categories/documentation/" title="">文档</a><a class="menu-item" href="/about/" title="">关于</a><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                <i class="fas fa-adjust fa-fw"></i>
            </a></div>
    </div>
</header>
<div class="search-dropdown desktop">
    <div id="search-dropdown-desktop"></div>
</div>
<div class="search-dropdown mobile">
    <div id="search-dropdown-mobile"></div>
</div>
<main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">目录</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animated flipInX">Rook-Ceph-v1.1-官方文档阅读笔记</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="/" title="Author" rel=" author" class="author"><i class="fas fa-user-circle fa-fw"></i>hekai</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime="2019-10-20">2019-10-20</time>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;约 11380 字&nbsp;
                <i class="far fa-clock fa-fw"></i>&nbsp;预计阅读 23 分钟&nbsp;</div>
        </div><div class="details toc" id="toc-static"  kept="">
                <div class="details-summary toc-title">
                    <span>目录</span>
                    <span><i class="details-icon fas fa-angle-right"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#rook">Rook</a></li>
    <li><a href="#quick-start-guides">Quick Start Guides</a>
      <ul>
        <li><a href="#ceph">Ceph</a>
          <ul>
            <li><a href="#prerequisites">Prerequisites</a>
              <ul>
                <li><a href="#minimum-version">Minimum Version</a></li>
                <li><a href="#privileges-and-rbac">Privileges and RBAC</a>
                  <ul>
                    <li><a href="#using-rook-with-pod-security-policies">Using Rook with Pod Security Policies</a></li>
                  </ul>
                </li>
                <li><a href="#flexvolume-configuration">Flexvolume Configuration</a>
                  <ul>
                    <li><a href="#not-a-listed-platform">Not a listed platform</a></li>
                    <li><a href="#atomic">Atomic</a></li>
                    <li><a href="#containerlinux">ContainerLinux</a></li>
                    <li><a href="#kubespray">Kubespray</a></li>
                    <li><a href="#openshift">OpenShift</a></li>
                    <li><a href="#rancher">Rancher</a></li>
                    <li><a href="#google-kubernetes-engine-gke">Google Kubernetes Engine (GKE)</a></li>
                    <li><a href="#azure-aks">Azure AKS</a></li>
                    <li><a href="#tectonic">Tectonic</a></li>
                    <li><a href="#openstack-magnum">OpenStack Magnum</a></li>
                    <li><a href="#custom-containerized-kubelet">Custom containerized kubelet</a></li>
                    <li><a href="#configuring-the-flexvolume-path">Configuring the FlexVolume path</a></li>
                  </ul>
                </li>
                <li><a href="#kernel-with-rbd-module">Kernel with RBD module</a></li>
                <li><a href="#kernel-modules-directory-configuration">Kernel modules directory configuration</a></li>
                <li><a href="#extra-agent-mounts">Extra agent mounts</a></li>
                <li><a href="#lvm-package">LVM package</a></li>
                <li><a href="#bootstrapping-kubernetes">Bootstrapping Kubernetes</a></li>
                <li><a href="#support-for-authenticated-docker-registries">Support for authenticated docker registries</a>
                  <ul>
                    <li><a href="#example-setup-for-a-ceph-cluster">Example setup for a ceph cluster</a></li>
                  </ul>
                </li>
              </ul>
            </li>
            <li><a href="#tldr">TL;DR</a></li>
            <li><a href="#deploy-the-rook-operator">Deploy the Rook Operator</a>
              <ul>
                <li><a href="#ceph-examples">Ceph Examples</a>
                  <ul>
                    <li><a href="#common-resources">Common Resources</a></li>
                    <li><a href="#operator">Operator</a></li>
                    <li><a href="#cluster-crd">Cluster CRD</a></li>
                    <li><a href="#setting-up-consumable-storage">Setting up consumable storage</a></li>
                  </ul>
                </li>
                <li><a href="#ceph-operator-helm-chart">Ceph Operator Helm Chart</a>
                  <ul>
                    <li><a href="#introduction">Introduction</a></li>
                    <li><a href="#prerequisites-1">Prerequisites</a></li>
                    <li><a href="#installing">Installing</a></li>
                    <li><a href="#uninstalling-the-chart">Uninstalling the Chart</a></li>
                    <li><a href="#configuration">Configuration</a></li>
                  </ul>
                </li>
              </ul>
            </li>
            <li><a href="#create-a-rook-ceph-cluster">Create a Rook Ceph Cluster</a></li>
            <li><a href="#storage">Storage</a>
              <ul>
                <li><a href="#block-storage">Block Storage</a>
                  <ul>
                    <li><a href="#prerequisites-2">Prerequisites</a></li>
                    <li><a href="#provision-storage">Provision Storage</a></li>
                    <li><a href="#consume-the-storage-wordpress-sample">Consume the storage: Wordpress sample</a></li>
                    <li><a href="#consume-the-storage-toolbox">Consume the storage: Toolbox</a></li>
                    <li><a href="#teardown">Teardown</a></li>
                    <li><a href="#flex-driver">Flex Driver</a></li>
                    <li><a href="#advanced-example-erasure-coded-block-storage">Advanced Example: Erasure Coded Block Storage</a></li>
                  </ul>
                </li>
                <li><a href="#object-storage">Object Storage</a>
                  <ul>
                    <li><a href="#prerequisites-3">Prerequisites</a></li>
                    <li><a href="#create-an-object-store">Create an Object Store</a></li>
                    <li><a href="#create-a-user">Create a User</a></li>
                    <li><a href="#consume-the-object-storage">Consume the Object Storage</a></li>
                    <li><a href="#access-external-to-the-cluster">Access External to the Cluster</a></li>
                  </ul>
                </li>
                <li><a href="#shared-file-system">Shared File System</a>
                  <ul>
                    <li><a href="#prerequisites-4">Prerequisites</a></li>
                    <li><a href="#multiple-file-systems-not-supported">Multiple File Systems Not Supported</a></li>
                    <li><a href="#create-the-file-system">Create the File System</a></li>
                    <li><a href="#provision-storage-1">Provision Storage</a></li>
                    <li><a href="#consume-the-shared-file-system-k8s-registry-sample">Consume the Shared File System: K8s Registry Sample</a></li>
                    <li><a href="#kernel-version-requirement">Kernel Version Requirement</a></li>
                    <li><a href="#consume-the-shared-file-system-toolbox">Consume the Shared File System: Toolbox</a></li>
                    <li><a href="#teardown-1">Teardown</a></li>
                    <li><a href="#flex-driver-1">Flex Driver</a></li>
                  </ul>
                </li>
              </ul>
            </li>
            <li><a href="#ceph-dashboard">Ceph Dashboard</a>
              <ul>
                <li><a href="#enable-the-dashboard">Enable the Dashboard</a>
                  <ul>
                    <li><a href="#credentials">Credentials</a></li>
                  </ul>
                </li>
                <li><a href="#configure-the-dashboard">Configure the Dashboard</a></li>
                <li><a href="#viewing-the-dashboard-external-to-the-cluster">Viewing the Dashboard External to the Cluster</a>
                  <ul>
                    <li><a href="#node-port">Node Port</a></li>
                    <li><a href="#load-balancer">Load Balancer</a></li>
                    <li><a href="#ingress-controller">Ingress Controller</a></li>
                  </ul>
                </li>
                <li><a href="#enabling-dashboard-object-gateway-management">Enabling Dashboard Object Gateway management</a></li>
              </ul>
            </li>
            <li><a href="#tools">Tools</a>
              <ul>
                <li><a href="#rook-toolbox">Rook Toolbox</a>
                  <ul>
                    <li><a href="#running-the-toolbox-in-kubernetes">Running the Toolbox in Kubernetes</a></li>
                    <li><a href="#troubleshooting-without-the-toolbox">Troubleshooting without the Toolbox</a></li>
                  </ul>
                </li>
              </ul>
            </li>
            <li><a href="#monitoring">Monitoring</a>
              <ul>
                <li><a href="#prometheus-operator">Prometheus Operator</a></li>
                <li><a href="#prometheus-instances">Prometheus Instances</a></li>
                <li><a href="#prometheus-web-console">Prometheus Web Console</a></li>
                <li><a href="#prometheus-consoles">Prometheus Consoles</a></li>
                <li><a href="#prometheus-alerts">Prometheus Alerts</a></li>
                <li><a href="#grafana-dashboards">Grafana Dashboards</a></li>
                <li><a href="#teardown-2">Teardown</a></li>
                <li><a href="#special-cases">Special Cases</a>
                  <ul>
                    <li><a href="#tectonic-bare-metal">Tectonic Bare Metal</a></li>
                    <li><a href="#csi-liveness">CSI Liveness</a></li>
                  </ul>
                </li>
              </ul>
            </li>
            <li><a href="#teardown-3">Teardown</a>
              <ul>
                <li><a href="#delete-the-block-and-file-artifacts">Delete the Block and File artifacts</a></li>
                <li><a href="#delete-the-cephcluster-crd">Delete the CephCluster CRD</a></li>
                <li><a href="#delete-the-operator-and-related-resources">Delete the Operator and related Resources</a></li>
                <li><a href="#delete-the-data-on-hosts">Delete the data on hosts</a></li>
                <li><a href="#troubleshooting">Troubleshooting</a>
                  <ul>
                    <li><a href="#removing-the-cluster-crd-finalizer">Removing the Cluster CRD Finalizer</a></li>
                  </ul>
                </li>
              </ul>
            </li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><blockquote>
<p>参考：<a href="https://rook.io/docs/rook/v1.1/">https://rook.io/docs/rook/v1.1/</a></p>
</blockquote>
<h2 id="rook">Rook</h2>
<p>Rook is an open source <code>cloud-native storage orchestrator</code>, providing the platform, framework, and support for a diverse set of storage solutions to natively integrate with cloud-native environments.</p>
<p>Rook turns storage software into <strong>self-managing</strong>, <strong>self-scaling</strong>, and <strong>self-healing</strong> storage services. It does this by automating deployment, bootstrapping, configuration, provisioning, scaling, upgrading, migration, disaster recovery, monitoring, and resource management. Rook uses the facilities provided by the underlying cloud-native container management, scheduling and orchestration platform to perform its duties.</p>
<p>Rook integrates deeply into cloud native environments leveraging extension points and providing a seamless experience for scheduling, lifecycle management, resource management, security, monitoring, and user experience.</p>
<p>For more details about the status of storage solutions currently supported by Rook, please refer to <a href="https://github.com/rook/rook/blob/master/README.md#project-status" target="_blank" rel="noopener noreffer">the project status</a> section of the Rook repository. We plan to continue adding support for other storage systems and environments based on community demand and engagement in future releases.</p>
<h2 id="quick-start-guides">Quick Start Guides</h2>
<p>Starting Rook in your cluster is as simple as two <code>kubectl</code> commands. See our <a href="https://rook.io/docs/rook/v1.1/quickstart-toc.html" target="_blank" rel="noopener noreffer">Quickstart</a> guide for the details on what you need to get going.</p>
<table>
<thead>
<tr>
<th>Storage Provider</th>
<th>Status</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://rook.io/docs/rook/v1.1/ceph-quickstart.html" target="_blank" rel="noopener noreffer">Ceph</a></td>
<td>V1</td>
<td>Ceph is a highly scalable distributed storage solution for block storage, object storage, and shared file systems with years of production deployments.</td>
</tr>
<tr>
<td><a href="https://rook.io/docs/rook/v1.1/edgefs-quickstart.html" target="_blank" rel="noopener noreffer">EdgeFS</a></td>
<td>V1</td>
<td>EdgeFS is high-performance and low-latency object storage system with Geo-Transparent data access via standard protocols (S3, NFS, iSCSI) from on-prem, private/public clouds or small footprint edge (IoT) devices.</td>
</tr>
<tr>
<td><a href="https://rook.io/docs/rook/v1.1/cassandra.html" target="_blank" rel="noopener noreffer">Cassandra</a></td>
<td>Alpha</td>
<td>Cassandra is a highly available NoSQL database featuring lightning fast performance, tunable consistency and massive scalability.</td>
</tr>
<tr>
<td><a href="https://rook.io/docs/rook/v1.1/cockroachdb.html" target="_blank" rel="noopener noreffer">CockroachDB</a></td>
<td>Alpha</td>
<td>CockroachDB is a cloud-native SQL database for building global, scalable cloud services that survive disasters.</td>
</tr>
<tr>
<td><a href="https://rook.io/docs/rook/v1.1/minio-object-store.html" target="_blank" rel="noopener noreffer">Minio</a></td>
<td>Alpha</td>
<td>Minio is a high performance distributed object storage server, designed for large-scale private cloud infrastructure.</td>
</tr>
<tr>
<td><a href="https://rook.io/docs/rook/v1.1/nfs.html" target="_blank" rel="noopener noreffer">NFS</a></td>
<td>Alpha</td>
<td>NFS allows remote hosts to mount file systems over a network and interact with those file systems as though they are mounted locally.</td>
</tr>
<tr>
<td><a href="https://rook.io/docs/rook/v1.1/yugabytedb.html" target="_blank" rel="noopener noreffer">YugabyteDB</a></td>
<td>Alpha</td>
<td>YugaByteDB is a high-performance, cloud-native distributed SQL database which can tolerate disk, node, zone and region failures automatically.</td>
</tr>
</tbody>
</table>
<h3 id="ceph">Ceph</h3>
<p>This guide will walk you through the basic setup of a Ceph cluster and enable you to consume <code>block</code>, <code>object</code>, and <code>file storage</code> from other pods running in your cluster.</p>
<h4 id="prerequisites">Prerequisites</h4>
<p>Rook can be installed on any existing Kubernetes clusters as long as it meets the minimum version and have the required privilege to run in the cluster (see below for more information). If you dont have a Kubernetes cluster, you can quickly set one up using <a href="#minikube" rel="">Minikube</a>, <a href="#kubeadm" rel="">Kubeadm</a> or <a href="#new-local-kubernetes-cluster-with-vagrant" rel="">CoreOS/Vagrant</a>.</p>
<p>If you are using <code>dataDirHostPath</code> to persist rook data on kubernetes hosts, make sure your host has at least 5GB of space available on the specified path.</p>
<h5 id="minimum-version">Minimum Version</h5>
<p>Kubernetes <strong>v1.10</strong> or higher is supported by Rook.</p>
<h5 id="privileges-and-rbac">Privileges and RBAC</h5>
<p>Rook requires privileges to manage the storage in your cluster. See the details <a href="https://rook.io/docs/rook/v1.1/psp.html" target="_blank" rel="noopener noreffer">here</a> or below for setting up Rook in a Kubernetes cluster with Pod Security Policies enabled.</p>
<h6 id="using-rook-with-pod-security-policies">Using Rook with Pod Security Policies</h6>
<p><em><strong>Cluster Role</strong></em></p>
<p><strong>NOTE</strong> Cluster role configuration is only needed when you are not already <code>cluster-admin</code> in your Kubernetes cluster!</p>
<p>Creating the Rook operator requires privileges for setting up RBAC. To launch the operator you need to have created your user certificate that is bound to ClusterRole <code>cluster-admin</code>.</p>
<p>One simple way to achieve it is to assign your certificate with the <code>system:masters</code> group:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">-subj <span class="s2">&#34;/CN=admin/O=system:masters&#34;</span>
</code></pre></td></tr></table>
</div>
</div><p><code>system:masters</code> is a special group that is bound to <code>cluster-admin</code> ClusterRole, but it can&rsquo;t be easily revoked so be careful with taking that route in a production setting.</p>
<p>Binding individual certificate to ClusterRole <code>cluster-admin</code> is revocable by deleting the ClusterRoleBinding.</p>
<p><em><strong>RBAC for PodSecurityPolicies</strong></em></p>
<p>If you have activated the <a href="https://kubernetes.io/docs/admin/admission-controllers/#podsecuritypolicy" target="_blank" rel="noopener noreffer">PodSecurityPolicy Admission Controller</a> and thus are using <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/" target="_blank" rel="noopener noreffer">PodSecurityPolicies</a>, you will require additional <code>(Cluster)RoleBindings</code> for the different <code>ServiceAccounts</code> Rook uses to start the Rook Storage Pods.</p>
<p>Security policies will differ for different backends. See Ceph&rsquo;s Pod Security Policies set up in its
<a href="https://github.com/rook/rook/blob/release-1.1/cluster/examples/kubernetes/ceph/common.yaml" target="_blank" rel="noopener noreffer">common.yaml</a> for an example of how this is done in practice.</p>
<p><strong>Note</strong>: You do not have to perform these steps if you do not have the <code>PodSecurityPolicy</code> Admission Controller activated!</p>
<p><strong>PodSecurityPolicy</strong></p>
<p>You need at least one <code>PodSecurityPolicy</code> that allows privileged <code>Pod</code> execution. Here is an example
which should be more permissive than is needed for any backend:</p>
<p>这个 PodSecurityPolicy 在 <a href="https://github.com/rook/rook/blob/release-1.1/cluster/examples/kubernetes/ceph/common.yaml" target="_blank" rel="noopener noreffer">common.yaml</a> 也有。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="k">apiVersion</span><span class="p">:</span><span class="w"> </span>policy/v1beta1<span class="w">
</span><span class="w"></span><span class="k">kind</span><span class="p">:</span><span class="w"> </span>PodSecurityPolicy<span class="w">
</span><span class="w"></span><span class="k">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="k">name</span><span class="p">:</span><span class="w"> </span>privileged<span class="w">
</span><span class="w"></span><span class="k">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="k">fsGroup</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="k">rule</span><span class="p">:</span><span class="w"> </span>RunAsAny<span class="w">
</span><span class="w">  </span><span class="k">privileged</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">  </span><span class="k">runAsUser</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="k">rule</span><span class="p">:</span><span class="w"> </span>RunAsAny<span class="w">
</span><span class="w">  </span><span class="k">seLinux</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="k">rule</span><span class="p">:</span><span class="w"> </span>RunAsAny<span class="w">
</span><span class="w">  </span><span class="k">supplementalGroups</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="k">rule</span><span class="p">:</span><span class="w"> </span>RunAsAny<span class="w">
</span><span class="w">  </span><span class="k">volumes</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="s1">&#39;*&#39;</span><span class="w">
</span><span class="w">  </span><span class="k">allowedCapabilities</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="s1">&#39;*&#39;</span><span class="w">
</span><span class="w">  </span><span class="k">hostPID</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">  </span><span class="c"># hostNetwork is required for using host networking</span><span class="w">
</span><span class="w">  </span><span class="k">hostNetwork</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p><strong>Hint</strong>: Allowing <code>hostNetwork</code> usage is required when using <code>hostNetwork: true</code> in a Cluster <code>CustomResourceDefinition</code>!</p>
<p>You are then also required to allow the usage of <code>hostPorts</code> in the <code>PodSecurityPolicy</code>. The given
port range will allow all ports:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="w">  </span><span class="k">hostPorts</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="c"># Ceph msgr2 port</span><span class="w">
</span><span class="w">    </span>- <span class="k">min</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">      </span><span class="k">max</span><span class="p">:</span><span class="w"> </span><span class="m">65535</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><h5 id="flexvolume-configuration">Flexvolume Configuration</h5>
<p>Rook uses <a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md" target="_blank" rel="noopener noreffer">FlexVolume</a> to integrate with Kubernetes for performing storage operations. In some operating systems where Kubernetes is deployed, the <a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md#prerequisites" target="_blank" rel="noopener noreffer">default Flexvolume plugin directory</a> (the directory where FlexVolume drivers are installed) is <strong>read-only</strong>.</p>
<p>This is the case for Kubernetes deployments on:</p>
<ul>
<li><a href="https://www.projectatomic.io/" target="_blank" rel="noopener noreffer">Atomic</a></li>
<li><a href="https://coreos.com/os/docs/latest/" target="_blank" rel="noopener noreffer">ContainerLinux</a> (previously named CoreOS)</li>
<li><a href="https://www.openshift.com/" target="_blank" rel="noopener noreffer">OpenShift</a></li>
<li><a href="http://rancher.com/" target="_blank" rel="noopener noreffer">Rancher</a></li>
<li><a href="https://cloud.google.com/kubernetes-engine/" target="_blank" rel="noopener noreffer">Google Kubernetes Engine (GKE)</a></li>
<li><a href="https://docs.microsoft.com/en-us/azure/aks/" target="_blank" rel="noopener noreffer">Azure AKS</a></li>
</ul>
<p>Especially in these environments, the kubelet needs to be told to use a different FlexVolume plugin directory that is accessible and read/write (<code>rw</code>). These steps need to be carried out on <strong>all nodes</strong> in your cluster.</p>
<p>Please refer to the section that is applicable to your environment/platform, it contains more information on FlexVolume on your platform.</p>
<h6 id="not-a-listed-platform">Not a listed platform</h6>
<p>If you are not using a platform that is listed above and the path <code>/usr/libexec/kubernetes/kubelet-plugins/volume/exec/</code> is read/write, you don&rsquo;t need to configure anything.</p>
<p>That is because <code>/usr/libexec/kubernetes/kubelet-plugins/volume/exec/</code> is the kubelet default FlexVolume path and Rook assumes the default FlexVolume path if not set differently.</p>
<p>If running <code>mkdir -p /usr/libexec/kubernetes/kubelet-plugins/volume/exec/</code> should give you an error about read-only filesystem, you need to use the <a href="#most-common-readwrite-flexvolume-path" rel="">most common read/write FlexVolume path</a> and configure it on the Rook operator and kubelet.</p>
<p>Continue with <a href="#configuring-the-flexvolume-path" rel="">configuring the FlexVolume path</a> to configure Rook to use the FlexVolume path.</p>
<h6 id="atomic">Atomic</h6>
<p>See the <a href="#openshift" rel="">OpenShift</a> section, unless running with OpenStack Magnum, then see <a href="#openstack-magnum" rel="">OpenStack Magnum</a> section.</p>
<h6 id="containerlinux">ContainerLinux</h6>
<p>Use the <a href="#most-common-readwrite-flexvolume-path" rel="">Most common read/write FlexVolume path</a> for the next steps.</p>
<p>The kubelet&rsquo;s systemD unit file can be located at: <code>/etc/systemd/system/kubelet.service</code>.</p>
<p>Continue with <a href="#configuring-the-flexvolume-path" rel="">configuring the FlexVolume path</a> to configure Rook to use the FlexVolume path.</p>
<h6 id="kubespray">Kubespray</h6>
<p>Kubespray uses a non-standard <a href="https://github.com/kubernetes-sigs/kubespray/blob/master/roles/kubernetes/node/defaults/main.yml#L55" target="_blank" rel="noopener noreffer">FlexVolume plugin directory</a>: <code>/var/lib/kubelet/volume-plugins</code>.</p>
<p>The Kubespray configured kubelet is already configured to use that directory.</p>
<p>Continue with <a href="#configuring-the-flexvolume-path" rel="">configuring the FlexVolume path</a> to configure Rook to use the FlexVolume path.</p>
<h6 id="openshift">OpenShift</h6>
<p>To find out which FlexVolume directory path you need to set on the Rook operator, please look at the OpenShift docs of the version you are using, <a href="https://docs.openshift.org/latest/install_config/persistent_storage/persistent_storage_flex_volume.html#flexvolume-installation" target="_blank" rel="noopener noreffer">latest OpenShift Flexvolume docs</a> (they also contain the FlexVolume path for Atomic).</p>
<p>Continue with <a href="#configuring-the-flexvolume-path" rel="">configuring the FlexVolume path</a> to configure Rook to use the FlexVolume path.</p>
<h6 id="rancher">Rancher</h6>
<p>Rancher provides an easy way to configure kubelet. The FlexVolume flag will be shown later on in the <a href="#configuring-the-flexvolume-path" rel="">configuring the FlexVolume path</a>.</p>
<p>It can be provided to the kubelet configuration template at deployment time or by using the <code>up to date</code> feature if Kubernetes is already deployed.</p>
<p>Rancher deploys kubelet as a docker container, you need to mount the host&rsquo;s flexvolume path into the kubelet image as a volume, this can be done in the <code>extra_binds</code> section of the kubelet cluster config.</p>
<p>Configure the Rancher deployed kubelet by updating the <code>cluster.yml</code> file kubelet section:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="k">services</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="k">kubelet</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="k">extra_args</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="k">volume-plugin-dir</span><span class="p">:</span><span class="w"> </span>/usr/libexec/kubernetes/kubelet-plugins/volume/exec<span class="w">
</span><span class="w">    </span><span class="k">extra_binds</span><span class="p">:</span><span class="w">
</span><span class="w">      </span>- /usr/libexec/kubernetes/kubelet-plugins/volume/exec<span class="p">:</span>/usr/libexec/kubernetes/kubelet-plugins/volume/exec<span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>If you&rsquo;re using <a href="https://github.com/rancher/rke" target="_blank" rel="noopener noreffer">rke</a>, run <code>rke up</code>, this will update and restart your kubernetes cluster system components, in this case the kubelet docker instance(s) will get restarted with the new volume bind and volume plugin dir flag.</p>
<p>The default FlexVolume path for Rancher is <code>/usr/libexec/kubernetes/kubelet-plugins/volume/exec</code> which is also the default FlexVolume path for the Rook operator.</p>
<p>If the default path as above is used no further configuration is required, otherwise if a different path is used the Rook operator will need to be reconfigured, to do this continue with <a href="#configuring-the-flexvolume-path" rel="">configuring the FlexVolume path</a> to configure Rook to use the FlexVolume path.</p>
<h6 id="google-kubernetes-engine-gke">Google Kubernetes Engine (GKE)</h6>
<p>Google&rsquo;s Kubernetes Engine uses a non-standard FlexVolume plugin directory: <code>/home/kubernetes/flexvolume</code></p>
<p>The kubelet on GKE is already configured to use that directory.</p>
<p>Continue with <a href="#configuring-the-flexvolume-path" rel="">configuring the FlexVolume path</a> to configure Rook to use the FlexVolume path.</p>
<h6 id="azure-aks">Azure AKS</h6>
<p>AKS uses a non-standard FlexVolume plugin directory: <code>/etc/kubernetes/volumeplugins</code></p>
<p>The kubelet on AKS is already configured to use that directory.</p>
<p>Continue with <a href="#configuring-the-flexvolume-path" rel="">configuring the FlexVolume path</a> to configure Rook to use the FlexVolume path.</p>
<h6 id="tectonic">Tectonic</h6>
<p>Follow <a href="https://rook.io/docs/rook/v1.1/tectonic.html" target="_blank" rel="noopener noreffer">these instructions</a> to configure the Flexvolume plugin for Rook on Tectonic during ContainerLinux node ignition file provisioning.
If you want to use Rook with an already provisioned Tectonic cluster, please refer to the <a href="#containerlinux" rel="">ContainerLinux</a> section.</p>
<p>Continue with <a href="#configuring-the-flexvolume-path" rel="">configuring the FlexVolume path</a> to configure Rook to use the FlexVolume path.</p>
<h6 id="openstack-magnum">OpenStack Magnum</h6>
<p>OpenStack Magnum is using Atomic, which uses a non-standard FlexVolume plugin directory at:  <code>/var/lib/kubelet/volumeplugins</code></p>
<p>The kubelet in OpenStack Magnum is already configured to use that directory.</p>
<p>You will need to use this value when <a href="#configuring-the-rook-operator" rel="">configuring the Rook operator</a></p>
<h6 id="custom-containerized-kubelet">Custom containerized kubelet</h6>
<p>Use the <a href="#most-common-readwrite-flexvolume-path" rel="">most common read/write FlexVolume path</a> for the next steps.</p>
<p>If your kubelet is running as a (Docker, rkt, etc) container you need to make sure that this directory from the host is reachable by the kubelet inside the container.</p>
<p>Continue with <a href="#configuring-the-flexvolume-path" rel="">configuring the FlexVolume path</a> to configure Rook to use the FlexVolume path.</p>
<h6 id="configuring-the-flexvolume-path">Configuring the FlexVolume path</h6>
<p>If the environment specific section doesn&rsquo;t mention a FlexVolume path in this doc or external docs, please refer to the <a href="#most-common-readwrite-flexvolume-path" rel="">most common read/write FlexVolume path</a> section, before continuing to <a href="#configuring-the-flexvolume-path" rel="">configuring the FlexVolume path</a>.</p>
<p><em><strong>Most common read/write FlexVolume path</strong></em></p>
<p>The most commonly used read/write FlexVolume path on most systems is <code>/var/lib/kubelet/volumeplugins</code>.</p>
<p>This path is commonly used for FlexVolume because <code>/var/lib/kubelet</code> is read write on most systems.</p>
<p><em><strong>Configuring the Rook operator</strong></em></p>
<p>You must provide the above found FlexVolume path when deploying the <a href="https://github.com/rook/rook/blob/master/cluster/examples/kubernetes/ceph/operator.yaml" target="_blank" rel="noopener noreffer">rook-operator</a> by setting the environment variable <code>FLEXVOLUME_DIR_PATH</code>.
For example:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="k">env</span><span class="p">:</span><span class="w">
</span><span class="w"></span><span class="p">[</span>...<span class="p">]</span><span class="w">
</span><span class="w"></span>- <span class="k">name</span><span class="p">:</span><span class="w"> </span>FLEXVOLUME_DIR_PATH<span class="w">
</span><span class="w">  </span><span class="k">value</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;/var/lib/kubelet/volumeplugins&#34;</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>(In the <code>operator.yaml</code> manifest replace <code>&lt;PathToFlexVolumes&gt;</code> with the path or if you use helm set the <code>agent.flexVolumeDirPath</code> to the FlexVolume path)</p>
<p><em><strong>Configuring the Kubernetes kubelet</strong></em></p>
<p>You need to add the flexvolume flag with the path to all nodes&rsquo;s kubelet in the Kubernetes cluster:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">--volume-plugin-dir<span class="o">=</span>PATH_TO_FLEXVOLUME
</code></pre></td></tr></table>
</div>
</div><p>(Where the <code>PATH_TO_FLEXVOLUME</code> is the above found FlexVolume path)</p>
<p>The location where you can set the kubelet FlexVolume path (flag) depends on your platform.
Please refer to your platform documentation for that and/or the <a href="#platform-specific-flexvolume-path" rel="">platform specific FlexVolume path</a> for information about that.</p>
<p>After adding the flag to kubelet, kubelet must be restarted for it to pick up the new flag.</p>
<h5 id="kernel-with-rbd-module">Kernel with RBD module</h5>
<p>Rook Ceph requires a Linux kernel built with the RBD module. Many distributions of Linux have this module but some don’t, e.g. the GKE Container-Optimised OS (COS) does not have RBD. You can test your Kubernetes nodes by running modprobe rbd. If it says ‘not found’, you may have to <a href="https://rook.io/docs/rook/master/common-issues.html#rook-agent-rbd-module-missing-error" target="_blank" rel="noopener noreffer">rebuild your kernel</a> or choose a different Linux distribution.</p>
<h5 id="kernel-modules-directory-configuration">Kernel modules directory configuration</h5>
<p>Normally, on Linux, kernel modules can be found in <code>/lib/modules</code>. However, there are some distributions that put them elsewhere. In that case the environment variable <code>LIB_MODULES_DIR_PATH</code> can be used to override the default. Also see the documentation in <a href="https://rook.io/docs/rook/v1.1/helm-operator.html" target="_blank" rel="noopener noreffer">helm-operator</a> on the parameter <code>agent.libModulesDirPath</code>. One notable distribution where this setting is useful would be <a href="https://nixos.org/" target="_blank" rel="noopener noreffer">NixOS</a>.</p>
<h5 id="extra-agent-mounts">Extra agent mounts</h5>
<p>On certain distributions it may be necessary to mount additional directories into the agent container. That is what the environment variable <code>AGENT_MOUNTS</code> is for. Also see the documentation in <a href="https://rook.io/docs/rook/v1.1/helm-operator.html" target="_blank" rel="noopener noreffer">helm-operator</a> on the parameter <code>agent.mounts</code>. The format of the variable content should be <code>mountname1=/host/path1:/container/path1,mountname2=/host/path2:/container/path2</code>.</p>
<h5 id="lvm-package">LVM package</h5>
<p>Some Linux distributions do not ship with the <code>lvm2</code> package. This package is required on all storage nodes in your k8s cluster. Please install it using your Linux distribution’s package manager; for example:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="c1"># Centos</span>
sudo yum install -y lvm2

<span class="c1"># Ubuntu</span>
sudo apt-get install -y lvm2
</code></pre></td></tr></table>
</div>
</div><h5 id="bootstrapping-kubernetes">Bootstrapping Kubernetes</h5>
<p>Rook will run wherever Kubernetes is running. Here are some simple environments to help you get started with Rook.</p>
<p><em><strong>Minikube</strong></em><br>
To install minikube, refer to this <a href="https://github.com/kubernetes/minikube/releases" target="_blank" rel="noopener noreffer">page</a>. Once you have <code>minikube</code> installed, start a cluster by doing the following:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">$ minikube start
Starting <span class="nb">local</span> Kubernetes cluster...
Starting VM...
SSH-ing files into VM...
Setting up certs...
Starting cluster components...
Connecting to cluster...
Setting up kubeconfig...
Kubectl is now configured to use the cluster.
</code></pre></td></tr></table>
</div>
</div><p>After these steps, your minikube cluster is ready to install Rook on.</p>
<p><em><strong>Kubeadm</strong></em><br>
You can easily spin up Rook on top of a <code>kubeadm</code> cluster. You can find the instructions on how to install <code>kubeadm</code> in the <a href="https://kubernetes.io/docs/setup/independent/install-kubeadm/" target="_blank" rel="noopener noreffer">Install</a> <code>kubeadm</code> page.</p>
<p>By using <code>kubeadm</code>, you can use Rook in just a few minutes!</p>
<p><em><strong>New local Kubernetes cluster with Vagrant</strong></em><br>
For a quick start with a new local cluster, use the Rook fork of <a href="https://github.com/rook/coreos-kubernetes" target="_blank" rel="noopener noreffer">coreos-kubernetes</a>. This will bring up a multi-node Kubernetes cluster with <code>vagrant</code> and CoreOS virtual machines ready to use Rook immediately.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">git clone https://github.com/rook/coreos-kubernetes.git
<span class="nb">cd</span> coreos-kubernetes/multi-node/vagrant
vagrant up
<span class="nb">export</span> <span class="nv">KUBECONFIG</span><span class="o">=</span><span class="s2">&#34;</span><span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span><span class="s2">/kubeconfig&#34;</span>
kubectl config use-context vagrant-multi
</code></pre></td></tr></table>
</div>
</div><p>Then wait for the cluster to come up and verify that kubernetes is done initializing (be patient, it takes a bit):</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">kubectl cluster-info
</code></pre></td></tr></table>
</div>
</div><p>Once you see a url response, your cluster is <a href="https://rook.io/docs/rook/v1.1/ceph-quickstart.html#deploy-rook" target="_blank" rel="noopener noreffer">ready for use by Rook</a>.</p>
<h5 id="support-for-authenticated-docker-registries">Support for authenticated docker registries</h5>
<p>If you want to use an image from authenticated docker registry (e.g. for image cache/mirror), you&rsquo;ll need to add an <code>imagePullSecret</code> to all relevant service accounts. This way all pods created by the operator (for service account: <code>rook-ceph-system</code>) or all new pods in the namespace (for service account: <code>default</code>) will have the <code>imagePullSecret</code> added to their spec.</p>
<p>The whole process is described in the <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#add-imagepullsecrets-to-a-service-account" target="_blank" rel="noopener noreffer">official kubernetes documentation</a>.</p>
<h6 id="example-setup-for-a-ceph-cluster">Example setup for a ceph cluster</h6>
<p>To get you started, here&rsquo;s a quick rundown for the ceph example from the <a href="https://rook.io/Documentation/ceph-quickstart.md" target="_blank" rel="noopener noreffer">quickstart guide</a>.</p>
<p>First, we&rsquo;ll create the secret for our registry as described <a href="https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod" target="_blank" rel="noopener noreffer">here</a>:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="c1"># for namespace rook-ceph</span>
kubectl -n rook-ceph create secret docker-registry my-registry-secret --docker-server<span class="o">=</span>DOCKER_REGISTRY_SERVER --docker-username<span class="o">=</span>DOCKER_USER --docker-password<span class="o">=</span>DOCKER_PASSWORD --docker-email<span class="o">=</span>DOCKER_EMAIL

<span class="c1"># and for namespace rook-ceph (cluster)</span>
kubectl -n rook-ceph create secret docker-registry my-registry-secret --docker-server<span class="o">=</span>DOCKER_REGISTRY_SERVER --docker-username<span class="o">=</span>DOCKER_USER --docker-password<span class="o">=</span>DOCKER_PASSWORD --docker-email<span class="o">=</span>DOCKER_EMAIL
</code></pre></td></tr></table>
</div>
</div><p>Next we&rsquo;ll add the following snippet to all relevant service accounts as described <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#add-imagepullsecrets-to-a-service-account" target="_blank" rel="noopener noreffer">here</a>:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="k">imagePullSecrets</span><span class="p">:</span><span class="w">
</span><span class="w"></span>- <span class="k">name</span><span class="p">:</span><span class="w"> </span>my-registry-secret<span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>The service accounts are:</p>
<ul>
<li><code>rook-ceph-system</code> (namespace: <code>rook-ceph</code>): Will affect all pods created by the rook operator in the <code>rook-ceph</code> namespace.</li>
<li><code>default</code> (namespace: <code>rook-ceph</code>): Will affect most pods in the <code>rook-ceph</code> namespace.</li>
<li><code>rook-ceph-mgr</code> (namespace: <code>rook-ceph</code>): Will affect the MGR pods in the <code>rook-ceph</code> namespace.</li>
<li><code>rook-ceph-osd</code> (namespace: <code>rook-ceph</code>): Will affect the OSD pods in the <code>rook-ceph</code> namespace.</li>
</ul>
<p>You can do it either via e.g. <code>kubectl -n &lt;namespace&gt; edit serviceaccount default</code> or by modifying the <a href="https://github.com/rook/rook/blob/master/cluster/examples/kubernetes/ceph/operator.yaml" target="_blank" rel="noopener noreffer"><code>operator.yaml</code></a>
and <a href="https://github.com/rook/rook/blob/master/cluster/examples/kubernetes/ceph/cluster.yaml" target="_blank" rel="noopener noreffer"><code>cluster.yaml</code></a> before deploying them.</p>
<p>Since it&rsquo;s the same procedure for all service accounts, here is just one example:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">kubectl -n rook-ceph edit serviceaccount default
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="k">apiVersion</span><span class="p">:</span><span class="w"> </span>v1<span class="w">
</span><span class="w"></span><span class="k">kind</span><span class="p">:</span><span class="w"> </span>ServiceAccount<span class="w">
</span><span class="w"></span><span class="k">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="k">name</span><span class="p">:</span><span class="w"> </span>default<span class="w">
</span><span class="w">  </span><span class="k">namespace</span><span class="p">:</span><span class="w"> </span>rook-ceph<span class="w">
</span><span class="w"></span><span class="k">secrets</span><span class="p">:</span><span class="w">
</span><span class="w"></span>- <span class="k">name</span><span class="p">:</span><span class="w"> </span>default-token<span class="m">-12345</span><span class="w">
</span><span class="w"></span><span class="k">imagePullSecrets</span><span class="p">:</span><span class="w">                </span><span class="c"># here are the new</span><span class="w">
</span><span class="w"></span>- <span class="k">name</span><span class="p">:</span><span class="w"> </span>my-registry-secret<span class="w">       </span><span class="c"># parts</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>After doing this for all service accounts all pods should be able to pull the image from your registry.</p>
<h4 id="tldr">TL;DR</h4>
<p>If you’re feeling lucky, a simple Rook cluster can be created with the following kubectl commands and <a href="https://github.com/rook/rook/blob/release-1.1/cluster/examples/kubernetes/ceph" target="_blank" rel="noopener noreffer">example yaml files</a>. For the more detailed install, skip to the next section to <a href="https://rook.io/docs/rook/v1.1/ceph-quickstart.html#deploy-the-rook-operator" target="_blank" rel="noopener noreffer">deploy the Rook operator</a>.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="nb">cd</span> cluster/examples/kubernetes/ceph
kubectl create -f common.yaml
kubectl create -f operator.yaml
kubectl create -f cluster-test.yaml
</code></pre></td></tr></table>
</div>
</div><p>After the cluster is running, you can create <a href="https://rook.io/docs/rook/v1.1/ceph-quickstart.html#storage" target="_blank" rel="noopener noreffer">block, object, or file storage</a> to be consumed by other applications in your cluster.</p>
<p><em><strong>Production Environments</strong></em><br>
For production environments it is required to have <strong>local storage devices</strong> attached to your nodes. In this walkthrough, the requirement of local storage devices is relaxed so you can get a cluster up and running as a “test” environment to experiment with Rook. A Ceph filestore OSD will be created in a <code>directory</code> instead of requiring a device. For production environments, you will want to follow the example in <code>cluster.yaml</code> instead of <code>cluster-test.yaml</code> in order to configure the devices instead of test directories. See the <a href="https://rook.io/docs/rook/v1.1/ceph-examples.html" target="_blank" rel="noopener noreffer">Ceph examples</a> for more details.<br>
生产环境需要使用本地存储设备连接到 k8s 的节点上，在本流程中，本地存储设备的要求被放松了，因此你可以以测试环境启动的方式启动和运行 rook-ceph 集群体验下 rook。 ceph filestore osd 将会在文件夹中创建，而不是需要一个存储设备。 对于生产环境，你需要按照 example 中的 cluster.yaml 而不是 cluster-test.yaml 配置存储设备而不是配置文件夹。</p>
<h4 id="deploy-the-rook-operator">Deploy the Rook Operator</h4>
<p>The first step is to deploy the Rook operator. Check that you are using the <a href="https://github.com/rook/rook/blob/release-1.1/cluster/examples/kubernetes/ceph" target="_blank" rel="noopener noreffer">example yaml files</a> that correspond to your release of Rook. For more options, see the <a href="https://rook.io/docs/rook/v1.1/ceph-examples.html" target="_blank" rel="noopener noreffer">examples documentation</a>.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="nb">cd</span> cluster/examples/kubernetes/ceph
kubectl create -f common.yaml
kubectl create -f operator.yaml

<span class="c1"># verify the rook-ceph-operator is in the `Running` state before proceeding</span>
kubectl -n rook-ceph get pod
</code></pre></td></tr></table>
</div>
</div><p>You can also deploy the operator with the <a href="https://rook.io/docs/rook/v1.1/helm-operator.html" target="_blank" rel="noopener noreffer">Rook Helm Chart</a>.</p>
<h5 id="ceph-examples">Ceph Examples</h5>
<p>Configuration for Rook and Ceph can be configured in multiple ways to provide block devices, shared file system volumes or object storage in a kubernetes namespace. We have provided several examples to simplify storage setup, but remember there are many tunables and you will need to decide what settings work for your use case and environment.</p>
<p>See the <strong><a href="https://github.com/rook/rook/blob/release-1.1/cluster/examples/kubernetes/ceph" target="_blank" rel="noopener noreffer">example yaml files</a></strong> folder for all the rook/ceph setup example spec files.</p>
<h6 id="common-resources">Common Resources</h6>
<p>The first step to deploy Rook is to create the common resources. The configuration for these resources will be the same for most deployments. The <a href="https://github.com/rook/rook/blob/release-1.1/cluster/examples/kubernetes/ceph/common.yaml" target="_blank" rel="noopener noreffer">common.yaml</a> sets these resources up.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">kubectl create -f common.yaml
</code></pre></td></tr></table>
</div>
</div><p>The examples all assume the operator and all Ceph daemons will be started in the same namespace. If you want to deploy the operator in a separate namespace, see the comments throughout <code>common.yaml</code>.</p>
<h6 id="operator">Operator</h6>
<p>After the common resources are created, the next step is to create the Operator deployment. Several spec file examples are provided in <a href="https://github.com/rook/rook/blob/release-1.1/cluster/examples/kubernetes/ceph/" target="_blank" rel="noopener noreffer">this directory</a>:</p>
<ul>
<li><code>operator.yaml</code>: The most common settings for production deployments
<ul>
<li><code>kubectl create -f operator.yaml</code></li>
</ul>
</li>
<li><code>operator-openshift.yaml</code>: Includes all of the operator settings for running a basic Rook cluster in an OpenShift environment. You will also want to review the <a href="https://rook.io/docs/rook/v1.1/openshift.html" target="_blank" rel="noopener noreffer">OpenShift Prerequisites</a> to confirm the settings.
<ul>
<li><code>oc create -f operator-openshift.yaml</code></li>
</ul>
</li>
</ul>
<p>Settings for the operator are configured through environment variables on the operator deployment. The individual settings are documented in <a href="https://github.com/rook/rook/blob/release-1.1/cluster/examples/kubernetes/ceph/operator.yaml" target="_blank" rel="noopener noreffer">operator.yaml</a>.</p>
<h6 id="cluster-crd">Cluster CRD</h6>
<p>Now that your operator is running, let&rsquo;s create your Ceph storage cluster:</p>
<ul>
<li><code>cluster.yaml</code>: This file contains common settings for a production storage cluster. Requires at least three nodes.</li>
<li><code>cluster-test.yaml</code>: Settings for a test cluster where redundancy is not configured. Requires only a single node.</li>
<li><code>cluster-minimal.yaml</code>: Brings up a cluster with only one <a href="http://docs.ceph.com/docs/nautilus/man/8/ceph-mon/" target="_blank" rel="noopener noreffer">ceph-mon</a> and a <a href="http://docs.ceph.com/docs/nautilus/mgr/" target="_blank" rel="noopener noreffer">ceph-mgr</a> so the Ceph dashboard can be used for the remaining cluster configuration.</li>
</ul>
<p>See the <a href="https://rook.io/docs/rook/v1.1/ceph-cluster-crd.html" target="_blank" rel="noopener noreffer">Cluster CRD</a> topic for more details on the settings.</p>
<p>Monitors may be configured to run on PVC storage. Details on <a href="https://rook.io/docs/rook/v1.1/ceph-cluster-crd.md#mon-settings" target="_blank" rel="noopener noreffer">how to set this up
and some minor restrctions are described here</a>.</p>
<h6 id="setting-up-consumable-storage">Setting up consumable storage</h6>
<p>Now we are ready to setup <a href="https://ceph.com/ceph-storage/block-storage/" target="_blank" rel="noopener noreffer">block</a>, <a href="https://ceph.com/ceph-storage/file-system/" target="_blank" rel="noopener noreffer">shared filesystem</a> or <a href="https://ceph.com/ceph-storage/object-storage/" target="_blank" rel="noopener noreffer">object storage</a> in the Rook Ceph cluster. These kinds of storage are respectively referred to as CephBlockPool, CephFilesystem and CephObjectStore in the spec files.</p>
<p><em><strong>Block Devices</strong></em></p>
<p>Ceph can provide raw block device volumes to pods. Each example below sets up a storage class which can then be used to provision a block device in kubernetes pods. The storage class is defined with <a href="http://docs.ceph.com/docs/master/rados/operations/pools/" target="_blank" rel="noopener noreffer">a pool</a> which defines the level of data redundancy in Ceph:</p>
<ul>
<li><code>storageclass.yaml</code>: This example illustrates replication of 3 for production scenarios and requires at least three nodes. Your data is replicated on three different kubernetes worker nodes and intermittent or long-lasting single node failures will not result in data unavailability or loss.</li>
<li><code>storageclass-ec.yaml</code>: Configures erasure coding for data durability rather than replication. <a href="http://docs.ceph.com/docs/master/rados/operations/erasure-code/" target="_blank" rel="noopener noreffer">Ceph&rsquo;s erasure coding</a> is more efficient than replication so you can get high reliability without the 3x replication cost of the preceding example (but at the cost of higher computational encoding and decoding costs on the worker nodes). Erasure coding requires at least three nodes. See the <a href="https://rook.io/docs/rook/v1.1/ceph-pool-crd.html#erasure-coded" target="_blank" rel="noopener noreffer">Erasure coding</a> documentation for more details. <strong>Note: Erasure coding is only available with the flex driver. Support from the CSI driver is coming soon.</strong></li>
<li><code>storageclass-test.yaml</code>: Replication of 1 for test scenarios and it requires only a single node. Do not use this for applications that store valuable data or have high-availability storage requirements, since a single node failure can result in data loss.</li>
</ul>
<p>The storage classes are found in different sub-directories depending on the driver:</p>
<ul>
<li><code>csi/rbd</code>: The CSI driver for block devices. This is the preferred driver going forward.</li>
<li><code>flex</code>: The flex driver will be deprecated in a future release to be determined.</li>
</ul>
<p>See the <a href="https://rook.io/docs/rook/v1.1/ceph-pool-crd.html" target="_blank" rel="noopener noreffer">Ceph Pool CRD</a> topic for more details on the settings.</p>
<p><em><strong>Shared File System</strong></em></p>
<p>Ceph file system (CephFS) allows the user to &lsquo;mount&rsquo; a shared posix-compliant folder into one or more hosts (pods in the container world). This storage is similar to NFS shared storage or CIFS shared folders, as explained <a href="https://ceph.com/ceph-storage/file-system/" target="_blank" rel="noopener noreffer">here</a>.</p>
<p>File storage contains multiple pools that can be configured for different scenarios:</p>
<ul>
<li><code>filesystem.yaml</code>: Replication of 3 for production scenarios. Requires at least three nodes.</li>
<li><code>filesystem-ec.yaml</code>: Erasure coding for production scenarios. Requires at least three nodes.</li>
<li><code>filesystem-test.yaml</code>: Replication of 1 for test scenarios. Requires only a single node.</li>
</ul>
<p>Dynamic provisioning is possible with the CSI driver. The storage class for shared file systems is found in the <code>csi/cephfs</code> directory.</p>
<p>See the <a href="https://rook.io/docs/rook/v1.1/ceph-filesystem-crd.html" target="_blank" rel="noopener noreffer">Shared File System CRD</a> topic for more details on the settings.</p>
<p><em><strong>Object Storage</strong></em></p>
<p>Ceph supports storing blobs of data called objects that support HTTP(s)-type get/put/post and delete semantics. This storage is similar to AWS S3 storage, for example.</p>
<p>Object storage contains multiple pools that can be configured for different scenarios:</p>
<ul>
<li><code>object.yaml</code>: Replication of 3 for production scenarios.  Requires at least three nodes.</li>
<li><code>object-openshift.yaml</code>: Replication of 3 with rgw in a port range valid for OpenShift.  Requires at least three nodes.</li>
<li><code>object-ec.yaml</code>: Erasure coding rather than replication for production scenarios.  Requires at least three nodes.</li>
<li><code>object-test.yaml</code>: Replication of 1 for test scenarios. Requires only a single node.</li>
</ul>
<p>See the <a href="https://rook.io/docs/rook/v1.1/ceph-object-store-crd.html" target="_blank" rel="noopener noreffer">Object Store CRD</a> topic for more details on the settings.</p>
<p><em><strong>Object Storage User</strong></em></p>
<ul>
<li><code>object-user.yaml</code>: Creates a simple object storage user and generates credentials for the S3 API</li>
</ul>
<p><em><strong>Object Storage Buckets</strong></em></p>
<p>The Ceph operator also runs an object store bucket provisioner which can grant access to existing buckets or dynamically provision new buckets.</p>
<ul>
<li><a href="https://github.com/rook/rook/blob/release-1.1/cluster/examples/kubernetes/ceph/object-bucket-claim-retain.yaml" target="_blank" rel="noopener noreffer">object-bucket-claim-retain.yaml</a> Creates a request for a new bucket by referencing a StorageClass which saves the bucket when the initiating OBC is deleted.</li>
<li><a href="https://github.com/rook/rook/blob/release-1.1/cluster/examples/kubernetes/ceph/object-bucket-claim-delete.yaml" target="_blank" rel="noopener noreffer">object-bucket-claim-delete.yaml</a> Creates a request for a new bucket by referencing a StorageClass which deletes the bucket when the initiating OBC is deleted.</li>
<li><a href="https://github.com/rook/rook/blob/release-1.1/cluster/examples/kubernetes/ceph/storageclass-bucket-retain.yaml" target="_blank" rel="noopener noreffer">storageclass-bucket-retain.yaml</a> Creates a new StorageClass which defines the Ceph Object Store, a region, and retains the bucket after the initiating OBC is deleted.</li>
<li><a href="https://github.com/rook/rook/blob/release-1.1/cluster/examples/kubernetes/ceph/storageclass-bucket-delete.yaml" target="_blank" rel="noopener noreffer">storageclass-bucket-delete.yaml</a> Creates a new StorageClass which defines the Ceph Object Store, a region, and deletes the bucket after the initiating OBC is deleted.</li>
</ul>
<h5 id="ceph-operator-helm-chart">Ceph Operator Helm Chart</h5>
<p>Installs <a href="https://github.com/rook/rook" target="_blank" rel="noopener noreffer">rook</a> to create, configure, and manage Ceph clusters on Kubernetes.</p>
<h6 id="introduction">Introduction</h6>
<p>This chart bootstraps a <a href="https://github.com/rook/rook" target="_blank" rel="noopener noreffer">rook-ceph-operator</a> deployment on a <a href="http://kubernetes.io" target="_blank" rel="noopener noreffer">Kubernetes</a> cluster using the <a href="https://helm.sh" target="_blank" rel="noopener noreffer">Helm</a> package manager.</p>
<h6 id="prerequisites-1">Prerequisites</h6>
<ul>
<li>Kubernetes 1.10+</li>
</ul>
<p><em><strong>RBAC</strong></em><br>
If role-based access control (RBAC) is enabled in your cluster, you may need to give Tiller (the server-side component of Helm) additional permissions. <strong>If RBAC is not enabled, be sure to set <code>rbacEnable</code> to <code>false</code> when installing the chart.</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback"># Create a ServiceAccount for Tiller in the `kube-system` namespace
kubectl --namespace kube-system create sa tiller

# Create a ClusterRoleBinding for Tiller
kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller

# Patch Tiller&#39;s Deployment to use the new ServiceAccount
kubectl --namespace kube-system patch deploy/tiller-deploy -p &#39;{&#34;spec&#34;: {&#34;template&#34;: {&#34;spec&#34;: {&#34;serviceAccountName&#34;: &#34;tiller&#34;}}}}&#39;
</code></pre></td></tr></table>
</div>
</div><h6 id="installing">Installing</h6>
<p>The Ceph Operator helm chart will install the basic components necessary to create a storage platform for your Kubernetes cluster. After the helm chart is installed, you will need to <a href="https://rook.io/docs/rook/v1.1/ceph-quickstart.html#create-a-rook-cluster" target="_blank" rel="noopener noreffer">create a Rook cluster</a>.</p>
<p>The <code>helm install</code> command deploys rook on the Kubernetes cluster in the default configuration. The <a href="#configuration" rel="">configuration</a> section lists the parameters that can be configured during installation. It is recommended that the rook operator be installed into the <code>rook-ceph</code> namespace (you will install your clusters into separate namespaces).</p>
<p>Rook currently publishes builds of the Ceph operator to the <code>release</code> and <code>master</code> channels.</p>
<p><em><strong>Release</strong></em><br>
The release channel is the most recent release of Rook that is considered stable for the community.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">helm repo add rook-release https://charts.rook.io/release
helm install --namespace rook-ceph rook-release/rook-ceph
</code></pre></td></tr></table>
</div>
</div><p><em><strong>Master</strong></em><br>
The master channel includes the latest commits, with all automated tests green. Historically it has been very stable, though it is only recommended for testing.</p>
<p>The critical point to consider is that upgrades are not supported to or from master builds.</p>
<p>To install the helm chart from master, you will need to pass the specific version returned by the <code>search</code> command.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">helm repo add rook-master https://charts.rook.io/master
helm search rook-ceph
helm install --namespace rook-ceph rook-master/rook-ceph --version &lt;version&gt;
</code></pre></td></tr></table>
</div>
</div><p>For example:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">helm install --namespace rook-ceph rook-master/rook-ceph --version v0.7.0-278.gcbd9726
</code></pre></td></tr></table>
</div>
</div><p><strong><strong>Development Build</strong></strong><br>
To deploy from a local build from your development environment:</p>
<ol>
<li>
<p>Build the Rook docker image: <code>make</code></p>
</li>
<li>
<p>Copy the image to your K8s cluster, such as with the <code>docker save</code> then the <code>docker load</code> commands</p>
</li>
<li>
<p>Install the helm chart</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">cd cluster/charts/rook-ceph
helm install --namespace rook-ceph --name rook-ceph .
</code></pre></td></tr></table>
</div>
</div></li>
</ol>
<h6 id="uninstalling-the-chart">Uninstalling the Chart</h6>
<p>To uninstall/delete the <code>rook-ceph</code> deployment:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">$ helm delete --purge rook-ceph
</code></pre></td></tr></table>
</div>
</div><p>The command removes all the Kubernetes components associated with the chart and deletes the release.</p>
<h6 id="configuration">Configuration</h6>
<p>The following tables lists the configurable parameters of the rook-operator chart and their default values.</p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>image.repository</code></td>
<td>Image</td>
<td><code>rook/ceph</code></td>
</tr>
<tr>
<td><code>image.tag</code></td>
<td>Image tag</td>
<td><code>master</code></td>
</tr>
<tr>
<td><code>image.pullPolicy</code></td>
<td>Image pull policy</td>
<td><code>IfNotPresent</code></td>
</tr>
<tr>
<td><code>rbacEnable</code></td>
<td>If true, create &amp; use RBAC resources</td>
<td><code>true</code></td>
</tr>
<tr>
<td><code>pspEnable</code></td>
<td>If true, create &amp; use PSP resources</td>
<td><code>true</code></td>
</tr>
<tr>
<td><code>resources</code></td>
<td>Pod resource requests &amp; limits</td>
<td><code>{}</code></td>
</tr>
<tr>
<td><code>annotations</code></td>
<td>Pod annotations</td>
<td><code>{}</code></td>
</tr>
<tr>
<td><code>logLevel</code></td>
<td>Global log level</td>
<td><code>INFO</code></td>
</tr>
<tr>
<td><code>nodeSelector</code></td>
<td>Kubernetes <code>nodeSelector</code> to add to the Deployment.</td>
<td><none></td>
</tr>
<tr>
<td><code>tolerations</code></td>
<td>List of Kubernetes <code>tolerations</code> to add to the Deployment.</td>
<td><code>[]</code></td>
</tr>
<tr>
<td><code>currentNamespaceOnly</code></td>
<td>Whether the operator should watch cluster CRD in its own namespace or not</td>
<td><code>false</code></td>
</tr>
<tr>
<td><code>hostpathRequiresPrivileged</code></td>
<td>Runs Ceph Pods as privileged to be able to write to <code>hostPath</code>s in OpenShift with SELinux restrictions.</td>
<td><code>false</code></td>
</tr>
<tr>
<td><code>agent.flexVolumeDirPath</code></td>
<td>Path where the Rook agent discovers the flex volume plugins (*)</td>
<td><code>/usr/libexec/kubernetes/kubelet-plugins/volume/exec/</code></td>
</tr>
<tr>
<td><code>agent.libModulesDirPath</code></td>
<td>Path where the Rook agent should look for kernel modules (*)</td>
<td><code>/lib/modules</code></td>
</tr>
<tr>
<td><code>agent.mounts</code></td>
<td>Additional paths to be mounted in the agent container (**)</td>
<td><none></td>
</tr>
<tr>
<td><code>agent.mountSecurityMode</code></td>
<td>Mount Security Mode for the agent.</td>
<td><code>Any</code></td>
</tr>
<tr>
<td><code>agent.toleration</code></td>
<td>Toleration for the agent pods</td>
<td><none></td>
</tr>
<tr>
<td><code>agent.tolerationKey</code></td>
<td>The specific key of the taint to tolerate</td>
<td><none></td>
</tr>
<tr>
<td><code>agent.tolerations</code></td>
<td>Array of tolerations in YAML format which will be added to agent deployment</td>
<td><none></td>
</tr>
<tr>
<td><code>agent.nodeAffinity</code></td>
<td>The node labels for affinity of <code>rook-agent</code> (***)</td>
<td><none></td>
</tr>
<tr>
<td><code>discover.toleration</code></td>
<td>Toleration for the discover pods</td>
<td><none></td>
</tr>
<tr>
<td><code>discover.tolerationKey</code></td>
<td>The specific key of the taint to tolerate</td>
<td><none></td>
</tr>
<tr>
<td><code>discover.tolerations</code></td>
<td>Array of tolerations in YAML format which will be added to discover deployment</td>
<td><none></td>
</tr>
<tr>
<td><code>discover.nodeAffinity</code></td>
<td>The node labels for affinity of <code>discover-agent</code> (***)</td>
<td><none></td>
</tr>
<tr>
<td><code>mon.healthCheckInterval</code></td>
<td>The frequency for the operator to check the mon health</td>
<td><code>45s</code></td>
</tr>
<tr>
<td><code>mon.monOutTimeout</code></td>
<td>The time to wait before failing over an unhealthy mon</td>
<td><code>600s</code></td>
</tr>
</tbody>
</table>
<p>For information on what to set <code>agent.flexVolumeDirPath</code> to, please refer to the <a href="https://rook.io/docs/rook/v1.1/flexvolume.html" target="_blank" rel="noopener noreffer">Rook flexvolume documentation</a></p>
<p><code>agent.mounts</code> should have this format <code>mountname1=/host/path:/container/path,mountname2=/host/path2:/container/path2</code></p>
<p><code>agent.nodeAffinity</code> and <code>discover.nodeAffinity</code> should have the format <code>&quot;role=storage,rook; storage=ceph&quot;</code> or <code>storage=;role=rook-example</code> or <code>storage=;</code> (<em>checks only for presence of key</em>)</p>
<p><em><strong>Command Line</strong></em><br>
You can pass the settings with helm command line parameters. Specify each parameter using the
<code>--set key=value[,key=value]</code> argument to <code>helm install</code>. For example, the following command will install rook where RBAC is not enabled.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">$ helm install --namespace rook-ceph --name rook-ceph rook-release/rook-ceph --set rbacEnable=false
</code></pre></td></tr></table>
</div>
</div><p><em><strong>Settings File</strong></em><br>
Alternatively, a yaml file that specifies the values for the above parameters (<code>values.yaml</code>) can be provided while installing the chart.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">$ helm install --namespace rook-ceph --name rook-ceph rook-release/rook-ceph -f values.yaml
</code></pre></td></tr></table>
</div>
</div><p>Here are the sample settings to get you started.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="k">image</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="k">prefix</span><span class="p">:</span><span class="w"> </span>rook<span class="w">
</span><span class="w">  </span><span class="k">repository</span><span class="p">:</span><span class="w"> </span>rook/ceph<span class="w">
</span><span class="w">  </span><span class="k">tag</span><span class="p">:</span><span class="w"> </span>master<span class="w">
</span><span class="w">  </span><span class="k">pullPolicy</span><span class="p">:</span><span class="w"> </span>IfNotPresent<span class="w">
</span><span class="w">
</span><span class="w"></span><span class="k">resources</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="k">limits</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="k">cpu</span><span class="p">:</span><span class="w"> </span>100m<span class="w">
</span><span class="w">    </span><span class="k">memory</span><span class="p">:</span><span class="w"> </span>128Mi<span class="w">
</span><span class="w">  </span><span class="k">requests</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="k">cpu</span><span class="p">:</span><span class="w"> </span>100m<span class="w">
</span><span class="w">    </span><span class="k">memory</span><span class="p">:</span><span class="w"> </span>128Mi<span class="w">
</span><span class="w">
</span><span class="w"></span><span class="k">rbacEnable</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w"></span><span class="k">pspEnable</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><h4 id="create-a-rook-ceph-cluster">Create a Rook Ceph Cluster</h4>
<p>Now that the Rook operator is running we can create the Ceph cluster. For the cluster to survive reboots, make sure you set the <code>dataDirHostPath</code> property that is valid for your hosts. For more settings, see the documentation on <a href="https://rook.io/docs/rook/v1.1/ceph-cluster-crd.html" target="_blank" rel="noopener noreffer">configuring the cluster</a>.</p>
<blockquote>
<p>CephCluster 这个 CRD 的介绍见 <a href="https://rook.io/docs/rook/v1.1/ceph-cluster-crd.html" target="_blank" rel="noopener noreffer">configuring the cluster</a></p>
</blockquote>
<p>Save the cluster spec as <code>cluster-test.yaml</code>:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="k">apiVersion</span><span class="p">:</span><span class="w"> </span>ceph.rook.io/v1<span class="w">
</span><span class="w"></span><span class="k">kind</span><span class="p">:</span><span class="w"> </span>CephCluster<span class="w">
</span><span class="w"></span><span class="k">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="k">name</span><span class="p">:</span><span class="w"> </span>rook-ceph<span class="w">
</span><span class="w">  </span><span class="k">namespace</span><span class="p">:</span><span class="w"> </span>rook-ceph<span class="w">
</span><span class="w"></span><span class="k">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="k">cephVersion</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="c"># For the latest ceph images, see https://hub.docker.com/r/ceph/ceph/tags</span><span class="w">
</span><span class="w">    </span><span class="k">image</span><span class="p">:</span><span class="w"> </span>ceph/ceph<span class="p">:</span>v14<span class="m">.2.4-20190917</span><span class="w">
</span><span class="w">  </span><span class="k">dataDirHostPath</span><span class="p">:</span><span class="w"> </span>/var/lib/rook<span class="w">
</span><span class="w">  </span><span class="k">mon</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="k">count</span><span class="p">:</span><span class="w"> </span><span class="m">3</span><span class="w">
</span><span class="w">  </span><span class="k">dashboard</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="k">enabled</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">  </span><span class="k">storage</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="k">useAllNodes</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">    </span><span class="k">useAllDevices</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="w">
</span><span class="w">    </span><span class="c"># Important: Directories should only be used in pre-production environments</span><span class="w">
</span><span class="w">    </span><span class="k">directories</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="k">path</span><span class="p">:</span><span class="w"> </span>/var/lib/rook<span class="w">
</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>Create the cluster:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">kubectl create -f cluster-test.yaml
</code></pre></td></tr></table>
</div>
</div><p>Use <code>kubectl</code> to list pods in the <code>rook-ceph</code> namespace. You should be able to see the following pods once they are all running.</p>
<p>The number of osd pods will depend on the number of nodes in the cluster and the number of devices and directories configured.</p>
<p>If you did not modify the <code>cluster-test.yaml</code> above, it is expected that one OSD will be created per node.</p>
<p>The <code>rook-ceph-agent</code> and <code>rook-discover</code> pods are also optional depending on your settings.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ kubectl -n rook-ceph get pod
NAME                                   READY   STATUS      RESTARTS   AGE
rook-ceph-agent-4zkg8                  1/1     Running     <span class="m">0</span>          140s
rook-ceph-mgr-a-d9dcf5748-5s9ft        1/1     Running     <span class="m">0</span>          77s
rook-ceph-mon-a-7d8f675889-nw5pl       1/1     Running     <span class="m">0</span>          105s
rook-ceph-mon-b-856fdd5cb9-5h2qk       1/1     Running     <span class="m">0</span>          94s
rook-ceph-mon-c-57545897fc-j576h       1/1     Running     <span class="m">0</span>          85s
rook-ceph-operator-6c49994c4f-9csfz    1/1     Running     <span class="m">0</span>          141s
rook-ceph-osd-0-7cbbbf749f-j8fsd       1/1     Running     <span class="m">0</span>          23s
rook-ceph-osd-1-7f67f9646d-44p7v       1/1     Running     <span class="m">0</span>          24s
rook-ceph-osd-2-6cd4b776ff-v4d68       1/1     Running     <span class="m">0</span>          25s
rook-ceph-osd-prepare-node1-vx2rz      0/2     Completed   <span class="m">0</span>          60s
rook-ceph-osd-prepare-node2-ab3fd      0/2     Completed   <span class="m">0</span>          60s
rook-ceph-osd-prepare-node3-w4xyz      0/2     Completed   <span class="m">0</span>          60s
rook-discover-dhkb8                    1/1     Running     <span class="m">0</span>          140s
</code></pre></td></tr></table>
</div>
</div><p>To verify that the cluster is in a healthy state, connect to the <a href="https://rook.io/docs/rook/v1.1/ceph-toolbox.html" target="_blank" rel="noopener noreffer">Rook toolbox</a> and run the <code>ceph status</code> command.</p>
<ul>
<li>All mons should be in quorum</li>
<li>A mgr should be active</li>
<li>At least one OSD should be active</li>
<li>If the health is not <code>HEALTH_OK</code>, the warnings or errors should be investigated</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">$ ceph status
  cluster:
    id:     a0452c76-30d9-4c1a-a948-5d8405f19a7c
    health: HEALTH_OK

  services:
    mon: 3 daemons, quorum a,b,c (age 3m)
    mgr: a(active, since 2m)
    osd: 3 osds: 3 up (since 1m), 3 in (since 1m)
...
</code></pre></td></tr></table>
</div>
</div><p>If the cluster is not healthy, please refer to the <a href="https://rook.io/docs/rook/v1.1/ceph-common-issues.html" target="_blank" rel="noopener noreffer">Ceph common issues</a> for more details and potential solutions.</p>
<h4 id="storage">Storage</h4>
<p>For a walkthrough of the three types of storage exposed by Rook, see the guides for:</p>
<ul>
<li><strong><a href="https://rook.io/docs/rook/v1.1/ceph-block.html" target="_blank" rel="noopener noreffer">Block</a></strong>: Create block storage to be consumed by a pod</li>
<li><strong><a href="https://rook.io/docs/rook/v1.1/ceph-object.html" target="_blank" rel="noopener noreffer">Object</a></strong>: Create an object store that is accessible inside or outside the Kubernetes cluster</li>
<li><strong><a href="https://rook.io/docs/rook/v1.1/ceph-filesystem.html" target="_blank" rel="noopener noreffer">Shared File System</a></strong>: Create a file system to be shared across multiple pods</li>
</ul>
<h5 id="block-storage">Block Storage</h5>
<p>Block storage allows a single pod to mount storage. This guide shows how to create a simple, multi-tier web application on Kubernetes using persistent volumes enabled by Rook.</p>
<h6 id="prerequisites-2">Prerequisites</h6>
<p>This guide assumes a Rook cluster as explained in the <a href="https://rook.io/docs/rook/v1.1/ceph-quickstart.html" target="_blank" rel="noopener noreffer">Quickstart</a>.</p>
<h6 id="provision-storage">Provision Storage</h6>
<p>Before Rook can provision storage, a <a href="https://kubernetes.io/docs/concepts/storage/storage-classes" target="_blank" rel="noopener noreffer"><code>StorageClass</code></a> and <a href="https://rook.io/docs/rook/v1.1/ceph-pool-crd.html" target="_blank" rel="noopener noreffer"><code>CephBlockPool</code></a> need to be created. This will allow Kubernetes to interoperate(互操作) with Rook when provisioning persistent volumes.</p>
<blockquote>
<p>storageclass.yaml 包含了 StorageClass 和 CephBlockPool，关于 CephBlockPool 这个 CRD （CustomResourceDefinition）的介绍见 <a href="https://rook.io/docs/rook/v1.1/ceph-pool-crd.html" target="_blank" rel="noopener noreffer"><code>CephBlockPool</code></a></p>
</blockquote>
<p><strong>NOTE:</strong> This sample requires <em>at least 1 OSD per node</em>, with each OSD located on <em>3 different nodes</em>.</p>
<p>Each OSD must be located on a different node, because the <a href="https://rook.io/docs/rook/v1.1/ceph-pool-crd.html#spec" target="_blank" rel="noopener noreffer"><code>failureDomain</code></a> is set to <code>host</code> and the <code>replicated.size</code> is set to <code>3</code>.</p>
<p><strong>NOTE</strong> This example uses the CSI driver, which is the preferred driver going forward for K8s 1.13 and newer. Examples are found in the <a href="https://github.com/rook/rook/tree/release-1.1/cluster/examples/kubernetes/ceph/csi/rbd" target="_blank" rel="noopener noreffer">CSI RBD</a> directory. For an example of a storage class using the flex driver (required for K8s 1.12 or earlier), see the <a href="#flex-driver" rel="">Flex Driver</a> section below, which has examples in the <a href="https://github.com/rook/rook/tree/release-1.1/cluster/examples/kubernetes/ceph/flex" target="_blank" rel="noopener noreffer">flex</a> directory.</p>
<p>Save this <code>StorageClass</code> definition as <code>storageclass.yaml</code>:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="k">apiVersion</span><span class="p">:</span><span class="w"> </span>ceph.rook.io/v1<span class="w">
</span><span class="w"></span><span class="k">kind</span><span class="p">:</span><span class="w"> </span>CephBlockPool<span class="w">
</span><span class="w"></span><span class="k">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="k">name</span><span class="p">:</span><span class="w"> </span>replicapool<span class="w">
</span><span class="w">  </span><span class="k">namespace</span><span class="p">:</span><span class="w"> </span>rook-ceph<span class="w">
</span><span class="w"></span><span class="k">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="k">failureDomain</span><span class="p">:</span><span class="w"> </span>host<span class="w">
</span><span class="w">  </span><span class="k">replicated</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="k">size</span><span class="p">:</span><span class="w"> </span><span class="m">3</span><span class="w">
</span><span class="w"></span>---<span class="w">
</span><span class="w"></span><span class="k">apiVersion</span><span class="p">:</span><span class="w"> </span>storage.k8s.io/v1<span class="w">
</span><span class="w"></span><span class="k">kind</span><span class="p">:</span><span class="w"> </span>StorageClass<span class="w">
</span><span class="w"></span><span class="k">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">   </span><span class="k">name</span><span class="p">:</span><span class="w"> </span>rook-ceph-block<span class="w">
</span><span class="w"></span><span class="c"># Change &#34;rook-ceph&#34; provisioner prefix to match the operator namespace if needed</span><span class="w">
</span><span class="w"></span><span class="k">provisioner</span><span class="p">:</span><span class="w"> </span>rook-ceph.rbd.csi.ceph.com<span class="w">
</span><span class="w"></span><span class="k">parameters</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="c"># clusterID is the namespace where the rook cluster is running</span><span class="w">
</span><span class="w">    </span><span class="k">clusterID</span><span class="p">:</span><span class="w"> </span>rook-ceph<span class="w">
</span><span class="w">    </span><span class="c"># Ceph pool into which the RBD image shall be created</span><span class="w">
</span><span class="w">    </span><span class="k">pool</span><span class="p">:</span><span class="w"> </span>replicapool<span class="w">
</span><span class="w">
</span><span class="w">    </span><span class="c"># RBD image format. Defaults to &#34;2&#34;.</span><span class="w">
</span><span class="w">    </span><span class="k">imageFormat</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;2&#34;</span><span class="w">
</span><span class="w">
</span><span class="w">    </span><span class="c"># RBD image features. Available for imageFormat: &#34;2&#34;. CSI RBD currently supports only `layering` feature.</span><span class="w">
</span><span class="w">    </span><span class="k">imageFeatures</span><span class="p">:</span><span class="w"> </span>layering<span class="w">
</span><span class="w">
</span><span class="w">    </span><span class="c"># The secrets contain Ceph admin credentials.</span><span class="w">
</span><span class="w">    </span><span class="k">csi.storage.k8s.io/provisioner-secret-name</span><span class="p">:</span><span class="w"> </span>rook-ceph-csi<span class="w">
</span><span class="w">    </span><span class="k">csi.storage.k8s.io/provisioner-secret-namespace</span><span class="p">:</span><span class="w"> </span>rook-ceph<span class="w">
</span><span class="w">    </span><span class="k">csi.storage.k8s.io/node-stage-secret-name</span><span class="p">:</span><span class="w"> </span>rook-ceph-csi<span class="w">
</span><span class="w">    </span><span class="k">csi.storage.k8s.io/node-stage-secret-namespace</span><span class="p">:</span><span class="w"> </span>rook-ceph<span class="w">
</span><span class="w">
</span><span class="w">    </span><span class="c"># Specify the filesystem type of the volume. If not specified, csi-provisioner</span><span class="w">
</span><span class="w">    </span><span class="c"># will set default as `ext4`.</span><span class="w">
</span><span class="w">    </span><span class="k">csi.storage.k8s.io/fstype</span><span class="p">:</span><span class="w"> </span>xfs<span class="w">
</span><span class="w">
</span><span class="w"></span><span class="c"># Delete the rbd volume when a PVC is deleted</span><span class="w">
</span><span class="w"></span><span class="k">reclaimPolicy</span><span class="p">:</span><span class="w"> </span>Delete<span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>If you&rsquo;ve deployed the Rook operator in a namespace other than &ldquo;rook-ceph&rdquo; as is common change the prefix in the provisioner to match the namespace you used. For example, if the Rook operator is running in &ldquo;rook-op&rdquo; the provisioner value should be &ldquo;rook-op.rbd.csi.ceph.com&rdquo;.</p>
<p>Create the storage class.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">kubectl create -f cluster/examples/kubernetes/ceph/csi/rbd/storageclass.yaml
</code></pre></td></tr></table>
</div>
</div><p><strong>NOTE</strong> As <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#retain" target="_blank" rel="noopener noreffer">specified by Kubernetes</a>, when using the <code>Retain</code> reclaim policy, any Ceph RBD image that is backed by a <code>PersistentVolume</code> will continue to exist even after the <code>PersistentVolume</code> has been deleted. These Ceph RBD images will need to be cleaned up manually using <code>rbd rm</code>.</p>
<h6 id="consume-the-storage-wordpress-sample">Consume the storage: Wordpress sample</h6>
<p>We create a sample app to consume the block storage provisioned by Rook with the classic wordpress and mysql apps. Both of these apps will make use of block volumes provisioned by Rook.</p>
<p>Start mysql and wordpress from the <code>cluster/examples/kubernetes</code> folder:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">kubectl create -f mysql.yaml
kubectl create -f wordpress.yaml
</code></pre></td></tr></table>
</div>
</div><p>Both of these apps create a block volume and mount it to their respective pod. You can see the Kubernetes volume claims by running the following:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ kubectl get pvc
NAME             STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE
mysql-pv-claim   Bound     pvc-95402dbc-efc0-11e6-bc9a-0cc47a3459ee   20Gi       RWO           1m
wp-pv-claim      Bound     pvc-39e43169-efc1-11e6-bc9a-0cc47a3459ee   20Gi       RWO           1m
</code></pre></td></tr></table>
</div>
</div><p>Once the wordpress and mysql pods are in the <code>Running</code> state, get the cluster IP of the wordpress app and enter it in your browser:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ kubectl get svc wordpress
NAME        CLUSTER-IP   EXTERNAL-IP   PORT<span class="o">(</span>S<span class="o">)</span>        AGE
wordpress   10.3.0.155   &lt;pending&gt;     80:30841/TCP   2m
</code></pre></td></tr></table>
</div>
</div><p>You should see the wordpress app running.</p>
<p>If you are using Minikube, the Wordpress URL can be retrieved with this one-line command:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">echo http://$(minikube ip):$(kubectl get service wordpress -o jsonpath=&#39;{.spec.ports[0].nodePort}&#39;)
</code></pre></td></tr></table>
</div>
</div><p><strong>NOTE:</strong> When running in a vagrant environment, there will be no external IP address to reach wordpress with.  You will only be able to reach wordpress via the <code>CLUSTER-IP</code> from inside the Kubernetes cluster.</p>
<h6 id="consume-the-storage-toolbox">Consume the storage: Toolbox</h6>
<p>With the pool that was created above, we can also create a block image and mount it directly in a pod. See the <a href="https://rook.io/docs/rook/v1.1/direct-tools.html#block-storage-tools" target="_blank" rel="noopener noreffer">Direct Block Tools</a> topic for more details.</p>
<h6 id="teardown">Teardown</h6>
<p>To clean up all the artifacts created by the block demo:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">kubectl delete -f wordpress.yaml
kubectl delete -f mysql.yaml
kubectl delete -n rook-ceph cephblockpools.ceph.rook.io replicapool
kubectl delete storageclass rook-ceph-block
</code></pre></td></tr></table>
</div>
</div><h6 id="flex-driver">Flex Driver</h6>
<p>To create a volume based on the flex driver instead of the CSI driver, see the following example of a storage class. Make sure the flex driver is enabled over Ceph CSI.</p>
<p>For this, you need to set <code>ROOK_ENABLE_FLEX_DRIVER</code> to <code>true</code> in your operator deployment in the <code>operator.yaml</code> file.</p>
<p>The pool definition is the same as for the CSI driver.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="k">apiVersion</span><span class="p">:</span><span class="w"> </span>ceph.rook.io/v1<span class="w">
</span><span class="w"></span><span class="k">kind</span><span class="p">:</span><span class="w"> </span>CephBlockPool<span class="w">
</span><span class="w"></span><span class="k">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="k">name</span><span class="p">:</span><span class="w"> </span>replicapool<span class="w">
</span><span class="w">  </span><span class="k">namespace</span><span class="p">:</span><span class="w"> </span>rook-ceph<span class="w">
</span><span class="w"></span><span class="k">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="k">failureDomain</span><span class="p">:</span><span class="w"> </span>host<span class="w">
</span><span class="w">  </span><span class="k">replicated</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="k">size</span><span class="p">:</span><span class="w"> </span><span class="m">3</span><span class="w">
</span><span class="w"></span>---<span class="w">
</span><span class="w"></span><span class="k">apiVersion</span><span class="p">:</span><span class="w"> </span>storage.k8s.io/v1<span class="w">
</span><span class="w"></span><span class="k">kind</span><span class="p">:</span><span class="w"> </span>StorageClass<span class="w">
</span><span class="w"></span><span class="k">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">   </span><span class="k">name</span><span class="p">:</span><span class="w"> </span>rook-ceph-block<span class="w">
</span><span class="w"></span><span class="k">provisioner</span><span class="p">:</span><span class="w"> </span>ceph.rook.io/block<span class="w">
</span><span class="w"></span><span class="k">parameters</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="k">blockPool</span><span class="p">:</span><span class="w"> </span>replicapool<span class="w">
</span><span class="w">  </span><span class="c"># The value of &#34;clusterNamespace&#34; MUST be the same as the one in which your rook cluster exist</span><span class="w">
</span><span class="w">  </span><span class="k">clusterNamespace</span><span class="p">:</span><span class="w"> </span>rook-ceph<span class="w">
</span><span class="w">  </span><span class="c"># Specify the filesystem type of the volume. If not specified, it will use `ext4`.</span><span class="w">
</span><span class="w">  </span><span class="k">fstype</span><span class="p">:</span><span class="w"> </span>xfs<span class="w">
</span><span class="w"></span><span class="c"># Optional, default reclaimPolicy is &#34;Delete&#34;. Other options are: &#34;Retain&#34;, &#34;Recycle&#34; as documented in https://kubernetes.io/docs/concepts/storage/storage-classes/</span><span class="w">
</span><span class="w"></span><span class="k">reclaimPolicy</span><span class="p">:</span><span class="w"> </span>Retain<span class="w">
</span><span class="w"></span><span class="c"># Optional, if you want to add dynamic resize for PVC. Works for Kubernetes 1.14+</span><span class="w">
</span><span class="w"></span><span class="c"># For now only ext3, ext4, xfs resize support provided, like in Kubernetes itself.</span><span class="w">
</span><span class="w"></span><span class="k">allowVolumeExpansion</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>Create the pool and storage class.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">kubectl create -f cluster/examples/kubernetes/ceph/flex/storageclass.yaml
</code></pre></td></tr></table>
</div>
</div><p>Continue with the example above for the <a href="#consume-the-storage-wordpress-sample" rel="">wordpress application</a>.</p>
<h6 id="advanced-example-erasure-coded-block-storage">Advanced Example: Erasure Coded Block Storage</h6>
<p><strong>IMPORTANT:</strong> This is only possible when using the Flex driver. Ceph CSI 1.2 (with Rook 1.1) does not support this type of configuration yet.</p>
<p>If you want to use erasure coded pool with RBD, your OSDs must use <code>bluestore</code> as their <code>storeType</code>.
Additionally the nodes that are going to mount the erasure coded RBD block storage must have Linux kernel &gt;= <code>4.11</code>.</p>
<p>To be able to use an erasure coded pool you need to create two pools (as seen below in the definitions): one erasure coded and one replicated. The replicated pool must be specified as the <code>blockPool</code> parameter. It is used for the metadata of the RBD images.</p>
<p>The erasure coded pool must be set as the <code>dataBlockPool</code> parameter below. It is used for the data of the RBD images.</p>
<p><strong>NOTE:</strong> This example requires <em>at least 3 bluestore OSDs</em>, with each OSD located on a <em>different node</em>.</p>
<p>The OSDs must be located on different nodes, because the <a href="https://rook.io/docs/rook/v1.1/ceph-pool-crd.html#spec" target="_blank" rel="noopener noreffer"><code>failureDomain</code></a> is set to <code>host</code> and the <code>erasureCoded</code> chunk settings require at least 3 different OSDs (2 <code>dataChunks</code> + 1 <code>codingChunks</code>).</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="k">apiVersion</span><span class="p">:</span><span class="w"> </span>ceph.rook.io/v1<span class="w">
</span><span class="w"></span><span class="k">kind</span><span class="p">:</span><span class="w"> </span>CephBlockPool<span class="w">
</span><span class="w"></span><span class="k">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="k">name</span><span class="p">:</span><span class="w"> </span>replicated-metadata-pool<span class="w">
</span><span class="w">  </span><span class="k">namespace</span><span class="p">:</span><span class="w"> </span>rook-ceph<span class="w">
</span><span class="w"></span><span class="k">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="k">failureDomain</span><span class="p">:</span><span class="w"> </span>host<span class="w">
</span><span class="w">  </span><span class="k">replicated</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="k">size</span><span class="p">:</span><span class="w"> </span><span class="m">3</span><span class="w">
</span><span class="w"></span>---<span class="w">
</span><span class="w"></span><span class="k">apiVersion</span><span class="p">:</span><span class="w"> </span>ceph.rook.io/v1<span class="w">
</span><span class="w"></span><span class="k">kind</span><span class="p">:</span><span class="w"> </span>CephBlockPool<span class="w">
</span><span class="w"></span><span class="k">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="k">name</span><span class="p">:</span><span class="w"> </span>ec-data-pool<span class="w">
</span><span class="w">  </span><span class="k">namespace</span><span class="p">:</span><span class="w"> </span>rook-ceph<span class="w">
</span><span class="w"></span><span class="k">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="k">failureDomain</span><span class="p">:</span><span class="w"> </span>host<span class="w">
</span><span class="w">  </span><span class="c"># Make sure you have enough nodes and OSDs running bluestore to support the replica size or erasure code chunks.</span><span class="w">
</span><span class="w">  </span><span class="c"># For the below settings, you need at least 3 OSDs on different nodes (because the `failureDomain` is `host` by default).</span><span class="w">
</span><span class="w">  </span><span class="k">erasureCoded</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="k">dataChunks</span><span class="p">:</span><span class="w"> </span><span class="m">2</span><span class="w">
</span><span class="w">    </span><span class="k">codingChunks</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w"></span>---<span class="w">
</span><span class="w"></span><span class="k">apiVersion</span><span class="p">:</span><span class="w"> </span>storage.k8s.io/v1<span class="w">
</span><span class="w"></span><span class="k">kind</span><span class="p">:</span><span class="w"> </span>StorageClass<span class="w">
</span><span class="w"></span><span class="k">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">   </span><span class="k">name</span><span class="p">:</span><span class="w"> </span>rook-ceph-block<span class="w">
</span><span class="w"></span><span class="k">provisioner</span><span class="p">:</span><span class="w"> </span>ceph.rook.io/block<span class="w">
</span><span class="w"></span><span class="k">parameters</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="k">blockPool</span><span class="p">:</span><span class="w"> </span>replicated-metadata-pool<span class="w">
</span><span class="w">  </span><span class="k">dataBlockPool</span><span class="p">:</span><span class="w"> </span>ec-data-pool<span class="w">
</span><span class="w">  </span><span class="c"># Specify the namespace of the rook cluster from which to create volumes.</span><span class="w">
</span><span class="w">  </span><span class="c"># If not specified, it will use `rook` as the default namespace of the cluster.</span><span class="w">
</span><span class="w">  </span><span class="c"># This is also the namespace where the cluster will be</span><span class="w">
</span><span class="w">  </span><span class="k">clusterNamespace</span><span class="p">:</span><span class="w"> </span>rook-ceph<span class="w">
</span><span class="w">  </span><span class="c"># Specify the filesystem type of the volume. If not specified, it will use `ext4`.</span><span class="w">
</span><span class="w">  </span><span class="k">fstype</span><span class="p">:</span><span class="w"> </span>xfs<span class="w">
</span><span class="w"></span><span class="c"># Works for Kubernetes 1.14+</span><span class="w">
</span><span class="w"></span><span class="k">allowVolumeExpansion</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>(These definitions can also be found in the <a href="https://github.com/rook/rook/blob/release-1.1/cluster/examples/kubernetes/ceph/flex/storage-class-ec.yaml" target="_blank" rel="noopener noreffer"><code>storageclass-ec.yaml</code></a> file)</p>
<h5 id="object-storage">Object Storage</h5>
<p>Object storage exposes an S3 API to the storage cluster for applications to put and get data.</p>
<h6 id="prerequisites-3">Prerequisites</h6>
<p>This guide assumes a Rook cluster as explained in the <a href="https://rook.io/docs/rook/v1.1/ceph-quickstart.html" target="_blank" rel="noopener noreffer">Quickstart</a>.</p>
<h6 id="create-an-object-store">Create an Object Store</h6>
<p>The below sample will create a <code>CephObjectStore</code> that starts the RGW service in the cluster with an S3 API.</p>
<p><strong>NOTE:</strong> This sample requires <em>at least 3 bluestore OSDs</em>, with each OSD located on a <em>different node</em>.</p>
<p>The OSDs must be located on different nodes, because the <a href="https://rook.io/docs/rook/v1.1/ceph-pool-crd.html#spec" target="_blank" rel="noopener noreffer"><code>failureDomain</code></a> is set to <code>host</code> and the <code>erasureCoded</code> chunk settings require at least 3 different OSDs (2 <code>dataChunks</code> + 1 <code>codingChunks</code>).</p>
<p>See the <a href="https://rook.io/docs/rook/v1.1/ceph-object-store-crd.html#object-store-settings" target="_blank" rel="noopener noreffer">Object Store CRD</a>, for more detail on the settings available for a <code>CephObjectStore</code>.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="k">apiVersion</span><span class="p">:</span><span class="w"> </span>ceph.rook.io/v1<span class="w">
</span><span class="w"></span><span class="k">kind</span><span class="p">:</span><span class="w"> </span>CephObjectStore<span class="w">
</span><span class="w"></span><span class="k">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="k">name</span><span class="p">:</span><span class="w"> </span>my-store<span class="w">
</span><span class="w">  </span><span class="k">namespace</span><span class="p">:</span><span class="w"> </span>rook-ceph<span class="w">
</span><span class="w"></span><span class="k">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="k">metadataPool</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="k">failureDomain</span><span class="p">:</span><span class="w"> </span>host<span class="w">
</span><span class="w">    </span><span class="k">replicated</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="k">size</span><span class="p">:</span><span class="w"> </span><span class="m">3</span><span class="w">
</span><span class="w">  </span><span class="k">dataPool</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="k">failureDomain</span><span class="p">:</span><span class="w"> </span>host<span class="w">
</span><span class="w">    </span><span class="k">erasureCoded</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="k">dataChunks</span><span class="p">:</span><span class="w"> </span><span class="m">2</span><span class="w">
</span><span class="w">      </span><span class="k">codingChunks</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">  </span><span class="k">preservePoolsOnDelete</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">  </span><span class="k">gateway</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="k">type</span><span class="p">:</span><span class="w"> </span>s3<span class="w">
</span><span class="w">    </span><span class="k">sslCertificateRef</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="k">port</span><span class="p">:</span><span class="w"> </span><span class="m">80</span><span class="w">
</span><span class="w">    </span><span class="k">securePort</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="k">instances</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>After the <code>CephObjectStore</code> is created, the Rook operator will then create all the pools and other resources necessary to start the service. This may take a minute to complete.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="c1"># Create the object store</span>
kubectl create -f object.yaml

<span class="c1"># To confirm the object store is configured, wait for the rgw pod to start</span>
kubectl -n rook-ceph get pod -l <span class="nv">app</span><span class="o">=</span>rook-ceph-rgw
</code></pre></td></tr></table>
</div>
</div><h6 id="create-a-user">Create a User</h6>
<p>Next, create a <code>CephObjectStoreUser</code>, which will be used to connect to the RGW service in the cluster using the S3 API.</p>
<p>See the <a href="https://rook.io/docs/rook/v1.1/ceph-object-store-user-crd.html" target="_blank" rel="noopener noreffer">Object Store User CRD</a> for more detail on the settings available for a <code>CephObjectStoreUser</code>.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="k">apiVersion</span><span class="p">:</span><span class="w"> </span>ceph.rook.io/v1<span class="w">
</span><span class="w"></span><span class="k">kind</span><span class="p">:</span><span class="w"> </span>CephObjectStoreUser<span class="w">
</span><span class="w"></span><span class="k">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="k">name</span><span class="p">:</span><span class="w"> </span>my-user<span class="w">
</span><span class="w">  </span><span class="k">namespace</span><span class="p">:</span><span class="w"> </span>rook-ceph<span class="w">
</span><span class="w"></span><span class="k">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="k">store</span><span class="p">:</span><span class="w"> </span>my-store<span class="w">
</span><span class="w">  </span><span class="k">displayName</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;my display name&#34;</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>When the <code>CephObjectStoreUser</code> is created, the Rook operator will then create the RGW user on the specified <code>CephObjectStore</code> and store the Access Key and Secret Key in a kubernetes secret in the same namespace as the <code>CephObjectStoreUser</code>.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="c1"># Create the object store user</span>
kubectl create -f object-user.yaml

<span class="c1"># To confirm the object store user is configured, describe the secret</span>
kubectl -n rook-ceph describe secret rook-ceph-object-user-my-store-my-user

Name:       rook-ceph-object-user-my-store-my-user
Namespace:  rook-ceph
Labels:       <span class="nv">app</span><span class="o">=</span>rook-ceph-rgw
                  <span class="nv">rook_cluster</span><span class="o">=</span>rook-ceph
                  <span class="nv">rook_object_store</span><span class="o">=</span>my-store
Annotations:    &lt;none&gt;

Type:   kubernetes.io/rook

<span class="nv">Data</span>
<span class="o">====</span>
AccessKey:  <span class="m">20</span> bytes
SecretKey:  <span class="m">40</span> bytes
</code></pre></td></tr></table>
</div>
</div><p>The AccessKey and SecretKey data fields can be mounted in a pod as an environment variable. More information on consuming kubernetes secrets can be found in the <a href="https://kubernetes.io/docs/concepts/configuration/secret/" target="_blank" rel="noopener noreffer">K8s secret documentation</a></p>
<p>To directly retrieve the secrets:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">kubectl -n rook-ceph get secret rook-ceph-object-user-my-store-my-user -o yaml <span class="p">|</span> grep AccessKey <span class="p">|</span> awk <span class="s1">&#39;{print $2}&#39;</span> <span class="p">|</span> base64 --decode
kubectl -n rook-ceph get secret rook-ceph-object-user-my-store-my-user -o yaml <span class="p">|</span> grep SecretKey <span class="p">|</span> awk <span class="s1">&#39;{print $2}&#39;</span> <span class="p">|</span> base64 --decode
</code></pre></td></tr></table>
</div>
</div><h6 id="consume-the-object-storage">Consume the Object Storage</h6>
<p>Use an S3 compatible client to create a bucket in the <code>CephObjectStore</code>.</p>
<p>This section will allow you to test connecting to the <code>CephObjectStore</code> and uploading and downloading from it. Run the following commands after you have connected to the <a href="https://rook.io/docs/rook/v1.1/ceph-toolbox.html" target="_blank" rel="noopener noreffer">Rook toolbox</a>.</p>
<p><em><strong>Install s3cmd</strong></em></p>
<p>To test the <code>CephObjectStore</code> we will install the <code>s3cmd</code> tool into the toobox pod.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">yum --assumeyes install s3cmd
</code></pre></td></tr></table>
</div>
</div><p><em><strong>Connection Environment Variables</strong></em></p>
<p>To simplify the s3 client commands, you will want to set the four environment variables for use by your client (ie. inside the toolbox):</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="nb">export</span> <span class="nv">AWS_HOST</span><span class="o">=</span>&lt;host&gt;
<span class="nb">export</span> <span class="nv">AWS_ENDPOINT</span><span class="o">=</span>&lt;endpoint&gt;
<span class="nb">export</span> <span class="nv">AWS_ACCESS_KEY_ID</span><span class="o">=</span>&lt;accessKey&gt;
<span class="nb">export</span> <span class="nv">AWS_SECRET_ACCESS_KEY</span><span class="o">=</span>&lt;secretKey&gt;
</code></pre></td></tr></table>
</div>
</div><ul>
<li><code>Host</code>: The DNS host name where the rgw service is found in the cluster. Assuming you are using the default <code>rook-ceph</code> cluster, it will be <code>rook-ceph-rgw-my-store.rook-ceph</code>.</li>
<li><code>Endpoint</code>: The endpoint where the rgw service is listening. Run <code>kubectl -n rook-ceph get svc rook-ceph-rgw-my-store</code>, then combine the clusterIP and the port.</li>
<li><code>Access key</code>: The user&rsquo;s <code>access_key</code> as printed above</li>
<li><code>Secret key</code>: The user&rsquo;s <code>secret_key</code> as printed above</li>
</ul>
<p>The variables for the user generated in this example would be:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="nb">export</span> <span class="nv">AWS_HOST</span><span class="o">=</span>rook-ceph-rgw-my-store.rook-ceph
<span class="nb">export</span> <span class="nv">AWS_ENDPOINT</span><span class="o">=</span>10.104.35.31:80
<span class="nb">export</span> <span class="nv">AWS_ACCESS_KEY_ID</span><span class="o">=</span>XEZDB3UJ6X7HVBE7X7MA
<span class="nb">export</span> <span class="nv">AWS_SECRET_ACCESS_KEY</span><span class="o">=</span>7yGIZON7EhFORz0I40BFniML36D2rl8CQQ5kXU6l
</code></pre></td></tr></table>
</div>
</div><p>The access key and secret key can be retrieved as described in the section above on <a href="#create-a-user" rel="">creating a user</a>.</p>
<p><em><strong>Create a bucket</strong></em></p>
<p>Now that the user connection variables were set above, we can proceed to perform operations such as creating buckets.</p>
<p>Create a bucket in the <code>CephObjectStore</code></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">s3cmd mb --no-ssl --host<span class="o">=</span><span class="si">${</span><span class="nv">AWS_HOST</span><span class="si">}</span> --region<span class="o">=</span><span class="s2">&#34;:default-placement&#34;</span> --host-bucket<span class="o">=</span><span class="s2">&#34;&#34;</span> s3://rookbucket
</code></pre></td></tr></table>
</div>
</div><p>List buckets in the <code>CephObjectStore</code></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">s3cmd ls --no-ssl --host<span class="o">=</span><span class="si">${</span><span class="nv">AWS_HOST</span><span class="si">}</span>
</code></pre></td></tr></table>
</div>
</div><p><em><strong>PUT or GET an object</strong></em></p>
<p>Upload a file to the newly created bucket</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="nb">echo</span> <span class="s2">&#34;Hello Rook&#34;</span> &gt; /tmp/rookObj
s3cmd put /tmp/rookObj --no-ssl --host<span class="o">=</span><span class="si">${</span><span class="nv">AWS_HOST</span><span class="si">}</span> --host-bucket<span class="o">=</span>  s3://rookbucket
</code></pre></td></tr></table>
</div>
</div><p>Download and verify the file from the bucket</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">s3cmd get s3://rookbucket/rookObj /tmp/rookObj-download --no-ssl --host<span class="o">=</span><span class="si">${</span><span class="nv">AWS_HOST</span><span class="si">}</span> --host-bucket<span class="o">=</span>
cat /tmp/rookObj-download
</code></pre></td></tr></table>
</div>
</div><h6 id="access-external-to-the-cluster">Access External to the Cluster</h6>
<p>Rook sets up the object storage so pods will have access internal to the cluster. If your applications are running outside the cluster, you will need to setup an external service through a <code>NodePort</code>.</p>
<p>First, note the service that exposes RGW internal to the cluster. We will leave this service intact and create a new service for external access.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ kubectl -n rook-ceph get service rook-ceph-rgw-my-store
NAME                     CLUSTER-IP   EXTERNAL-IP   PORT<span class="o">(</span>S<span class="o">)</span>     AGE
rook-ceph-rgw-my-store   10.3.0.177   &lt;none&gt;        80/TCP      2m
</code></pre></td></tr></table>
</div>
</div><p>Save the external service as <code>rgw-external.yaml</code>:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="k">apiVersion</span><span class="p">:</span><span class="w"> </span>v1<span class="w">
</span><span class="w"></span><span class="k">kind</span><span class="p">:</span><span class="w"> </span>Service<span class="w">
</span><span class="w"></span><span class="k">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="k">name</span><span class="p">:</span><span class="w"> </span>rook-ceph-rgw-my-store-external<span class="w">
</span><span class="w">  </span><span class="k">namespace</span><span class="p">:</span><span class="w"> </span>rook-ceph<span class="w">
</span><span class="w">  </span><span class="k">labels</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="k">app</span><span class="p">:</span><span class="w"> </span>rook-ceph-rgw<span class="w">
</span><span class="w">    </span><span class="k">rook_cluster</span><span class="p">:</span><span class="w"> </span>rook-ceph<span class="w">
</span><span class="w">    </span><span class="k">rook_object_store</span><span class="p">:</span><span class="w"> </span>my-store<span class="w">
</span><span class="w"></span><span class="k">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="k">ports</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="k">name</span><span class="p">:</span><span class="w"> </span>rgw<span class="w">
</span><span class="w">    </span><span class="k">port</span><span class="p">:</span><span class="w"> </span><span class="m">80</span><span class="w">
</span><span class="w">    </span><span class="k">protocol</span><span class="p">:</span><span class="w"> </span>TCP<span class="w">
</span><span class="w">    </span><span class="k">targetPort</span><span class="p">:</span><span class="w"> </span><span class="m">80</span><span class="w">
</span><span class="w">  </span><span class="k">selector</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="k">app</span><span class="p">:</span><span class="w"> </span>rook-ceph-rgw<span class="w">
</span><span class="w">    </span><span class="k">rook_cluster</span><span class="p">:</span><span class="w"> </span>rook-ceph<span class="w">
</span><span class="w">    </span><span class="k">rook_object_store</span><span class="p">:</span><span class="w"> </span>my-store<span class="w">
</span><span class="w">  </span><span class="k">sessionAffinity</span><span class="p">:</span><span class="w"> </span>None<span class="w">
</span><span class="w">  </span><span class="k">type</span><span class="p">:</span><span class="w"> </span>NodePort<span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>Now create the external service.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">kubectl create -f rgw-external.yaml
</code></pre></td></tr></table>
</div>
</div><p>See both rgw services running and notice what port the external service is running on:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ kubectl -n rook-ceph get service rook-ceph-rgw-my-store rook-ceph-rgw-my-store-external
NAME                              TYPE        CLUSTER-IP       EXTERNAL-IP   PORT<span class="o">(</span>S<span class="o">)</span>        AGE
rook-ceph-rgw-my-store            ClusterIP   10.104.82.228    &lt;none&gt;        80/TCP         4m
rook-ceph-rgw-my-store-external   NodePort    10.111.113.237   &lt;none&gt;        80:31536/TCP   39s
</code></pre></td></tr></table>
</div>
</div><p>Internally the rgw service is running on port <code>80</code>. The external port in this case is <code>31536</code>. Now you can access the <code>CephObjectStore</code> from anywhere! All you need is the hostname for any machine in the cluster, the external port, and the user credentials.</p>
<h5 id="shared-file-system">Shared File System</h5>
<p>A shared file system can be mounted with read/write permission from multiple pods. This may be useful for applications which can be clustered using a shared filesystem.</p>
<p>This example runs a shared file system for the <a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/registry" target="_blank" rel="noopener noreffer">kube-registry</a>.</p>
<h6 id="prerequisites-4">Prerequisites</h6>
<p>This guide assumes you have created a Rook cluster as explained in the main <a href="https://rook.io/docs/rook/v1.1/ceph-quickstart.html" target="_blank" rel="noopener noreffer">Kubernetes guide</a></p>
<h6 id="multiple-file-systems-not-supported">Multiple File Systems Not Supported</h6>
<p>By default only one shared file system can be created with Rook. Multiple file system support in Ceph is still considered experimental and can be enabled with the environment variable <code>ROOK_ALLOW_MULTIPLE_FILESYSTEMS</code> defined in <code>operator.yaml</code>.</p>
<p>Please refer to <a href="http://docs.ceph.com/docs/master/cephfs/experimental-features/#multiple-filesystems-within-a-ceph-cluster" target="_blank" rel="noopener noreffer">cephfs experimental features</a> page for more information.</p>
<h6 id="create-the-file-system">Create the File System</h6>
<p>Create the file system by specifying the desired settings for the metadata pool, data pools, and metadata server in the <code>CephFilesystem</code> CRD. In this example we create the metadata pool with replication of three and a single data pool with replication of three. For more options, see the documentation on <a href="https://rook.io/docs/rook/v1.1/ceph-filesystem-crd.html" target="_blank" rel="noopener noreffer">creating shared file systems</a>.</p>
<blockquote>
<p>CephFilesystem 这个 CRD 的介绍见：<a href="https://rook.io/docs/rook/v1.1/ceph-filesystem-crd.html">https://rook.io/docs/rook/v1.1/ceph-filesystem-crd.html</a></p>
</blockquote>
<p>Save this shared file system definition as <code>filesystem.yaml</code>:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="k">apiVersion</span><span class="p">:</span><span class="w"> </span>ceph.rook.io/v1<span class="w">
</span><span class="w"></span><span class="k">kind</span><span class="p">:</span><span class="w"> </span>CephFilesystem<span class="w">
</span><span class="w"></span><span class="k">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="k">name</span><span class="p">:</span><span class="w"> </span>myfs<span class="w">
</span><span class="w">  </span><span class="k">namespace</span><span class="p">:</span><span class="w"> </span>rook-ceph<span class="w">
</span><span class="w"></span><span class="k">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="k">metadataPool</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="k">replicated</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="k">size</span><span class="p">:</span><span class="w"> </span><span class="m">3</span><span class="w">
</span><span class="w">  </span><span class="k">dataPools</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="k">replicated</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="k">size</span><span class="p">:</span><span class="w"> </span><span class="m">3</span><span class="w">
</span><span class="w">  </span><span class="k">preservePoolsOnDelete</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">  </span><span class="k">metadataServer</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="k">activeCount</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">    </span><span class="k">activeStandby</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>The Rook operator will create all the pools and other resources necessary to start the service. This may take a minute to complete.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="c1"># Create the file system</span>
$ kubectl create -f filesystem.yaml

<span class="c1"># To confirm the file system is configured, wait for the mds pods to start</span>
$ kubectl -n rook-ceph get pod -l <span class="nv">app</span><span class="o">=</span>rook-ceph-mds
NAME                                      READY     STATUS    RESTARTS   AGE
rook-ceph-mds-myfs-7d59fdfcf4-h8kw9       1/1       Running   <span class="m">0</span>          12s
rook-ceph-mds-myfs-7d59fdfcf4-kgkjp       1/1       Running   <span class="m">0</span>          12s
</code></pre></td></tr></table>
</div>
</div><p>To see detailed status of the file system, start and connect to the <a href="https://rook.io/docs/rook/v1.1/ceph-toolbox.html" target="_blank" rel="noopener noreffer">Rook toolbox</a>. A new line will be shown with <code>ceph status</code> for the <code>mds</code> service. In this example, there is one active instance of MDS which is up, with one MDS instance in <code>standby-replay</code> mode in case of failover.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ ceph status
  ...
  services:
    mds: myfs-1/1/1 up <span class="o">{[</span>myfs:0<span class="o">]=</span><span class="nv">mzw58b</span><span class="o">=</span>up:active<span class="o">}</span>, <span class="m">1</span> up:standby-replay
</code></pre></td></tr></table>
</div>
</div><h6 id="provision-storage-1">Provision Storage</h6>
<p>Before Rook can start provisioning storage, a StorageClass needs to be created based on the filesystem. This is needed for Kubernetes to interoperate with the CSI driver to create persistent volumes.</p>
<p><strong>NOTE</strong> This example uses the CSI driver, which is the preferred driver going forward for K8s 1.13 and newer. Examples are found in the <a href="https://github.com/rook/rook/tree/release-1.1/cluster/examples/kubernetes/ceph/csi/cephfs" target="_blank" rel="noopener noreffer">CSI CephFS</a> directory. For an example of a volume using the flex driver (required for K8s 1.12 and earlier), see the <a href="#flex-driver" rel="">Flex Driver</a> section below.</p>
<p>Save this storage class definition as <code>storageclass.yaml</code>:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="k">apiVersion</span><span class="p">:</span><span class="w"> </span>storage.k8s.io/v1<span class="w">
</span><span class="w"></span><span class="k">kind</span><span class="p">:</span><span class="w"> </span>StorageClass<span class="w">
</span><span class="w"></span><span class="k">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="k">name</span><span class="p">:</span><span class="w"> </span>rook-cephfs<span class="w">
</span><span class="w"></span><span class="c"># Change &#34;rook-ceph&#34; provisioner prefix to match the operator namespace if needed</span><span class="w">
</span><span class="w"></span><span class="k">provisioner</span><span class="p">:</span><span class="w"> </span>rook-ceph.cephfs.csi.ceph.com<span class="w">
</span><span class="w"></span><span class="k">parameters</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="c"># clusterID is the namespace where operator is deployed.</span><span class="w">
</span><span class="w">  </span><span class="k">clusterID</span><span class="p">:</span><span class="w"> </span>rook-ceph<span class="w">
</span><span class="w">
</span><span class="w">  </span><span class="c"># CephFS filesystem name into which the volume shall be created</span><span class="w">
</span><span class="w">  </span><span class="k">fsName</span><span class="p">:</span><span class="w"> </span>myfs<span class="w">
</span><span class="w">
</span><span class="w">  </span><span class="c"># Ceph pool into which the volume shall be created</span><span class="w">
</span><span class="w">  </span><span class="c"># Required for provisionVolume: &#34;true&#34;</span><span class="w">
</span><span class="w">  </span><span class="k">pool</span><span class="p">:</span><span class="w"> </span>myfs-data0<span class="w">
</span><span class="w">
</span><span class="w">  </span><span class="c"># Root path of an existing CephFS volume</span><span class="w">
</span><span class="w">  </span><span class="c"># Required for provisionVolume: &#34;false&#34;</span><span class="w">
</span><span class="w">  </span><span class="c"># rootPath: /absolute/path</span><span class="w">
</span><span class="w">
</span><span class="w">  </span><span class="c"># The secrets contain Ceph admin credentials. These are generated automatically by the operator</span><span class="w">
</span><span class="w">  </span><span class="c"># in the same namespace as the cluster.</span><span class="w">
</span><span class="w">  </span><span class="k">csi.storage.k8s.io/provisioner-secret-name</span><span class="p">:</span><span class="w"> </span>rook-ceph-csi<span class="w">
</span><span class="w">  </span><span class="k">csi.storage.k8s.io/provisioner-secret-namespace</span><span class="p">:</span><span class="w"> </span>rook-ceph<span class="w">
</span><span class="w">  </span><span class="k">csi.storage.k8s.io/node-stage-secret-name</span><span class="p">:</span><span class="w"> </span>rook-ceph-csi<span class="w">
</span><span class="w">  </span><span class="k">csi.storage.k8s.io/node-stage-secret-namespace</span><span class="p">:</span><span class="w"> </span>rook-ceph<span class="w">
</span><span class="w">
</span><span class="w"></span><span class="k">reclaimPolicy</span><span class="p">:</span><span class="w"> </span>Delete<span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>If you&rsquo;ve deployed the Rook operator in a namespace other than &ldquo;rook-ceph&rdquo; as is common change the prefix in the provisioner to match the namespace you used. For example, if the Rook operator is running in &ldquo;rook-op&rdquo; the provisioner value should be &ldquo;rook-op.rbd.csi.ceph.com&rdquo;.</p>
<p>Create the storage class.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">kubectl create -f cluster/examples/kubernetes/ceph/csi/cephfs/storageclass.yaml
</code></pre></td></tr></table>
</div>
</div><h6 id="consume-the-shared-file-system-k8s-registry-sample">Consume the Shared File System: K8s Registry Sample</h6>
<p>As an example, we will start the kube-registry pod with the shared file system as the backing store.
Save the following spec as <code>kube-registry.yaml</code>:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="k">apiVersion</span><span class="p">:</span><span class="w"> </span>v1<span class="w">
</span><span class="w"></span><span class="k">kind</span><span class="p">:</span><span class="w"> </span>PersistentVolumeClaim<span class="w">
</span><span class="w"></span><span class="k">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="k">name</span><span class="p">:</span><span class="w"> </span>cephfs-pvc<span class="w">
</span><span class="w"></span><span class="k">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="k">accessModes</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- ReadWriteMany<span class="w">
</span><span class="w">  </span><span class="k">resources</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="k">requests</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="k">storage</span><span class="p">:</span><span class="w"> </span>1Gi<span class="w">
</span><span class="w">  </span><span class="k">storageClassName</span><span class="p">:</span><span class="w"> </span>csi-cephfs<span class="w">
</span><span class="w"></span>---<span class="w">
</span><span class="w"></span><span class="k">apiVersion</span><span class="p">:</span><span class="w"> </span>apps/v1<span class="w">
</span><span class="w"></span><span class="k">kind</span><span class="p">:</span><span class="w"> </span>Deployment<span class="w">
</span><span class="w"></span><span class="k">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="k">name</span><span class="p">:</span><span class="w"> </span>kube-registry<span class="w">
</span><span class="w">  </span><span class="k">namespace</span><span class="p">:</span><span class="w"> </span>kube-system<span class="w">
</span><span class="w">  </span><span class="k">labels</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="k">k8s-app</span><span class="p">:</span><span class="w"> </span>kube-registry<span class="w">
</span><span class="w">    </span><span class="k">kubernetes.io/cluster-service</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;true&#34;</span><span class="w">
</span><span class="w"></span><span class="k">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="k">replicas</span><span class="p">:</span><span class="w"> </span><span class="m">3</span><span class="w">
</span><span class="w">  </span><span class="k">selector</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="k">matchLabels</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="k">k8s-app</span><span class="p">:</span><span class="w"> </span>kube-registry<span class="w">
</span><span class="w">  </span><span class="k">template</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="k">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="k">labels</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="k">k8s-app</span><span class="p">:</span><span class="w"> </span>kube-registry<span class="w">
</span><span class="w">        </span><span class="k">kubernetes.io/cluster-service</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;true&#34;</span><span class="w">
</span><span class="w">    </span><span class="k">spec</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="k">containers</span><span class="p">:</span><span class="w">
</span><span class="w">      </span>- <span class="k">name</span><span class="p">:</span><span class="w"> </span>registry<span class="w">
</span><span class="w">        </span><span class="k">image</span><span class="p">:</span><span class="w"> </span>registry<span class="p">:</span><span class="m">2</span><span class="w">
</span><span class="w">        </span><span class="k">imagePullPolicy</span><span class="p">:</span><span class="w"> </span>Always<span class="w">
</span><span class="w">        </span><span class="k">resources</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="k">limits</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="k">cpu</span><span class="p">:</span><span class="w"> </span>100m<span class="w">
</span><span class="w">            </span><span class="k">memory</span><span class="p">:</span><span class="w"> </span>100Mi<span class="w">
</span><span class="w">        </span><span class="k">env</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="c"># Configuration reference: https://docs.docker.com/registry/configuration/</span><span class="w">
</span><span class="w">        </span>- <span class="k">name</span><span class="p">:</span><span class="w"> </span>REGISTRY_HTTP_ADDR<span class="w">
</span><span class="w">          </span><span class="k">value</span><span class="p">:</span><span class="w"> </span><span class="p">:</span><span class="m">5000</span><span class="w">
</span><span class="w">        </span>- <span class="k">name</span><span class="p">:</span><span class="w"> </span>REGISTRY_HTTP_SECRET<span class="w">
</span><span class="w">          </span><span class="k">value</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;Ple4seCh4ngeThisN0tAVerySecretV4lue&#34;</span><span class="w">
</span><span class="w">        </span>- <span class="k">name</span><span class="p">:</span><span class="w"> </span>REGISTRY_STORAGE_FILESYSTEM_ROOTDIRECTORY<span class="w">
</span><span class="w">          </span><span class="k">value</span><span class="p">:</span><span class="w"> </span>/var/lib/registry<span class="w">
</span><span class="w">        </span><span class="k">volumeMounts</span><span class="p">:</span><span class="w">
</span><span class="w">        </span>- <span class="k">name</span><span class="p">:</span><span class="w"> </span>image-store<span class="w">
</span><span class="w">          </span><span class="k">mountPath</span><span class="p">:</span><span class="w"> </span>/var/lib/registry<span class="w">
</span><span class="w">        </span><span class="k">ports</span><span class="p">:</span><span class="w">
</span><span class="w">        </span>- <span class="k">containerPort</span><span class="p">:</span><span class="w"> </span><span class="m">5000</span><span class="w">
</span><span class="w">          </span><span class="k">name</span><span class="p">:</span><span class="w"> </span>registry<span class="w">
</span><span class="w">          </span><span class="k">protocol</span><span class="p">:</span><span class="w"> </span>TCP<span class="w">
</span><span class="w">        </span><span class="k">livenessProbe</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="k">httpGet</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="k">path</span><span class="p">:</span><span class="w"> </span>/<span class="w">
</span><span class="w">            </span><span class="k">port</span><span class="p">:</span><span class="w"> </span>registry<span class="w">
</span><span class="w">        </span><span class="k">readinessProbe</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="k">httpGet</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="k">path</span><span class="p">:</span><span class="w"> </span>/<span class="w">
</span><span class="w">            </span><span class="k">port</span><span class="p">:</span><span class="w"> </span>registry<span class="w">
</span><span class="w">      </span><span class="k">volumes</span><span class="p">:</span><span class="w">
</span><span class="w">      </span>- <span class="k">name</span><span class="p">:</span><span class="w"> </span>image-store<span class="w">
</span><span class="w">        </span><span class="k">persistentVolumeClaim</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="k">claimName</span><span class="p">:</span><span class="w"> </span>cephfs-pvc<span class="w">
</span><span class="w">          </span><span class="k">readOnly</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>Create the Kube registry deployment:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">kubectl create -f cluster/examples/kubernetes/ceph/csi/cephfs/kube-registry.yaml
</code></pre></td></tr></table>
</div>
</div><p>You now have a docker registry which is HA with persistent storage.</p>
<h6 id="kernel-version-requirement">Kernel Version Requirement</h6>
<p>If the Rook cluster has more than one filesystem and the application pod is scheduled to a node with kernel version older than 4.7, inconsistent results may arise since kernels older than 4.7 do not support specifying filesystem namespaces.</p>
<h6 id="consume-the-shared-file-system-toolbox">Consume the Shared File System: Toolbox</h6>
<p>Once you have pushed an image to the registry (see the <a href="https://github.com/kubernetes/kubernetes/tree/release-1.9/cluster/addons/registry" target="_blank" rel="noopener noreffer">instructions</a> to expose and use the kube-registry), verify that kube-registry is using the filesystem that was configured above by mounting the shared file system in the toolbox pod. See the <a href="https://rook.io/docs/rook/v1.1/direct-tools.html#shared-filesystem-tools" target="_blank" rel="noopener noreffer">Direct Filesystem</a> topic for more details.</p>
<h6 id="teardown-1">Teardown</h6>
<p>To clean up all the artifacts created by the file system demo:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">kubectl delete -f kube-registry.yaml
</code></pre></td></tr></table>
</div>
</div><p>To delete the filesystem components and backing data, delete the Filesystem CRD. <strong>Warning: Data will be deleted if preservePoolsOnDelete=false</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">kubectl -n rook-ceph delete cephfilesystem myfs
</code></pre></td></tr></table>
</div>
</div><p>Note: If the &ldquo;preservePoolsOnDelete&rdquo; filesystem attribute is set to true, the above command won&rsquo;t delete the pools. Creating again the filesystem with the same CRD will reuse again the previous pools.</p>
<h6 id="flex-driver-1">Flex Driver</h6>
<p>To create a volume based on the flex driver instead of the CSI driver, see the <a href="https://github.com/rook/rook/blob/release-1.1/cluster/examples/kubernetes/ceph/flex/kube-registry.yaml" target="_blank" rel="noopener noreffer">kube-registry.yaml</a> example manifest or refer to the complete flow in the Rook v1.0 <a href="https://rook.io/docs/rook/v1.0/ceph-filesystem.html" target="_blank" rel="noopener noreffer">Shared File System</a> documentation.</p>
<p><em><strong>Advanced Example: Erasure Coded Filesystem</strong></em><br>
The Ceph filesystem example can be found here: <a href="https://rook.io/docs/rook/v1.1/ceph-filesystem-crd.html#erasure-coded" target="_blank" rel="noopener noreffer">Ceph Shared File System - Samples - Erasure Coded</a>.</p>
<h4 id="ceph-dashboard">Ceph Dashboard</h4>
<p>The dashboard is a very helpful tool to give you an overview of the status of your cluster, including overall health, status of the mon quorum, status of the mgr, osd, and other Ceph daemons, view pools and PG status, show logs for the daemons, and more. Rook makes it simple to enable the dashboard.</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="../../images/rook-ceph-dashboard-20191020.png"
        data-srcset="../../images/rook-ceph-dashboard-20191020.png, ../../images/rook-ceph-dashboard-20191020.png 1.5x, ../../images/rook-ceph-dashboard-20191020.png 2x"
        data-sizes="auto"
        alt="../../images/rook-ceph-dashboard-20191020.png"
        title="The Ceph dashboard" /></p>
<h5 id="enable-the-dashboard">Enable the Dashboard</h5>
<p>The <a href="http://docs.ceph.com/docs/mimic/mgr/dashboard/" target="_blank" rel="noopener noreffer">dashboard</a> can be enabled with settings in the cluster CRD. The cluster CRD must have the dashboard <code>enabled</code> setting set to <code>true</code>.</p>
<p>This is the default setting in the example manifests.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="w">  </span><span class="k">spec</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="k">dashboard</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="k">enabled</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>The Rook operator will enable the ceph-mgr dashboard module. A K8s service will be created to expose that port inside the cluster. Rook will enable port 8443 for https access.</p>
<p>This example shows that port 8443 was configured.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">kubectl -n rook-ceph get service
NAME                         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT<span class="o">(</span>S<span class="o">)</span>          AGE
rook-ceph-mgr                ClusterIP   10.108.111.192   &lt;none&gt;        9283/TCP         3h
rook-ceph-mgr-dashboard      ClusterIP   10.110.113.240   &lt;none&gt;        8443/TCP         3h
</code></pre></td></tr></table>
</div>
</div><p>The first service is for reporting the <a href="ceph-monitoring.md" rel="">Prometheus metrics</a>, while the latter service is for the dashboard.</p>
<p>If you are on a node in the cluster, you will be able to connect to the dashboard by using either the DNS name of the service at <code>https://rook-ceph-mgr-dashboard-https:8443</code> or by connecting to the cluster IP, in this example at <code>https://10.110.113.240:8443</code>.</p>
<h6 id="credentials">Credentials</h6>
<p>After you connect to the dashboard you will need to login for secure access. Rook creates a default user named <code>admin</code> and generates a secret called <code>rook-ceph-dashboard-admin-password</code> in the namespace where rook is running.</p>
<p>To retrieve the generated password, you can run the following:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">kubectl -n rook-ceph get secret rook-ceph-dashboard-password -o jsonpath=&#34;{[&#39;data&#39;][&#39;password&#39;]}&#34; | base64 --decode &amp;&amp; echo
</code></pre></td></tr></table>
</div>
</div><h5 id="configure-the-dashboard">Configure the Dashboard</h5>
<p>The following dashboard configuration settings are supported:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="w">  </span><span class="k">spec</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="k">dashboard</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="k">urlPrefix</span><span class="p">:</span><span class="w"> </span>/ceph-dashboard<span class="w">
</span><span class="w">      </span><span class="k">port</span><span class="p">:</span><span class="w"> </span><span class="m">8443</span><span class="w">
</span><span class="w">      </span><span class="k">ssl</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><ul>
<li>
<p><code>urlPrefix</code> If you are accessing the dashboard via a reverse proxy, you may wish to serve it under a URL prefix.  To get the dashboard to use hyperlinks that include your prefix, you can set the <code>urlPrefix</code> setting.</p>
</li>
<li>
<p><code>port</code> The port that the dashboard is served on may be changed from the default using the <code>port</code> setting. The corresponding K8s service exposing the port will automatically be updated.</p>
</li>
<li>
<p><code>ssl</code> The dashboard may be served without SSL (useful for when you deploy the dashboard behind a proxy already served using SSL) by setting the <code>ssl</code> option to be false.</p>
</li>
</ul>
<h5 id="viewing-the-dashboard-external-to-the-cluster">Viewing the Dashboard External to the Cluster</h5>
<p>Commonly you will want to view the dashboard from outside the cluster. For example, on a development machine with the cluster running inside minikube you will want to access the dashboard from the host.</p>
<p>There are several ways to expose a service that will depend on the environment you are running in.
You can use an <a href="https://kubernetes.io/docs/concepts/services-networking/ingress/" target="_blank" rel="noopener noreffer">Ingress Controller</a> or <a href="https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types" target="_blank" rel="noopener noreffer">other methods</a> for exposing services such as NodePort, LoadBalancer, or ExternalIPs.</p>
<h6 id="node-port">Node Port</h6>
<p>The simplest way to expose the service in minikube or similar environment is using the NodePort to open a port on the VM that can be accessed by the host. To create a service with the NodePort, save this yaml as <code>dashboard-external-https.yaml</code>.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="k">apiVersion</span><span class="p">:</span><span class="w"> </span>v1<span class="w">
</span><span class="w"></span><span class="k">kind</span><span class="p">:</span><span class="w"> </span>Service<span class="w">
</span><span class="w"></span><span class="k">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="k">name</span><span class="p">:</span><span class="w"> </span>rook-ceph-mgr-dashboard-external-https<span class="w">
</span><span class="w">  </span><span class="k">namespace</span><span class="p">:</span><span class="w"> </span>rook-ceph<span class="w">
</span><span class="w">  </span><span class="k">labels</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="k">app</span><span class="p">:</span><span class="w"> </span>rook-ceph-mgr<span class="w">
</span><span class="w">    </span><span class="k">rook_cluster</span><span class="p">:</span><span class="w"> </span>rook-ceph<span class="w">
</span><span class="w"></span><span class="k">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="k">ports</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="k">name</span><span class="p">:</span><span class="w"> </span>dashboard<span class="w">
</span><span class="w">    </span><span class="k">port</span><span class="p">:</span><span class="w"> </span><span class="m">8443</span><span class="w">
</span><span class="w">    </span><span class="k">protocol</span><span class="p">:</span><span class="w"> </span>TCP<span class="w">
</span><span class="w">    </span><span class="k">targetPort</span><span class="p">:</span><span class="w"> </span><span class="m">8443</span><span class="w">
</span><span class="w">  </span><span class="k">selector</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="k">app</span><span class="p">:</span><span class="w"> </span>rook-ceph-mgr<span class="w">
</span><span class="w">    </span><span class="k">rook_cluster</span><span class="p">:</span><span class="w"> </span>rook-ceph<span class="w">
</span><span class="w">  </span><span class="k">sessionAffinity</span><span class="p">:</span><span class="w"> </span>None<span class="w">
</span><span class="w">  </span><span class="k">type</span><span class="p">:</span><span class="w"> </span>NodePort<span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>Now create the service:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ kubectl create -f dashboard-external-https.yaml
</code></pre></td></tr></table>
</div>
</div><p>You will see the new service <code>rook-ceph-mgr-dashboard-external-https</code> created:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ kubectl -n rook-ceph get service
NAME                                    TYPE        CLUSTER-IP       EXTERNAL-IP   PORT<span class="o">(</span>S<span class="o">)</span>          AGE
rook-ceph-mgr                           ClusterIP   10.108.111.192   &lt;none&gt;        9283/TCP         4h
rook-ceph-mgr-dashboard                 ClusterIP   10.110.113.240   &lt;none&gt;        8443/TCP         4h
rook-ceph-mgr-dashboard-external-https  NodePort    10.101.209.6     &lt;none&gt;        8443:31176/TCP   4h
</code></pre></td></tr></table>
</div>
</div><p>In this example, port <code>31176</code> will be opened to expose port <code>8443</code> from the ceph-mgr pod. Find the ip address of the VM. If using minikube, you can run <code>minikube ip</code> to find the ip address.</p>
<p>Now you can enter the URL in your browser such as <code>https://192.168.99.110:31176</code> and the dashboard will appear.</p>
<h6 id="load-balancer">Load Balancer</h6>
<p>If you have a cluster on a cloud provider that supports load balancers, you can create a service that is provisioned with a public hostname.</p>
<p>The yaml is the same as <code>dashboard-external-https.yaml</code> except for the following line:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="w">  </span><span class="k">type</span><span class="p">:</span><span class="w"> </span>LoadBalancer<span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>Now create the service:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ kubectl create -f dashboard-loadbalancer.yaml
</code></pre></td></tr></table>
</div>
</div><p>You will see the new service <code>rook-ceph-mgr-dashboard-loadbalancer</code> created:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ kubectl -n rook-ceph get service
NAME                                     TYPE           CLUSTER-IP       EXTERNAL-IP                                                               PORT<span class="o">(</span>S<span class="o">)</span>             AGE
rook-ceph-mgr                            ClusterIP      172.30.11.40     &lt;none&gt;                                                                    9283/TCP            4h
rook-ceph-mgr-dashboard                  ClusterIP      172.30.203.185   &lt;none&gt;                                                                    8443/TCP            4h
rook-ceph-mgr-dashboard-loadbalancer     LoadBalancer   172.30.27.242    a7f23e8e2839511e9b7a5122b08f2038-1251669398.us-east-1.elb.amazonaws.com   8443:32747/TCP      4h
</code></pre></td></tr></table>
</div>
</div><p>Now you can enter the URL in your browser such as</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">https://a7f23e8e2839511e9b7a5122b08f2038-1251669398.us-east-1.elb.amazonaws.com:8443
</code></pre></td></tr></table>
</div>
</div><p>and the dashboard will appear.</p>
<h6 id="ingress-controller">Ingress Controller</h6>
<p>If you have a cluster with an <a href="https://kubernetes.github.io/ingress-nginx/" target="_blank" rel="noopener noreffer">nginx Ingress Controller</a> and a Certificate Manager (e.g. <a href="https://cert-manager.readthedocs.io/" target="_blank" rel="noopener noreffer">cert-manager</a>) then you can create an Ingress like the one below. This example achieves four things:</p>
<ol>
<li>Exposes the dashboard on the Internet (using an reverse proxy)</li>
<li>Issues an valid TLS Certificate for the specified domain name (using <a href="https://en.wikipedia.org/wiki/Automated_Certificate_Management_Environment" target="_blank" rel="noopener noreffer">ACME</a>)</li>
<li>Tells the reverse proxy that the dashboard itself uses HTTPS</li>
<li>Tells the reverse proxy that the dashboard itself does not have a valid certificate (it is self-signed)</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="k">apiVersion</span><span class="p">:</span><span class="w"> </span>extensions/v1beta1<span class="w">
</span><span class="w"></span><span class="k">kind</span><span class="p">:</span><span class="w"> </span>Ingress<span class="w">
</span><span class="w"></span><span class="k">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="k">name</span><span class="p">:</span><span class="w"> </span>rook-ceph-mgr-dashboard<span class="w">
</span><span class="w">  </span><span class="k">namespace</span><span class="p">:</span><span class="w"> </span>rook-ceph<span class="w">
</span><span class="w">  </span><span class="k">annotations</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="k">kubernetes.io/ingress.class</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;nginx&#34;</span><span class="w">
</span><span class="w">    </span><span class="k">kubernetes.io/tls-acme</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;true&#34;</span><span class="w">
</span><span class="w">    </span><span class="k">nginx.ingress.kubernetes.io/backend-protocol</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;HTTPS&#34;</span><span class="w">
</span><span class="w">    </span><span class="k">nginx.ingress.kubernetes.io/server-snippet</span><span class="p">:</span><span class="w"> </span><span class="sd">|
</span><span class="sd">      proxy_ssl_verify off;</span><span class="w">
</span><span class="w"></span><span class="k">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="k">tls</span><span class="p">:</span><span class="w">
</span><span class="w">   </span>- <span class="k">hosts</span><span class="p">:</span><span class="w">
</span><span class="w">     </span>- rook-ceph.example.com<span class="w">
</span><span class="w">     </span><span class="k">secretName</span><span class="p">:</span><span class="w"> </span>rook-ceph.example.com<span class="w">
</span><span class="w">  </span><span class="k">rules</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="k">host</span><span class="p">:</span><span class="w"> </span>rook-ceph.example.com<span class="w">
</span><span class="w">    </span><span class="k">http</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="k">paths</span><span class="p">:</span><span class="w">
</span><span class="w">      </span>- <span class="k">path</span><span class="p">:</span><span class="w"> </span>/<span class="w">
</span><span class="w">        </span><span class="k">backend</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="k">serviceName</span><span class="p">:</span><span class="w"> </span>rook-ceph-mgr-dashboard<span class="w">
</span><span class="w">          </span><span class="k">servicePort</span><span class="p">:</span><span class="w"> </span>https-dashboard<span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>Customise the Ingress resource to match your cluster. Replace the example domain name <code>rook-ceph.example.com</code> with a domain name that will resolve to your Ingress Controller (creating the DNS entry if required).</p>
<p>Now create the Ingress:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ kubectl create -f dashboard-ingress-https.yaml
</code></pre></td></tr></table>
</div>
</div><p>You will see the new Ingress <code>rook-ceph-mgr-dashboard</code> created:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ kubectl -n rook-ceph get ingress
NAME                      HOSTS                      ADDRESS   PORTS     AGE
rook-ceph-mgr-dashboard   rook-ceph.example.com      80, <span class="m">443</span>   5m
</code></pre></td></tr></table>
</div>
</div><p>And the new Secret for the TLS certificate:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ kubectl -n rook-ceph get secret rook-ceph.example.com
NAME                       TYPE                DATA      AGE
rook-ceph.example.com      kubernetes.io/tls   <span class="m">2</span>         4m
</code></pre></td></tr></table>
</div>
</div><p>You can now browse to <code>https://rook-ceph.example.com/</code> to log into the dashboard.</p>
<h5 id="enabling-dashboard-object-gateway-management">Enabling Dashboard Object Gateway management</h5>
<p>Provided you have deployed the <a href="https://rook.io/docs/rook/v1.1/ceph-toolbox.html" target="_blank" rel="noopener noreffer">Ceph Toolbox</a>, created an <a href="https://rook.io/docs/rook/v1.1/ceph-object.html" target="_blank" rel="noopener noreffer">Object Store</a> and a user, you can enable <a href="http://docs.ceph.com/docs/master/mgr/dashboard/#enabling-the-object-gateway-management-frontend" target="_blank" rel="noopener noreffer">Object Gateway management</a> by providing the user credentials to the dashboard:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback"># Access toolbox CLI:
kubectl -n rook-ceph exec -it $(kubectl -n rook-ceph get pod -l &#34;app=rook-ceph-tools&#34; -o jsonpath=&#39;{.items[0].metadata.name}&#39;) bash

# Enable system flag on the user:
radosgw-admin user modify --uid=my-user --system

# Provide the user credentials:
ceph dashboard set-rgw-api-user-id my-user
ceph dashboard set-rgw-api-access-key &lt;access-key&gt;
ceph dashboard set-rgw-api-secret-key &lt;secret-key&gt;
</code></pre></td></tr></table>
</div>
</div><p>Now you can access the <em>Object Gateway</em> menu items.</p>
<h4 id="tools">Tools</h4>
<p>We have created a toolbox container that contains the full suite of Ceph clients for debugging and troubleshooting your Rook cluster.  Please see the <a href="https://rook.io/docs/rook/v1.1/ceph-toolbox.html" target="_blank" rel="noopener noreffer">toolbox readme</a> for setup and usage information. Also see our <a href="https://rook.io/docs/rook/v1.1/advanced-configuration.md" target="_blank" rel="noopener noreffer">advanced configuration</a> document for helpful maintenance and tuning examples.</p>
<h5 id="rook-toolbox">Rook Toolbox</h5>
<p>The Rook toolbox is a container with common tools used for rook debugging and testing.</p>
<p>The toolbox is based on CentOS, so more tools of your choosing can be easily installed with <code>yum</code>.</p>
<h6 id="running-the-toolbox-in-kubernetes">Running the Toolbox in Kubernetes</h6>
<p>The rook toolbox can run as a deployment in a Kubernetes cluster.  After you ensure you have a running Kubernetes cluster with rook deployed (see the <a href="https://rook.io/docs/rook/v1.1/ceph-quickstart.html" target="_blank" rel="noopener noreffer">Kubernetes</a> instructions), launch the rook-ceph-tools pod.</p>
<p>Save the tools spec as <code>toolbox.yaml</code>:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="k">apiVersion</span><span class="p">:</span><span class="w"> </span>apps/v1<span class="w">
</span><span class="w"></span><span class="k">kind</span><span class="p">:</span><span class="w"> </span>Deployment<span class="w">
</span><span class="w"></span><span class="k">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="k">name</span><span class="p">:</span><span class="w"> </span>rook-ceph-tools<span class="w">
</span><span class="w">  </span><span class="k">namespace</span><span class="p">:</span><span class="w"> </span>rook-ceph<span class="w">
</span><span class="w">  </span><span class="k">labels</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="k">app</span><span class="p">:</span><span class="w"> </span>rook-ceph-tools<span class="w">
</span><span class="w"></span><span class="k">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="k">replicas</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">  </span><span class="k">selector</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="k">matchLabels</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="k">app</span><span class="p">:</span><span class="w"> </span>rook-ceph-tools<span class="w">
</span><span class="w">  </span><span class="k">template</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="k">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="k">labels</span><span class="p">:</span><span class="w">
</span><span class="w">        </span><span class="k">app</span><span class="p">:</span><span class="w"> </span>rook-ceph-tools<span class="w">
</span><span class="w">    </span><span class="k">spec</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="k">dnsPolicy</span><span class="p">:</span><span class="w"> </span>ClusterFirstWithHostNet<span class="w">
</span><span class="w">      </span><span class="k">containers</span><span class="p">:</span><span class="w">
</span><span class="w">      </span>- <span class="k">name</span><span class="p">:</span><span class="w"> </span>rook-ceph-tools<span class="w">
</span><span class="w">        </span><span class="k">image</span><span class="p">:</span><span class="w"> </span>rook/ceph<span class="p">:</span>master<span class="w">
</span><span class="w">        </span><span class="k">command</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">&#34;/tini&#34;</span><span class="p">]</span><span class="w">
</span><span class="w">        </span><span class="k">args</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">&#34;-g&#34;</span><span class="p">,</span><span class="w"> </span><span class="s2">&#34;--&#34;</span><span class="p">,</span><span class="w"> </span><span class="s2">&#34;/usr/local/bin/toolbox.sh&#34;</span><span class="p">]</span><span class="w">
</span><span class="w">        </span><span class="k">imagePullPolicy</span><span class="p">:</span><span class="w"> </span>IfNotPresent<span class="w">
</span><span class="w">        </span><span class="k">env</span><span class="p">:</span><span class="w">
</span><span class="w">          </span>- <span class="k">name</span><span class="p">:</span><span class="w"> </span>ROOK_ADMIN_SECRET<span class="w">
</span><span class="w">            </span><span class="k">valueFrom</span><span class="p">:</span><span class="w">
</span><span class="w">              </span><span class="k">secretKeyRef</span><span class="p">:</span><span class="w">
</span><span class="w">                </span><span class="k">name</span><span class="p">:</span><span class="w"> </span>rook-ceph-mon<span class="w">
</span><span class="w">                </span><span class="k">key</span><span class="p">:</span><span class="w"> </span>admin-secret<span class="w">
</span><span class="w">        </span><span class="k">securityContext</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="k">privileged</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">        </span><span class="k">volumeMounts</span><span class="p">:</span><span class="w">
</span><span class="w">          </span>- <span class="k">mountPath</span><span class="p">:</span><span class="w"> </span>/dev<span class="w">
</span><span class="w">            </span><span class="k">name</span><span class="p">:</span><span class="w"> </span>dev<span class="w">
</span><span class="w">          </span>- <span class="k">mountPath</span><span class="p">:</span><span class="w"> </span>/sys/bus<span class="w">
</span><span class="w">            </span><span class="k">name</span><span class="p">:</span><span class="w"> </span>sysbus<span class="w">
</span><span class="w">          </span>- <span class="k">mountPath</span><span class="p">:</span><span class="w"> </span>/lib/modules<span class="w">
</span><span class="w">            </span><span class="k">name</span><span class="p">:</span><span class="w"> </span>libmodules<span class="w">
</span><span class="w">          </span>- <span class="k">name</span><span class="p">:</span><span class="w"> </span>mon-endpoint-volume<span class="w">
</span><span class="w">            </span><span class="k">mountPath</span><span class="p">:</span><span class="w"> </span>/etc/rook<span class="w">
</span><span class="w">      </span><span class="c"># if hostNetwork: false, the &#34;rbd map&#34; command hangs, see https://github.com/rook/rook/issues/2021</span><span class="w">
</span><span class="w">      </span><span class="k">hostNetwork</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">      </span><span class="k">volumes</span><span class="p">:</span><span class="w">
</span><span class="w">        </span>- <span class="k">name</span><span class="p">:</span><span class="w"> </span>dev<span class="w">
</span><span class="w">          </span><span class="k">hostPath</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="k">path</span><span class="p">:</span><span class="w"> </span>/dev<span class="w">
</span><span class="w">        </span>- <span class="k">name</span><span class="p">:</span><span class="w"> </span>sysbus<span class="w">
</span><span class="w">          </span><span class="k">hostPath</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="k">path</span><span class="p">:</span><span class="w"> </span>/sys/bus<span class="w">
</span><span class="w">        </span>- <span class="k">name</span><span class="p">:</span><span class="w"> </span>libmodules<span class="w">
</span><span class="w">          </span><span class="k">hostPath</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="k">path</span><span class="p">:</span><span class="w"> </span>/lib/modules<span class="w">
</span><span class="w">        </span>- <span class="k">name</span><span class="p">:</span><span class="w"> </span>mon-endpoint-volume<span class="w">
</span><span class="w">          </span><span class="k">configMap</span><span class="p">:</span><span class="w">
</span><span class="w">            </span><span class="k">name</span><span class="p">:</span><span class="w"> </span>rook-ceph-mon-endpoints<span class="w">
</span><span class="w">            </span><span class="k">items</span><span class="p">:</span><span class="w">
</span><span class="w">            </span>- <span class="k">key</span><span class="p">:</span><span class="w"> </span>data<span class="w">
</span><span class="w">              </span><span class="k">path</span><span class="p">:</span><span class="w"> </span>mon-endpoints<span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>Launch the rook-ceph-tools pod:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">kubectl create -f toolbox.yaml
</code></pre></td></tr></table>
</div>
</div><p>Wait for the toolbox pod to download its container and get to the <code>running</code> state:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">kubectl -n rook-ceph get pod -l <span class="s2">&#34;app=rook-ceph-tools&#34;</span>
</code></pre></td></tr></table>
</div>
</div><p>Once the rook-ceph-tools pod is running, you can connect to it with:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">kubectl -n rook-ceph <span class="nb">exec</span> -it <span class="k">$(</span>kubectl -n rook-ceph get pod -l <span class="s2">&#34;app=rook-ceph-tools&#34;</span> -o <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">&#39;{.items[0].metadata.name}&#39;</span><span class="k">)</span> bash
</code></pre></td></tr></table>
</div>
</div><p>All available tools in the toolbox are ready for your troubleshooting needs.  Example:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">ceph status
ceph osd status
ceph df
rados df
</code></pre></td></tr></table>
</div>
</div><p>When you are done with the toolbox, you can remove the deployment:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">kubectl -n rook-ceph delete deployment rook-ceph-tools
</code></pre></td></tr></table>
</div>
</div><h6 id="troubleshooting-without-the-toolbox">Troubleshooting without the Toolbox</h6>
<p>The Ceph tools will commonly be the only tools needed to troubleshoot a cluster. In that case, you can connect to any of the rook pods and execute the ceph commands in the same way that you would in the toolbox pod such as the mon pods or the operator pod.</p>
<p>If connecting to the mon pods, make sure you connect to the mon most recently started. The mons keep the config updated in memory after starting and may not have the latest config on disk.
For example, after starting the cluster connect to the <code>mon2</code> pod instead of <code>mon0</code>.</p>
<h4 id="monitoring">Monitoring</h4>
<p>Each Rook Ceph cluster has some built in metrics collectors/exporters for monitoring with <a href="https://prometheus.io/" target="_blank" rel="noopener noreffer">Prometheus</a>.</p>
<p>If you do not have Prometheus running, follow the steps below to enable monitoring of Rook. If your cluster already contains a Prometheus instance, it will automatically discover Rooks scrape endpoint using the standard <code>prometheus.io/scrape</code> and <code>prometheus.io/port</code> annotations.</p>
<h5 id="prometheus-operator">Prometheus Operator</h5>
<p>First the Prometheus operator needs to be started in the cluster so it can watch for our requests to start monitoring Rook and respond by deploying the correct Prometheus pods and configuration.
A full explanation can be found in the <a href="https://github.com/coreos/prometheus-operator" target="_blank" rel="noopener noreffer">Prometheus operator repository on GitHub</a>, but the quick instructions can be found here:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/v0.26.0/bundle.yaml
</code></pre></td></tr></table>
</div>
</div><p>This will start the Prometheus operator, but before moving on, wait until the operator is in the <code>Running</code> state:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">kubectl get pod
</code></pre></td></tr></table>
</div>
</div><p>Once the Prometheus operator is in the <code>Running</code> state, proceed to the next section.</p>
<h5 id="prometheus-instances">Prometheus Instances</h5>
<p>With the Prometheus operator running, we can create a service monitor that will watch the Rook cluster and collect metrics regularly.</p>
<p>From the root of your locally cloned Rook repo, go the monitoring directory:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="nb">cd</span> cluster/examples/kubernetes/ceph/monitoring
</code></pre></td></tr></table>
</div>
</div><p>Create the service monitor as well as the Prometheus server pod and service:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">kubectl create -f service-monitor.yaml
kubectl create -f prometheus.yaml
kubectl create -f prometheus-service.yaml
</code></pre></td></tr></table>
</div>
</div><p>Ensure that the Prometheus server pod gets created and advances to the <code>Running</code> state before moving on:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">kubectl -n rook-ceph get pod prometheus-rook-prometheus-0
</code></pre></td></tr></table>
</div>
</div><h5 id="prometheus-web-console">Prometheus Web Console</h5>
<p>Once the Prometheus server is running, you can open a web browser and go to the URL that is output from this command:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="nb">echo</span> <span class="s2">&#34;http://</span><span class="k">$(</span>kubectl -n rook-ceph -o <span class="nv">jsonpath</span><span class="o">={</span>.status.hostIP<span class="o">}</span> get pod prometheus-rook-prometheus-0<span class="k">)</span><span class="s2">:30900&#34;</span>
</code></pre></td></tr></table>
</div>
</div><p>You should now see the Prometheus monitoring website.</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="../../images/rook-ceph-prometheus-monitor-20191020.png"
        data-srcset="../../images/rook-ceph-prometheus-monitor-20191020.png, ../../images/rook-ceph-prometheus-monitor-20191020.png 1.5x, ../../images/rook-ceph-prometheus-monitor-20191020.png 2x"
        data-sizes="auto"
        alt="../../images/rook-ceph-prometheus-monitor-20191020.png"
        title="Prometheus Monitoring Website" /></p>
<p>Click on <code>Graph</code> in the top navigation bar.</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="../../images/rook-ceph-prometheus-graph-20191020.png"
        data-srcset="../../images/rook-ceph-prometheus-graph-20191020.png, ../../images/rook-ceph-prometheus-graph-20191020.png 1.5x, ../../images/rook-ceph-prometheus-graph-20191020.png 2x"
        data-sizes="auto"
        alt="../../images/rook-ceph-prometheus-graph-20191020.png"
        title="Prometheus Add graph" /></p>
<p>In the dropdown that says <code>insert metric at cursor</code>, select any metric you would like to see, for example <code>ceph_cluster_total_used_bytes</code></p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="../../images/rook-ceph-prometheus-metric-cursor-20191020.png"
        data-srcset="../../images/rook-ceph-prometheus-metric-cursor-20191020.png, ../../images/rook-ceph-prometheus-metric-cursor-20191020.png 1.5x, ../../images/rook-ceph-prometheus-metric-cursor-20191020.png 2x"
        data-sizes="auto"
        alt="../../images/rook-ceph-prometheus-metric-cursor-20191020.png"
        title="Prometheus Select Metric" /></p>
<p>Click on the <code>Execute</code> button.</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="../../images/rook-ceph-prometheus-execute-metric-cursor-20191020.png"
        data-srcset="../../images/rook-ceph-prometheus-execute-metric-cursor-20191020.png, ../../images/rook-ceph-prometheus-execute-metric-cursor-20191020.png 1.5x, ../../images/rook-ceph-prometheus-execute-metric-cursor-20191020.png 2x"
        data-sizes="auto"
        alt="../../images/rook-ceph-prometheus-execute-metric-cursor-20191020.png"
        title="Prometheus Execute Metric" /></p>
<p>Below the <code>Execute</code> button, ensure the <code>Graph</code> tab is selected and you should now see a graph of your chosen metric over time.</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="../../images/rook-ceph-prometheus-metric-cursor-graph-20191020.png"
        data-srcset="../../images/rook-ceph-prometheus-metric-cursor-graph-20191020.png, ../../images/rook-ceph-prometheus-metric-cursor-graph-20191020.png 1.5x, ../../images/rook-ceph-prometheus-metric-cursor-graph-20191020.png 2x"
        data-sizes="auto"
        alt="../../images/rook-ceph-prometheus-metric-cursor-graph-20191020.png"
        title="Prometheus Execute Metric" /></p>
<h5 id="prometheus-consoles">Prometheus Consoles</h5>
<p>You can find Prometheus Consoles here: <a href="https://github.com/ceph/cephmetrics/tree/master/dashboards/current">https://github.com/ceph/cephmetrics/tree/master/dashboards/current</a>.</p>
<p>A guide to how you can write your own Prometheus consoles can be found on the official Prometheus site here: <a href="https://prometheus.io/docs/visualization/consoles/">https://prometheus.io/docs/visualization/consoles/</a>.</p>
<h5 id="prometheus-alerts">Prometheus Alerts</h5>
<p>To enable prometheus alerts,</p>
<p>first, create the RBAC rules to enable monitoring</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-BASH" data-lang="BASH">kubectl create -f cluster/examples/kubernetes/ceph/monitoring/rbac.yaml
</code></pre></td></tr></table>
</div>
</div><p>then, make following changes to <code>cluster.yaml</code> and deploy.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-YAML" data-lang="YAML"><span class="k">monitoring</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="k">enabled</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">  </span><span class="k">rulesNamespace</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;rook-ceph&#34;</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-BASH" data-lang="BASH">kubectl apply -f cluster.yaml
</code></pre></td></tr></table>
</div>
</div><p><em>Note: This expects prometheus to be pre-installed by the admin.</em></p>
<h5 id="grafana-dashboards">Grafana Dashboards</h5>
<p>The dashboards have been created by <a href="https://github.com/galexrt" target="_blank" rel="noopener noreffer">@galexrt</a>. For feedback on the dashboards please reach out to him on the <a href="https://slack.rook.io" target="_blank" rel="noopener noreffer">Rook.io Slack</a>.</p>
<blockquote>
<p><strong>NOTE</strong> The dashboards are only compatible with Grafana 5.0.3 or higher.</p>
</blockquote>
<p>The following Grafana dashboards are available:</p>
<ul>
<li><a href="https://grafana.com/dashboards/2842" target="_blank" rel="noopener noreffer">Ceph - Cluster</a></li>
<li><a href="https://grafana.com/dashboards/5336" target="_blank" rel="noopener noreffer">Ceph - OSD</a></li>
<li><a href="https://grafana.com/dashboards/5342" target="_blank" rel="noopener noreffer">Ceph - Pools</a></li>
</ul>
<h5 id="teardown-2">Teardown</h5>
<p>To clean up all the artifacts created by the monitoring walkthrough, copy/paste the entire block below (note that errors about resources &ldquo;not found&rdquo; can be ignored):</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">kubectl delete -f service-monitor.yaml
kubectl delete -f prometheus.yaml
kubectl delete -f prometheus-service.yaml
kubectl delete -f https://raw.githubusercontent.com/coreos/prometheus-operator/v0.26.0/bundle.yaml
</code></pre></td></tr></table>
</div>
</div><p>Then the rest of the instructions in the <a href="https://github.com/coreos/prometheus-operator#removal" target="_blank" rel="noopener noreffer">Prometheus Operator docs</a> can be followed to finish cleaning up.</p>
<h5 id="special-cases">Special Cases</h5>
<h6 id="tectonic-bare-metal">Tectonic Bare Metal</h6>
<p>Tectonic strongly discourages the <code>tectonic-system</code> Prometheus instance to be used outside their intentions, so you need to create a new <a href="https://coreos.com/operators/prometheus/docs/latest/" target="_blank" rel="noopener noreffer">Prometheus Operator</a> yourself.</p>
<p>After this you only need to create the service monitor as stated above.</p>
<h6 id="csi-liveness">CSI Liveness</h6>
<p>To integrate CSI liveness and grpc into ceph monitoring we will need to deploy a service and service monitor.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">kubectl create -f csi-metrics-service-monitor.yaml
</code></pre></td></tr></table>
</div>
</div><p>This will create the service monitor to have promethues monitor CSI</p>
<h4 id="teardown-3">Teardown</h4>
<p>If you want to tear down the cluster and bring up a new one, be aware of the following resources that will need to be cleaned up:</p>
<ul>
<li><code>rook-ceph</code> namespace: The Rook operator and cluster created by <code>operator.yaml</code> and <code>cluster.yaml</code> (the cluster CRD)</li>
<li><code>/var/lib/rook</code>: Path on each host in the cluster where configuration is cached by the ceph mons and osds</li>
</ul>
<p>Note that if you changed the default namespaces or paths such as <code>dataDirHostPath</code> in the sample yaml files, you will need to adjust these namespaces and paths throughout these instructions.</p>
<p>If you see issues tearing down the cluster, see the <a href="#troubleshooting" rel="">Troubleshooting</a> section below.</p>
<p>If you are tearing down a cluster frequently for development purposes, it is instead recommended to use an environment such as Minikube that can easily be reset without worrying about any of these steps.</p>
<h5 id="delete-the-block-and-file-artifacts">Delete the Block and File artifacts</h5>
<p>First you will need to clean up the resources created on top of the Rook cluster.</p>
<p>These commands will clean up the resources from the <a href="https://rook.io/docs/rook/v1.1/ceph-block.html#teardown" target="_blank" rel="noopener noreffer">block</a> and <a href="https://rook.io/docs/rook/v1.1/ceph-filesystem.html#teardown" target="_blank" rel="noopener noreffer">file</a> walkthroughs (unmount volumes, delete volume claims, etc). If you did not complete those parts of the walkthrough, you can skip these instructions:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">kubectl delete -f ../wordpress.yaml
kubectl delete -f ../mysql.yaml
kubectl delete -n rook-ceph cephblockpool replicapool
kubectl delete storageclass rook-ceph-block
kubectl delete -f csi/cephfs/kube-registry.yaml
kubectl delete storageclass rook-cephfs
</code></pre></td></tr></table>
</div>
</div><h5 id="delete-the-cephcluster-crd">Delete the CephCluster CRD</h5>
<p>After those block and file resources have been cleaned up, you can then delete your Rook cluster. This is important to delete <strong>before removing the Rook operator and agent or else resources may not be cleaned up properly</strong>.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">kubectl -n rook-ceph delete cephcluster rook-ceph
</code></pre></td></tr></table>
</div>
</div><p>Verify that the cluster CRD has been deleted before continuing to the next step.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">kubectl -n rook-ceph get cephcluster
</code></pre></td></tr></table>
</div>
</div><h5 id="delete-the-operator-and-related-resources">Delete the Operator and related Resources</h5>
<p>This will begin the process of the Rook Ceph operator and all other resources being cleaned up.
This includes related resources such as the agent and discover daemonsets with the following commands:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">kubectl delete -f operator.yaml
kubectl delete -f common.yaml
</code></pre></td></tr></table>
</div>
</div><h5 id="delete-the-data-on-hosts">Delete the data on hosts</h5>
<p>IMPORTANT: The final cleanup step requires deleting files on each host in the cluster. All files under the <code>dataDirHostPath</code> property specified in the cluster CRD will need to be deleted. Otherwise, inconsistent state will remain when a new cluster is started.</p>
<p>Connect to each machine and delete <code>/var/lib/rook</code>, or the path specified by the <code>dataDirHostPath</code>.</p>
<p>In the future this step will not be necessary when we build on the K8s local storage feature.</p>
<p>If you modified the demo settings, additional cleanup is up to you for devices, host paths, etc.</p>
<p>Disks on nodes used by Rook for osds can be reset to a usable state with the following methods:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-sh" data-lang="sh"><span class="cp">#!/usr/bin/env bash
</span><span class="cp"></span><span class="nv">DISK</span><span class="o">=</span><span class="s2">&#34;/dev/sdb&#34;</span>
<span class="c1"># Zap the disk to a fresh, usable state (zap-all is important, b/c MBR has to be clean)</span>
<span class="c1"># You will have to run this step for all disks.</span>
sgdisk --zap-all <span class="nv">$DISK</span>

<span class="c1"># These steps only have to be run once on each node</span>
<span class="c1"># If rook sets up osds using ceph-volume, teardown leaves some devices mapped that lock the disks.</span>
ls /dev/mapper/ceph-* <span class="p">|</span> xargs -I% -- dmsetup remove %
<span class="c1"># ceph-volume setup can leave ceph-&lt;UUID&gt; directories in /dev (unnecessary clutter)</span>
rm -rf /dev/ceph-*
</code></pre></td></tr></table>
</div>
</div><h5 id="troubleshooting">Troubleshooting</h5>
<p>If the cleanup instructions are not executed in the order above, or you otherwise have difficulty cleaning up the cluster, here are a few things to try.</p>
<p>The most common issue cleaning up the cluster is that the <code>rook-ceph</code> namespace or the cluster CRD remain indefinitely in the <code>terminating</code> state. A namespace cannot be removed until all of its resources are removed, so look at which resources are pending termination.</p>
<p>Look at the pods:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">kubectl -n rook-ceph get pod
</code></pre></td></tr></table>
</div>
</div><p>If a pod is still terminating, you will need to wait or else attempt to forcefully terminate it (<code>kubectl delete pod &lt;name&gt;</code>).</p>
<p>Now look at the cluster CRD:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">kubectl -n rook-ceph get cephcluster
</code></pre></td></tr></table>
</div>
</div><p>If the cluster CRD still exists even though you have executed the delete command earlier, see the next section on removing the finalizer.</p>
<h6 id="removing-the-cluster-crd-finalizer">Removing the Cluster CRD Finalizer</h6>
<p>When a Cluster CRD is created, a <a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/extend-api-custom-resource-definitions/#finalizers" target="_blank" rel="noopener noreffer">finalizer</a> is added automatically by the Rook operator.</p>
<p>The finalizer will allow the operator to ensure that before the cluster CRD is deleted, all block and file mounts will be cleaned up. Without proper cleanup, pods consuming the storage will be hung indefinitely until a system reboot.</p>
<p>The operator is responsible for removing the finalizer after the mounts have been cleaned up.
If for some reason the operator is not able to remove the finalizer (ie. the operator is not running anymore), you can delete the finalizer manually with the following command:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">kubectl -n rook-ceph patch crd cephclusters.ceph.rook.io --type merge -p &#39;{&#34;metadata&#34;:{&#34;finalizers&#34;: [null]}}&#39;
</code></pre></td></tr></table>
</div>
</div><p>Within a few seconds you should see that the cluster CRD has been deleted and will no longer block other cleanup such as deleting the <code>rook-ceph</code> namespace.</p>
</div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>更新于 2019-10-20</span>
            </div>
            <div class="post-info-license"></div>
        </div>
        <div class="post-info-line">
            <div class="post-info-md"><span>
                            <a class="link-to-markdown" href="/rook-ceph-v1.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/index.md" target="_blank">阅读原始文档</a>
                        </span></div>
            <div class="post-info-share">
                <span><a href="javascript:void(0);" title="分享到 Twitter" data-sharer="twitter" data-url="https://msdemt.github.io/rook-ceph-v1.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" data-title="Rook-Ceph-v1.1-官方文档阅读笔记"><i class="fab fa-twitter fa-fw"></i></a><a href="javascript:void(0);" title="分享到 Facebook" data-sharer="facebook" data-url="https://msdemt.github.io/rook-ceph-v1.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"><i class="fab fa-facebook-square fa-fw"></i></a><a href="javascript:void(0);" title="分享到 Hacker News" data-sharer="hackernews" data-url="https://msdemt.github.io/rook-ceph-v1.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" data-title="Rook-Ceph-v1.1-官方文档阅读笔记"><i class="fab fa-hacker-news fa-fw"></i></a><a href="javascript:void(0);" title="分享到 Line" data-sharer="line" data-url="https://msdemt.github.io/rook-ceph-v1.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" data-title="Rook-Ceph-v1.1-官方文档阅读笔记"><i data-svg-src="https://cdn.jsdelivr.net/npm/simple-icons@2.14.0/icons/line.svg"></i></a><a href="javascript:void(0);" title="分享到 微博" data-sharer="weibo" data-url="https://msdemt.github.io/rook-ceph-v1.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" data-title="Rook-Ceph-v1.1-官方文档阅读笔记" data-ralateuid="xxxx"><i class="fab fa-weibo fa-fw"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">返回</a></span>&nbsp;|&nbsp;<span><a href="/">主页</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/centos7%E6%90%AD%E5%BB%BAnfs-server-20191019/" class="prev" rel="prev" title="Centos7搭建NFS-Server-20191019"><i class="fas fa-angle-left fa-fw"></i>Centos7搭建NFS-Server-20191019</a>
            <a href="/kubernetes-dashboard-v1.10.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" class="next" rel="next" title="Kubernetes-Dashboard-V1.10.1-官方文档阅读笔记">Kubernetes-Dashboard-V1.10.1-官方文档阅读笔记<i class="fas fa-angle-right fa-fw"></i></a></div>
</div>
</article></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line">由 <a href="https://gohugo.io/" target="_blank" rel="noopener noreffer" title="Hugo 0.72.0">Hugo</a> 强力驱动 | 主题 - <a href="https://github.com/dillonzq/LoveIt" target="_blank" rel="noopener noreffer" title="LoveIt 0.2.10"><i class="far fa-kiss-wink-heart fa-fw"></i> LoveIt</a>
                </div><div class="footer-line"><i class="far fa-copyright fa-fw"></i><span itemprop="copyrightYear">2019 - 2020</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="/" target="_blank">hekai</a></span>&nbsp;|&nbsp;<span class="license"><a rel="license external nofollow noopener noreffer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="回到顶部">
                <i class="fas fa-arrow-up fa-fw"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="查看评论">
                <i class="fas fa-comment fa-fw"></i>
            </a>
        </div><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/smooth-scroll@16.1.3/dist/smooth-scroll.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/autocomplete.js@0.37.1/dist/autocomplete.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lunr@2.3.8/lunr.min.js"></script><script type="text/javascript" src="/lib/lunr/lunr.stemmer.support.min.js"></script><script type="text/javascript" src="/lib/lunr/lunr.zh.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lazysizes@5.2.2/lazysizes.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/clipboard@2.0.6/dist/clipboard.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/sharer.js@0.4.0/sharer.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"复制到剪贴板","maxShownLines":10},"comment":{},"search":{"highlightTag":"em","lunrIndexURL":"/index.json","lunrLanguageCode":"zh","lunrSegmentitURL":"/lib/lunr/lunr.segmentit.js","maxResultLength":10,"noResultsFound":"没有找到结果","snippetLength":50,"type":"lunr"}};</script><script type="text/javascript" src="/js/theme.min.js"></script></body>
</html>
