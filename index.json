[{"categories":null,"content":"idea 安装后，第一件事应该就是配置下字符集编码，因为默认情况下，idea 新建某些文件时使用的是操作系统的默认编码，该编码可能为 GBK ，然而我们平常使用的编码是 UTF-8 。 gbk 编码的文件，使用 utf-8 打开后会出现中文乱码的问题。 安装 idea 后，在左栏中选择 Customize ，然后选择 All settings ，依次点击 Editor -\u003e File Encodings -\u003e 将本页的字符集编码修改为 UTF-8 ，然后保存。这样就配置好idea的默认项目的字符集编码了。新建的项目和新打开的项目都会使用该字符集编码。 如果，安装 idea 后，打开过某个项目A，那么该项目的字符集编码配置会保存在 .idea 文件夹中，当然，现在该项目的字符集编码应该为系统字符集编码，比如 GBK ，这不是我们期望的。 解决方式1：在idea中打开该项目，然后依次点击 File -\u003e Settings -\u003e Editor -\u003e File Encodings -\u003e 将本页的字符集编码修改为 UTF-8 ，然后保存。 解决方式2: 删除该项目下的 .idea 文件夹，然后使用上述方法配置 idea 的默认字符集编码，然后重新打开该项目 建议，同时配置下包自动导入，可以在开发中节省时间。在左栏中选择 Customize ，然后选择 All settings ，依次点击 Editor -\u003e General -\u003e Auto Import -\u003e 选中 Add unambiguous import on the fly 和 Optimize imports on the fly 这个 All Settings 配置 也可以在已打开项目的 idea 页面 中 File -\u003e New Projects Settings -\u003e Settings For New Projects 进行配置。 建议，配置下 Annotation Processing ，lombok需要该功能，位置：File -\u003e New Projects Settings -\u003e Settings For New Projects -\u003e Build, Execution, Deployment -\u003e Compiler -\u003e Annotation Processors -\u003e 选中 Enable annotation processing -\u003e Apply ","date":"2021-02-03","objectID":"/posts/idea/idea%E9%85%8D%E7%BD%AE%E5%AD%97%E7%AC%A6%E9%9B%86%E7%BC%96%E7%A0%81/:0:0","tags":null,"title":"Idea配置字符集编码","uri":"/posts/idea/idea%E9%85%8D%E7%BD%AE%E5%AD%97%E7%AC%A6%E9%9B%86%E7%BC%96%E7%A0%81/"},{"categories":null,"content":" 参考： https://www.jianshu.com/p/3f3d9e8d1efa https://www.jianshu.com/p/179daa24ef52 https://www.cnblogs.com/wang-yaz/p/13225830.html 本文使用的 spring boot 版本为 2.4.2 ","date":"2021-02-01","objectID":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/springboot%E7%BB%9F%E4%B8%80%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/:0:0","tags":null,"title":"Springboot统一异常处理","uri":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/springboot%E7%BB%9F%E4%B8%80%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/"},{"categories":null,"content":"使用 ExceptionHandler 注解处理 Controller 异常 Spring 3.0 版本新增了 @ExceptionHandler 注解，它的作用是：若在某个 Controller 类中定义一个异常处理方法，并在该异常处理方法上添加 @ExceptionHandler 注解（注解中指定需要处理的异常），那么当该 Controller 类中出现指定的异常时，会执行该异常处理方法。该异常处理方法可以使用 spring mvc 提供的数据绑定，比如注入 HttpServletRequest 等，还可以接受一个当前抛出的 Throwable 对象。 示例 package org.msdemt.simple.exception_handler.controller; import lombok.extern.slf4j.Slf4j; import org.springframework.web.bind.annotation.ExceptionHandler; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RestController; @Slf4j @RequestMapping(\"test\") @RestController public class UserController{ @RequestMapping(\"run\") public String RuntimeExceptionTest(){ throw new RuntimeException(\"runtime exception\"); } @RequestMapping(\"null\") public String NullPointerExceptionTest(){ throw new NullPointerException(\"null pointer exception\"); } @ExceptionHandler({NullPointerException.class, RuntimeException.class}) public String handleException(Exception e) { return e.getMessage(); } } 但是，这样一来，就必须在每个 Controller 类都定义一套这样的异常处理方法，就会造成冗余代码，而且若需要新增一种异常的处理逻辑，就必须修改所有的 Controller 类，很不优雅。 为了解决冗余代码，可以将所有的 Controller 继承一个 BaseController 基类，在 BaseController 统一处理异常。但是这样的代码由侵入性和耦合性，而且由于 Java 是单继承的，如此 Controller 就无法继承其他类了。 示例： package org.msdemt.simple.exception_handler.controller; import org.springframework.web.bind.annotation.ExceptionHandler; public class BaseController { @ExceptionHandler({NullPointerException.class, RuntimeException.class}) public String handleException(Exception e) { return e.getMessage(); } } package org.msdemt.simple.exception_handler.controller; import lombok.extern.slf4j.Slf4j; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RestController; @Slf4j @RequestMapping(\"test\") @RestController public class UserController extends BaseController{ @RequestMapping(\"run\") public String RuntimeExceptionTest(){ throw new RuntimeException(\"runtime exception\"); } @RequestMapping(\"null\") public String NullPointerExceptionTest(){ throw new NullPointerException(\"null pointer exception\"); } } java 8 版本开始支持接口中定义静态方法或默认方法，将 BaseController 定义为接口，这样 Controller 就可以继承其他类了。 package org.msdemt.simple.exception_handler.controller; import org.springframework.web.bind.annotation.ExceptionHandler; public interface BaseController { @ExceptionHandler({NullPointerException.class, RuntimeException.class}) static String handleException(Exception e) { return e.getMessage(); } } 或 package org.msdemt.simple.exception_handler.controller; import org.springframework.web.bind.annotation.ExceptionHandler; public interface BaseController { @ExceptionHandler({NullPointerException.class, RuntimeException.class}) default String handleException(Exception e) { return e.getMessage(); } } package org.msdemt.simple.exception_handler.controller; import lombok.extern.slf4j.Slf4j; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RestController; @Slf4j @RequestMapping(\"test\") @RestController public class UserController implements BaseController{ @RequestMapping(\"run\") public String RuntimeExceptionTest(){ throw new RuntimeException(\"runtime exception\"); } @RequestMapping(\"null\") public String NullPointerExceptionTest(){ throw new NullPointerException(\"null pointer exception\"); } } 上述的各种方式，Controller 和 异常处理器（@ExceptionHandler 注解的方法）耦合在一起的。 Spring 3.2 版本新增了 @ControllerAdvice 注解，可以与 @ExceptionHandler 注解配套使用，实现 Controller 和 异常处理器 的解耦：在单独的一个类中，使用 @ControllerAdvice 注解该类，并且在该类中添加使用 @ExceptionHandler 注解的各种异常的处理方法，实现统一处理所有 Controller 中出现的异常。 Spring 4.3 版本增加了一个注解 @RestControllerAdvice，该注解结合了 @ControllerAdvice 和 @ResponseBody 。 ","date":"2021-02-01","objectID":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/springboot%E7%BB%9F%E4%B8%80%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/:1:0","tags":null,"title":"Springboot统一异常处理","uri":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/springboot%E7%BB%9F%E4%B8%80%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/"},{"categories":null,"content":"ControllerAdvice 和 ExceptionHandler 注解组合处理异常 ","date":"2021-02-01","objectID":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/springboot%E7%BB%9F%E4%B8%80%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/:2:0","tags":null,"title":"Springboot统一异常处理","uri":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/springboot%E7%BB%9F%E4%B8%80%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/"},{"categories":null,"content":"异常的优雅判断 一般，我们使用 if {...} 等判断语句对异常进行判断，如果需要判断的地方很多，则 if 判断语句特别多，很不优雅。Spring 提供的 org.springframework.util.Assert ，能够让我们优雅地判断并抛出异常。示例如下： @Test public void test1() { Integer i = null; if (i == null) { throw new IllegalArgumentException(\"参数 i 不能为 null\"); } } @Test public void test2() { Integer i = null; Assert.notNull(i, \"参数 i 不能为 null\"); } Assert.notNull() 是怎么实现的呢？如下是 Assert.notNull() 的源码 /** * Assert that an object is not null. * Assert.notNull(clazz, \"The class must not be null\"); * Params: * object – the object to check * message – the exception message to use if the assertion fails * Throws: * IllegalArgumentException – if the object is null */ public static void notNull(@Nullable Object object, String message) { if (object == null) { throw new IllegalArgumentException(message); } } 可以看到， Assert 其实就是帮助我们把 if {...} 封装了下，从而使代码更加优雅。 那么我们能不能模仿 org.springframework.util.Assert ，也写一个断言类，不过断言失败后抛出的异常不是 IllegalArgumentException 这些内置异常，而是我们自定义的异常？ ","date":"2021-02-01","objectID":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/springboot%E7%BB%9F%E4%B8%80%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/:2:1","tags":null,"title":"Springboot统一异常处理","uri":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/springboot%E7%BB%9F%E4%B8%80%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/"},{"categories":null,"content":"自定义 Assert public interface Assert { /** * 新建异常 * @param args * @return */ BaseException newException(Object... args); /** * 新建异常 * @param t * @param args * @return */ BaseException newException(Throwable t, Object... args); /** * 若判断 obj 对象为空，则新建自定义异常并抛出 * * @param obj 待判断对象 */ default void assertNotNull(Object obj) { if (obj == null) { throw newException(); } } 其中，BaseException 是所有自定义异常的基类。 上面的 Assert 断言方法是使用接口的默认方法（jdk8新特性），当断言失败后，抛出的异常不是具体的某个异常，而是由 newException 接口方法提供。因为业务逻辑中出现的异常基本都是对应特定的场景，比如根据用户 id 获取用户信息，查询结果为 null ，此时抛出的异常可能为 UserNotFoundException ，并且有特定的异常代码（比如 7001）和异常描述（比如“用户不存在”），所以，具体抛出什么异常，由 Assert 的实现类决定。 按照上面的说法，岂不是有多少异常，就得定义多少的断言类和异常类？ ","date":"2021-02-01","objectID":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/springboot%E7%BB%9F%E4%B8%80%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/:2:2","tags":null,"title":"Springboot统一异常处理","uri":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/springboot%E7%BB%9F%E4%B8%80%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/"},{"categories":null,"content":"善解人意的 Enum 自定义异常 BaseException 有 2 个属性，即 code 、 mess ，可以和枚举 enum 结合 异常枚举接口 /** * 返回枚举接口 */ public interface IResponseEnum { /** * 获取返回代码 * @return 返回代码 */ String getCode(); /** * 获取返回描述 * @return 返回描述 */ String getMessage(); } 基础异常类（包含异常枚举接口） @Getter public class BaseException extends RuntimeException { private static final long serialVersionUID = 1L; /** * 异常信息枚举 */ protected IResponseEnum responseEnum; /** * 异常消息参数 */ protected Object[] args; public BaseException(IResponseEnum responseEnum) { super(responseEnum.getMessage()); this.responseEnum = responseEnum; } public BaseException(String code, String message) { super(message); this.responseEnum = new IResponseEnum() { @Override public String getCode() { return code; } @Override public String getMessage() { return message; } }; } public BaseException(IResponseEnum responseEnum, Object[] args, String message) { super(message); this.responseEnum = responseEnum; this.args = args; } public BaseException(IResponseEnum responseEnum, Object[] args, String message, Throwable cause) { super(message, cause); this.responseEnum = responseEnum; this.args = args; } } 业务异常类（继承基础异常类，也包含了异常枚举接口） public class BusinessException extends BaseException { private static final long serialVersionUID = 1L; public BusinessException(IResponseEnum responseEnum, Object[] args, String message) { super(responseEnum, args, message); } public BusinessException(IResponseEnum responseEnum, Object[] args, String message, Throwable cause) { super(responseEnum, args, message, cause); } } 业务异常断言接口（接口可以多继承，该接口继承了异常枚举接口和自定义断言接口，使用默认方法覆盖了自定义断言接口的新建异常方法） public interface BusinessExceptionAssert extends IResponseEnum, Assert { @Override default BaseException newException(Object... args) { String msg = this.getMessage(); if (args != null \u0026\u0026 args.length \u003e 0) { msg = MessageFormat.format(this.getMessage(), args); } return new BusinessException(this, args, msg); } @Override default BaseException newException(Throwable t, Object... args) { String msg = this.getMessage(); if (args != null \u0026\u0026 args.length \u003e 0) { msg = MessageFormat.format(this.getMessage(), args); } return new BusinessException(this, args, msg, t); } } 业务异常枚举类，该类实现了业务异常断言接口 @Getter @AllArgsConstructor public enum ResponseEnum implements BusinessExceptionAssert { /** * 请求实体为空 */ REQUEST_IS_NULL(\"1001\", \"Request is null.\"), /** * 用户不存在 */ USER_NOT_EXIST(\"1002\", \"User not exist.\") ; /** * 返回码 */ private String code; /** * 返回消息 */ private String message; } 使用示例 @RestController @RequestMapping(\"/user\") public class UserController { private IUserService userService; @Autowired public UserController(IUserService userService) { this.userService = userService; } @GetMapping(\"/{id}\") public R\u003cUser\u003e getUserById(@PathVariable(\"id\") Integer id){ User user = userService.findUserById(id); ResponseEnum.USER_NOT_EXIST.assertNotNull(user); return R.ok(user); } } 上文使用 assertNotNull() 判断 user 是否为空， 若不使用断言，代码可能如下 if(user == null){ throw new BusinessException(\"1002\", \"User not exist.\"); } 断言 assertNotNull() 方法实现如下 /** * 若判断 obj 对象为空，则新建自定义异常并抛出 * * @param obj 待判断对象 */ default void assertNotNull(Object obj) { if (obj == null) { throw newException(); } } 下面来分析下代码 ResponseEnum.USER_NOT_EXIST.assertNotNull(user); ，来看是如何实现的 因为枚举类 ResponseEnum 实现了接口 BusinessExceptionAssert，且接口 BusinessExceptionAssert 实现了自定义的 Assert 接口，所以枚举类 ResponseEnum 中的枚举实例可以调用接口 Assert 中的方法（此处为 assertNotNull()），方法内判断对象 user 为 null ，调用 newException() 方法新建异常，并将异常抛出。 此时 newException() 方法由接口 BusinessExceptionAssert 的默认方法实现，方法内容如下 @Override default BaseException newException(Object... args) { String msg = this.getMessage(); if (args != null \u0026\u0026 args.length \u003e 0) { msg = MessageFormat.format(this.getMessage(), args); } return new BusinessException(this, args, msg); } 此时， this 是 ResponseEnum.USER_NOT_EXIST ，获取枚举实例 ResponseEnum.USER_NOT_EXIST 中的 message ，并将参数配置到 message 的占位符中（如果有参数的话），然后新建一个业务异常。 public class BusinessException extends BaseException { private static final long serialVersionUID = 1L; public BusinessException(IRe","date":"2021-02-01","objectID":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/springboot%E7%BB%9F%E4%B8%80%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/:2:3","tags":null,"title":"Springboot统一异常处理","uri":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/springboot%E7%BB%9F%E4%B8%80%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/"},{"categories":null,"content":"统一异常处理类 上文使用枚举类结合（继承）Assert实现了判断并抛出异常，下面来介绍异常的统一处理。 使用 @ControllerAdvice 结合 @ExceptionHandler 实现异常的统一处理。 统一异常处理器代码如下 package org.msdemt.simple.unified_exception_handler_demo.exception.handler; import lombok.extern.slf4j.Slf4j; import org.msdemt.simple.unified_exception_handler_demo.constant.enums.ArgumentResponseEnum; import org.msdemt.simple.unified_exception_handler_demo.constant.enums.CommonResponseEnum; import org.msdemt.simple.unified_exception_handler_demo.constant.enums.ServletResponseEnum; import org.msdemt.simple.unified_exception_handler_demo.exception.BaseException; import org.msdemt.simple.unified_exception_handler_demo.exception.BusinessException; import org.msdemt.simple.unified_exception_handler_demo.i18n.UnifiedMessageSource; import org.msdemt.simple.unified_exception_handler_demo.pojo.response.ErrorResponse; import org.springframework.beans.ConversionNotSupportedException; import org.springframework.beans.TypeMismatchException; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.beans.factory.annotation.Value; import org.springframework.http.converter.HttpMessageNotReadableException; import org.springframework.http.converter.HttpMessageNotWritableException; import org.springframework.validation.BindException; import org.springframework.validation.BindingResult; import org.springframework.validation.FieldError; import org.springframework.validation.ObjectError; import org.springframework.web.HttpMediaTypeNotAcceptableException; import org.springframework.web.HttpMediaTypeNotSupportedException; import org.springframework.web.HttpRequestMethodNotSupportedException; import org.springframework.web.bind.MethodArgumentNotValidException; import org.springframework.web.bind.MissingPathVariableException; import org.springframework.web.bind.MissingServletRequestParameterException; import org.springframework.web.bind.ServletRequestBindingException; import org.springframework.web.bind.annotation.ExceptionHandler; import org.springframework.web.bind.annotation.RestControllerAdvice; import org.springframework.web.context.request.async.AsyncRequestTimeoutException; import org.springframework.web.multipart.support.MissingServletRequestPartException; /** * 全局异常处理器 */ @Slf4j @RestControllerAdvice public class UnifiedExceptionHandler { /** * 生产环境 */ private final static String ENV_PROD = \"prod\"; private UnifiedMessageSource unifiedMessageSource; @Autowired public UnifiedExceptionHandler(UnifiedMessageSource unifiedMessageSource) { this.unifiedMessageSource = unifiedMessageSource; } /** * 当前环境，默认dev环境 */ @Value(\"${spring.profiles.active:dev}\") private String profile; /** * 获取国际化消息 * * @param e 异常 * @return */ public String getMessage(BaseException e) { String code = \"response.\" + e.getResponseEnum().toString(); String message = unifiedMessageSource.getMessage(code, e.getArgs()); if (message == null || message.isEmpty()) { return e.getMessage(); } return message; } /** * 业务异常处理器 * * @param e 异常 * @return 异常结果 */ @ExceptionHandler(value = BusinessException.class) public ErrorResponse handleBusinessException(BaseException e) { log.error(e.getMessage(), e); return new ErrorResponse(e.getResponseEnum().getCode(), getMessage(e)); } /** * 自定义异常处理器 * * @param e 异常 * @return 异常结果 */ @ExceptionHandler(value = BaseException.class) public ErrorResponse handleBaseException(BaseException e) { log.error(e.getMessage(), e); return new ErrorResponse(e.getResponseEnum().getCode(), getMessage(e)); } /** * servlet容器异常处理 * * @param e 异常 * @return 异常结果 */ @ExceptionHandler({ //NoHandlerFoundException.class, //该异常由自定义控制器捕获处理 HttpRequestMethodNotSupportedException.class, HttpMediaTypeNotSupportedException.class, HttpMediaTypeNotAcceptableException.class, MissingPathVariableException.class, MissingServletRequestParameterException.class, TypeMismatchException.class, HttpMessageNotReadableException.class, HttpMessageNotWritableException.class, HttpMediaTypeNotAcceptableException.class, // BindException.class, // MethodA","date":"2021-02-01","objectID":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/springboot%E7%BB%9F%E4%B8%80%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/:2:4","tags":null,"title":"Springboot统一异常处理","uri":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/springboot%E7%BB%9F%E4%B8%80%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/"},{"categories":null,"content":"edge浏览器安装 JSON View插件后，有些json格式的字符串无法显示为json的易读形式 原因是 controller 返回类型是 String，所以http响应里的格式是text/html格式 @RequestMapping(value = \"/get/{id}\", method = RequestMethod.GET) @ResponseBody public String get(@PathVariable(\"id\") final int id) throws JsonProcessingException { User User = null; try{ User = redisUtil.getObject(key(id)); }catch(Exception e){ log.error(\"get from redis error, \", e); } return null==User ? (\"can not get User by id [\" + id + \"]\") : new ObjectMapper().writeValueAsString(User); } 解决： controller返回类型改为对象（User），spring会自动将对象转为json，或者指定响应类型 解决方案一 @RequestMapping(value = \"/get/{id}\", method = RequestMethod.GET) @ResponseBody() public User get(@PathVariable(\"id\") final int id) throws JsonProcessingException { User user = null; try{ user = redisUtil.getObject(key(id)); }catch(Exception e){ log.error(\"get from redis error, \", e); } //return null== user ? (\"can not get User by id [\" + id + \"]\") : new ObjectMapper().writeValueAsString(user); return user; } 解决方案二 @RequestMapping(value = \"/get/{id}\", method = RequestMethod.GET, produces = {\"application/json\"}) @ResponseBody() public String get(@PathVariable(\"id\") final int id) throws JsonProcessingException { User user = null; try{ user = redisUtil.getObject(key(id)); }catch(Exception e){ log.error(\"get from redis error, \", e); } return null== user ? (\"can not get User by id [\" + id + \"]\") : new ObjectMapper().writeValueAsString(user); } ","date":"2021-01-29","objectID":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/edge%E6%B5%8F%E8%A7%88%E5%99%A8%E7%9A%84jsonview%E6%8F%92%E4%BB%B6%E6%97%A0%E6%B3%95%E6%98%BE%E7%A4%BAjson/:0:0","tags":null,"title":"Edge浏览器的JSONView插件无法显示json","uri":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/edge%E6%B5%8F%E8%A7%88%E5%99%A8%E7%9A%84jsonview%E6%8F%92%E4%BB%B6%E6%97%A0%E6%B3%95%E6%98%BE%E7%A4%BAjson/"},{"categories":null,"content":"今天发现笔记本（T460）异常卡顿，动不动就停止响应了，查看任务管理器，磁盘占用一直处于100%，但是通过磁盘占用排序来看，一共只有0.6MB/秒的占用，以前没出现过这种卡顿的问题，所以猜测可能是某些软件导致的。 当关闭 edge 浏览器后，待任务管理器中edge浏览器没有进程运行了，磁盘占用立马就较少了，所以推测是edge的问题 然后通过排查插件，发现当把 Adblock Plus 插件关闭后，磁盘占用立马恢复正常了，系统也流畅了，最终锁定是 Adblock Plus 插件的问题。 将 Adblock Plus 插件删除后，然后重新安装，磁盘占用就恢复正常了。 ","date":"2021-01-29","objectID":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/win10%E7%A3%81%E7%9B%98%E8%AF%BB%E5%86%99100%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/:0:0","tags":null,"title":"Win10磁盘读写100%问题排查","uri":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/win10%E7%A3%81%E7%9B%98%E8%AF%BB%E5%86%99100%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"},{"categories":null,"content":"由springboot官方参考文档可知 Redis is a cache, message broker, and richly-featured key-value store. Spring Boot offers basic auto-configuration for the Lettuce and Jedis client libraries and the abstractions on top of them provided by Spring Data Redis. redis是一个缓存、消息代理、也是功能丰富的键值存储仓库。spring boot 为 Lettuce 和 Jedis 客户端库提供了基本的自动配置，并在 Lettuce 和 Jedis 客户端库之上由 spring data redis 提供了抽象。 There is a spring-boot-starter-data-redis “Starter” for collecting the dependencies in a convenient way. By default, it uses Lettuce. That starter handles both traditional and reactive applications. spring-boot-starter-data-redis 这个starter用于方便地收集redis的相关依赖项，默认情况使用 Lettuce 作为 redis客户端，这个starter可以处理传统应用和响应式应用。 You can inject an auto-configured RedisConnectionFactory, StringRedisTemplate, or vanilla RedisTemplate instance as you would any other Spring Bean. By default, the instance tries to connect to a Redis server at localhost:6379. The following listing shows an example of such a bean: 你可以注入一个已自动配置好的RedisConnectionFactory、StringRedisTemplate或 普通的 RedisTemplate 实例像注入其他spring bean 一样。默认情况下，该实例尝试连接到地址为 localhost:6379 的redis服务器，如下列表展示了注入这样的bean的例子： @Component public class MyBean { private StringRedisTemplate template; @Autowired public MyBean(StringRedisTemplate template) { this.template = template; } // ... } You can also register an arbitrary number of beans that implement LettuceClientConfigurationBuilderCustomizer for more advanced customizations. If you use Jedis, JedisClientConfigurationBuilderCustomizer is also available. 你也可以注册任意数量的实现了LettuceClientConfigurationBuilderCustomizer的bean，来获得高级的自定义配置。如果你使用的是Jedis，JedisClientConfigurationBuilderCustomizer也是可以使用的 If you add your own @Bean of any of the auto-configured types, it replaces the default (except in the case of RedisTemplate, when the exclusion is based on the bean name, redisTemplate, not its type). By default, if commons-pool2 is on the classpath, you get a pooled connection factory. 如果你添加自己的任意类型的自动配置的@Bean，它将替换默认的配置（当排除的是基于bean名称，而不是基于类型的redisTemplate的时候，RedisTemplate不会被替换）。默认情况下，如果类路径中存在 commons-pools2 依赖包，你可以得到一个池化的连接工厂。 springboot官方文档中对springboot如何集成redis的介绍不太详细。 github上由spring-projects官方的examples，地址：https://github.com/spring-projects/spring-data-examples.git 参考： https://blog.csdn.net/qq_41921994/article/details/109627736 Lettuce 和 Jedis 的都是连接Redis Server的客户端程序。Jedis在实现上是直连redis server，多线程环境下非线程安全（即多个线程对一个连接实例操作，是线程不安全的），除非使用连接池，为每个Jedis实例增加物理连接。Lettuce基于Netty的连接实例（StatefulRedisConnection），可以在多个线程间并发访问，且线程安全，满足多线程环境下的并发访问（即多个线程公用一个连接实例，线程安全），同时它是可伸缩的设计，一个连接实例不够的情况也可以按需增加连接实例。 lettuce依赖commons-pool2 ","date":"2021-01-28","objectID":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/springboot%E9%9B%86%E6%88%90redis/:0:0","tags":null,"title":"Springboot集成redis","uri":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/springboot%E9%9B%86%E6%88%90redis/"},{"categories":null,"content":"实践 ","date":"2021-01-28","objectID":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/springboot%E9%9B%86%E6%88%90redis/:1:0","tags":null,"title":"Springboot集成redis","uri":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/springboot%E9%9B%86%E6%88%90redis/"},{"categories":null,"content":"win10系统启动 redis-server 下载 redis server win10版本安装包，下载地址：https://github.com/MicrosoftArchive/redis/releases 比如下载 Redis-x64-3.0.504.zip，解压到某个目录下，双击 redis-server.exe 即可将 redis 启动。 若启动失败，显示如下错误 PS D:\\Develop\\Redis-x64-3.0.504\u003e .\\redis-server.exe [21368] 29 Jan 09:16:00.558 # Warning: no config file specified, using the default config. In order to specify a config file use D:\\Develop\\Redis-x64-3.0.504\\redis-server.exe /path/to/redis.conf [21368] 29 Jan 09:16:00.565 # Creating Server TCP listening socket *:6379: bind: No such file or directory 网上搜索的方法不好使，重启下电脑就可以了 ","date":"2021-01-28","objectID":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/springboot%E9%9B%86%E6%88%90redis/:1:1","tags":null,"title":"Springboot集成redis","uri":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/springboot%E9%9B%86%E6%88%90redis/"},{"categories":null,"content":"写代码 新建 simple-011-springboot-redis-demo 项目 src/main/java目录添加springboot启动类 package org.msdemt.simple.redis_demo; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; @SpringBootApplication public class RedisApplication { public static void main(String[] args) { SpringApplication.run(RedisApplication.class, args); } } 添加User类 package org.msdemt.simple.redis_demo.pojo; import lombok.AllArgsConstructor; import lombok.Data; import lombok.NoArgsConstructor; import java.io.Serializable; @Data @AllArgsConstructor @NoArgsConstructor public class User implements Serializable { private static final long serialVersionUID = 2489911387827654482L; private String name; private Integer age; private String address; } pom文件中添加如下依赖 \u003cdependencies\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-web\u003c/artifactId\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-data-redis\u003c/artifactId\u003e \u003c/dependency\u003e \u003c!--spring-boot-starter-data-redis 默认使用 Lettuce 作为 redis 客户端，Lettuce 需要依赖 commons-pool2--\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.apache.commons\u003c/groupId\u003e \u003cartifactId\u003ecommons-pool2\u003c/artifactId\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-test\u003c/artifactId\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.projectlombok\u003c/groupId\u003e \u003cartifactId\u003elombok\u003c/artifactId\u003e \u003c/dependency\u003e \u003c/dependencies\u003e 然后在 idea 右边栏，maven页面，reload一下，这样项目才能下载依赖包。 因为 spring-boot-starter-data-redis 默认连接 127.0.0.1:6379 的 redis server，为了方便，此时可以不修改 application.yml 文件 当然，如果想要显式地配置redis的参数，使得项目更清晰，可以新建 src/main/resources/application.yml 文件，并在application.yml文件中添加如下内容 spring:redis:host:127.0.0.1port:6379password:lettuce:#springboot默认使用lettuce作为redis客户端#在关闭客户端连接之前等待任务处理完成的最长时间，在这之后，无论任务是否执行完成，都会被执行器关闭，默认100msshutdown-timeout:100pool:# 连接池最大连接数（负值表示没有限制）max-active:8# 连接池中的最大空闲连接max-idle:8# 连接池中的最小空闲连接min-idle:0# 连接池最大阻塞等待时间（使用负值代表没有限制），单位msmax-wait:-1cache:redis:#是否缓存空值，默认为truecache-null-values:false 在src/test目录下新建测试源码目录（src/test/java）和测试资源目录（src/test/resources） src/test/java 目录下添加测试文件 package org.msdemt.simple.redis_demo.test; import com.fasterxml.jackson.core.JsonProcessingException; import com.fasterxml.jackson.databind.ObjectMapper; import org.junit.jupiter.api.Assertions; import org.junit.jupiter.api.Test; import org.msdemt.simple.redis_demo.pojo.User; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.boot.test.context.SpringBootTest; import org.springframework.data.redis.core.RedisTemplate; import org.springframework.data.redis.core.StringRedisTemplate; @SpringBootTest public class RedisTest { @Autowired private RedisTemplate redisTemplate; @Autowired private StringRedisTemplate stringRedisTemplate; @Test public void testRedisTemplateForSetStringValue(){ redisTemplate.opsForValue().set(\"hello\", \"world\"); stringRedisTemplate.opsForValue().set(\"key\", \"value\"); Assertions.assertEquals(\"world\", redisTemplate.opsForValue().get(\"hello\")); Assertions.assertEquals(\"value\", stringRedisTemplate.opsForValue().get(\"key\")); } @Test public void testRedisTemplateForSetObjectValue() throws JsonProcessingException { User user = new User(\"nike\", 30, \"beijing\"); redisTemplate.opsForValue().set(\"admin\", user); System.out.println(user.toString()); System.out.println(new ObjectMapper().writeValueAsString(user)); System.out.println((User)redisTemplate.opsForValue().get(\"admin\")); Assertions.assertEquals(user, redisTemplate.opsForValue().get(\"admin\")); } } 执行该测试包中的测试类时，是不会按照src/main/resources/application.yml中的配置进行的，如果想定义测试类的redis配置，需要在src/test/resources/application.yml文件中添加redis的配置 执行 testRedisTemplateForSetObjectValue 后，使用 redis desktop manager 查看 redis server 中的数据，发现 redisTemplate 存储的键值是乱码的，stringRedisTemplate 存储的键值是正常的，为什么呢？ StringRedisTemplate 是 RedisTemplate\u003cString, String\u003e 的子类 public class StringRedisTemplate extends RedisTemplate\u003cString, String\u003e { 通过debug，可以发现 RedisTemplate 的","date":"2021-01-28","objectID":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/springboot%E9%9B%86%E6%88%90redis/:1:2","tags":null,"title":"Springboot集成redis","uri":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/springboot%E9%9B%86%E6%88%90redis/"},{"categories":null,"content":"高性能序列化器 kyro 参考: https://www.cnblogs.com/hntyzgn/p/7122709.html https://blog.csdn.net/boling_cavalry/article/details/80710774 https://github.com/EsotericSoftware/kryo Kryo is a fast and efficient binary object graph serialization framework for Java. Kryo是用于Java的快速高效的二进制对象图序列化框架。 Kryo publishes two kinds of artifacts/jars: the default jar (with the usual library dependencies) which is meant for direct usage in applications (not libraries) a dependency-free, “versioned” jar which should be used by other libraries. Different libraries shall be able to use different major versions of Kryo. To use the latest Kryo release in your application, use this dependency entry in your pom.xml: \u003cdependency\u003e \u003cgroupId\u003ecom.esotericsoftware\u003c/groupId\u003e \u003cartifactId\u003ekryo\u003c/artifactId\u003e \u003cversion\u003e5.0.3\u003c/version\u003e \u003c/dependency\u003e To use the latest Kryo release in a library you want to publish, use this dependency entry in your pom.xml: \u003cdependency\u003e \u003cgroupId\u003ecom.esotericsoftware.kryo\u003c/groupId\u003e \u003cartifactId\u003ekryo5\u003c/artifactId\u003e \u003cversion\u003e5.0.3\u003c/version\u003e \u003c/dependency\u003e 2021-01-29 14:18:45.088 ERROR 10824 --- [nio-8080-exec-1] o.m.s.r.serializer.KryoRedisSerializer : Class is not registered: org.msdemt.simple.redis_kyro.pojo.User Note: To register this class use: kryo.register(org.msdemt.simple.redis_kyro.pojo.User.class); java.lang.IllegalArgumentException: Class is not registered: org.msdemt.simple.redis_kyro.pojo.User Note: To register this class use: kryo.register(org.msdemt.simple.redis_kyro.pojo.User.class); ","date":"2021-01-28","objectID":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/springboot%E9%9B%86%E6%88%90redis/:2:0","tags":null,"title":"Springboot集成redis","uri":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/springboot%E9%9B%86%E6%88%90redis/"},{"categories":null,"content":"vscode显示打开文件的完整路径 https://www.cnblogs.com/eternityz/p/12269985.html Preference-Settings-window.title “window.title”: “${activeEditorLong}${separator}${dirty}${activeEditorShort}${separator}${rootName}${separator}${appName}”, ","date":"2021-01-27","objectID":"/posts/vscode/vscode%E6%98%BE%E7%A4%BA%E6%89%93%E5%BC%80%E6%96%87%E4%BB%B6%E7%9A%84%E5%AE%8C%E6%95%B4%E8%B7%AF%E5%BE%84/:1:0","tags":null,"title":"Vscode显示打开文件的完整路径","uri":"/posts/vscode/vscode%E6%98%BE%E7%A4%BA%E6%89%93%E5%BC%80%E6%96%87%E4%BB%B6%E7%9A%84%E5%AE%8C%E6%95%B4%E8%B7%AF%E5%BE%84/"},{"categories":null,"content":"idea显示空格 点击菜单 File -\u003e Settings 在弹出的窗口点击 Editor -\u003e General -\u003e Appearance 把Show whitespaces 勾选上就行。 ","date":"2021-01-27","objectID":"/posts/idea/idea%E6%98%BE%E7%A4%BA%E7%A9%BA%E6%A0%BC/:1:0","tags":null,"title":"Idea显示空格","uri":"/posts/idea/idea%E6%98%BE%E7%A4%BA%E7%A9%BA%E6%A0%BC/"},{"categories":null,"content":"https://www.cnblogs.com/FraserYu/p/11796301.html 比如：https://docs.spring.io/spring-boot/docs/current/reference/htmlsingle/#using-boot-devtools \u003cdependencies\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-devtools\u003c/artifactId\u003e \u003coptional\u003etrue\u003c/optional\u003e \u003c/dependency\u003e \u003c/dependencies\u003e dependencies { developmentOnly(\"org.springframework.boot:spring-boot-devtools\") } Flagging the dependency as optional in Maven or using the developmentOnly configuration in Gradle (as shown above) prevents devtools from being transitively applied to other modules that use your project. ","date":"2021-01-27","objectID":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/maven_optional%E5%8F%82%E6%95%B0/:0:0","tags":null,"title":"Maven_optional参数","uri":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/maven_optional%E5%8F%82%E6%95%B0/"},{"categories":null,"content":"idea注释开头怎么才能不显示在行首 如下图，取消红框中的勾 idea自动去除未引用的import idea配置注释 ** * $description$ $param$ $return$ * @author $author$ * @date $date$ */ para groovy 脚本内容 groovyScript(\"if(\\\"${_1}\\\".length() == 2) {return '';} else {def result=''; def params=\\\"${_1}\\\".replaceAll('[\\\\\\\\[|\\\\\\\\]|\\\\\\\\s]', '').split(',').toList();for(i = 0; i \u003c params.size(); i++) {if(i==0){result+='\\\\n * @param ' + params[i] + ': '}else{result+='\\\\n' + ' * @param ' + params[i] + ': '}}; return result;}\", methodParameters()); return groovy脚本内容 groovyScript(\"def returnType = \\\"${_1}\\\"; if(returnType == 'void') {return '';} else { def result = '\\\\n * @return: ' + returnType; return result;}\", methodReturnType()); idea自动添加class注释 /** * @description: ${DESCRIPTION} * @author: ${USER} * @date: ${DATE} */ idea代码自动格式化 https://www.cnblogs.com/xiaojf/p/13098663.html idea鼠标悬浮显示提示 ","date":"2021-01-27","objectID":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/idea%E9%85%8D%E7%BD%AE%E8%87%AA%E5%8A%A8%E6%B3%A8%E9%87%8A/:0:0","tags":null,"title":"Idea配置自动注释","uri":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/idea%E9%85%8D%E7%BD%AE%E8%87%AA%E5%8A%A8%E6%B3%A8%E9%87%8A/"},{"categories":null,"content":" \u003cplugin\u003e \u003cgroupId\u003ecom.github.wvengen\u003c/groupId\u003e \u003cartifactId\u003eproguard-maven-plugin\u003c/artifactId\u003e \u003cversion\u003e2.3.1\u003c/version\u003e \u003cexecutions\u003e \u003cexecution\u003e \u003cphase\u003epackage\u003c/phase\u003e \u003cgoals\u003e \u003cgoal\u003eproguard\u003c/goal\u003e \u003c/goals\u003e \u003c/execution\u003e \u003c/executions\u003e \u003cconfiguration\u003e \u003cattach\u003etrue\u003c/attach\u003e \u003cobfuscate\u003etrue\u003c/obfuscate\u003e \u003cattachArtifactClassifier\u003epg\u003c/attachArtifactClassifier\u003e \u003coptions\u003e \u003coption\u003e-printmapping '${project.build.directory}/${project.artifactId}.log'\u003c/option\u003e \u003coption\u003e-ignorewarnings\u003c/option\u003e \u003coption\u003e-dontshrink\u003c/option\u003e \u003coption\u003e-dontoptimize\u003c/option\u003e \u003coption\u003e-dontusemixedcaseclassnames\u003c/option\u003e \u003coption\u003e-dontskipnonpubliclibraryclasses\u003c/option\u003e \u003coption\u003e-dontskipnonpubliclibraryclassmembers\u003c/option\u003e \u003coption\u003e-allowaccessmodification\u003c/option\u003e \u003coption\u003e-useuniqueclassmembernames\u003c/option\u003e \u003coption\u003e-keeppackagenames\u003c/option\u003e \u003coption\u003e-adaptclassstrings\u003c/option\u003e \u003coption\u003e-keepdirectories\u003c/option\u003e \u003coption\u003e-keepattributes Exceptions,InnerClasses,Signature,Deprecated,SourceFile,LineNumberTable,LocalVariable*Table,*Annotation*,Synthetic,EnclosingMethod \u003c/option\u003e \u003coption\u003e-keepnames interface **\u003c/option\u003e \u003c!-- This option will save all original methods parameters in files defined in -keep sections, otherwise all parameter names will be obfuscate. --\u003e \u003coption\u003e-keepparameternames\u003c/option\u003e \u003coption\u003e-keepclassmembers class * { @org.springframework.beans.factory.annotation.Autowired *; @org.springframework.beans.factory.annotation.Value *; @org.springframework.context.annotation.Bean *; @org.springframework.beans.factory.annotation.Qualifier *; @org.springframework.stereotype.Repository *; @org.springframework.data.repository.NoRepositoryBean *; } \u003c/option\u003e \u003coption\u003e-keepclassmembers enum * { *; }\u003c/option\u003e \u003coption\u003e-keep class * implements java.io.Serializable\u003c/option\u003e \u003coption\u003e-keep class com.example.ykp.App { *; }\u003c/option\u003e \u003coption\u003e-keepclassmembers public class * {void set*(***);*** get*();}\u003c/option\u003e \u003c/options\u003e \u003coutjar\u003e${project.build.finalName}-pg.jar\u003c/outjar\u003e \u003clibs\u003e \u003c!-- \u003clib\u003e${java.home}/jmods/java.base.jmod\u003c/lib\u003e --\u003e \u003clib\u003e${java.home}/lib/rt.jar\u003c/lib\u003e \u003c!-- \u003clib\u003e${java.home}/lib/jce.jar\u003c/lib\u003e --\u003e \u003c/libs\u003e \u003cinjar\u003eclasses\u003c/injar\u003e \u003coutputDirectory\u003e${project.build.directory}\u003c/outputDirectory\u003e \u003c/configuration\u003e \u003c/plugin\u003e 踩坑 [proguard] Note: duplicate definition of library class [jdk.internal.jimage.decompressor.CompressIndexes] [proguard] (https://www.guardsquare.com/proguard/manual/troubleshooting#duplicateclass) [proguard] Warning: com.taoyuanx.ca.ui.controller.ApiController: can't find referenced class javax.sql.rowset.serial.SerialException [proguard] Note: duplicate definition of library class [jdk.internal.jimage.decompressor.Decompressor] [proguard] Warning: com.taoyuanx.ca.ui.controller.ApiController: can't find referenced class javax.sql.rowset.serial.SerialExceptionNote: duplicate definition of library class [jdk.internal.jimage.decompressor.ResourceDecompressor$StringsProvider] [proguard] [proguard] Note: duplicate definition of library class [jdk.internal.jimage.decompressor.ResourceDecompressor] [proguard] Note: duplicate definition of library class [jdk.internal.jimage.decompressor.ResourceDecompressorFactory] [proguard] Note: duplicate definition of library class [jdk.internal.jimage.decompressor.ResourceDecompressorRepository] [proguard] Note: duplicate definition of library class [jdk.internal.jimage.decompressor.SignatureParser$ParseResult] [proguard] Note: duplicate definition of library class [jdk.internal.jimage.decompressor.SignatureParser] [proguard] Note: duplicate definition of library class [jdk.internal.jimage.decompressor.StringSharingDecompressor] [proguard] Note: duplicate definition of library class [jdk.internal.jimage.decompressor.StringSharingDecompressorFactory] [proguard] Note: duplicate definition of library class [jdk.internal.jimage.decompressor.ZipDecompressor] [proguard] Note: duplicate definition of library class [jdk.internal.jimage.decompressor.ZipDecompressorFactory] [proguard] Note: duplicate definition of library class [j","date":"2021-01-27","objectID":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/java%E4%BB%A3%E7%A0%81%E6%B7%B7%E6%B7%86%E5%B7%A5%E5%85%B7proguard%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/:0:0","tags":null,"title":"Java代码混淆工具proguard问题记录","uri":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/java%E4%BB%A3%E7%A0%81%E6%B7%B7%E6%B7%86%E5%B7%A5%E5%85%B7proguard%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/"},{"categories":null,"content":"https://blog.csdn.net/u014792301/article/details/107575799 https://github.com/redhat-developer/vscode-java/wiki/JDK-Requirements#java.configuration.runtimes 由于 Language Support for Java™ by Red Hat这个拓展更新到0.65.0，该扩展依赖eclipsejdt.ls服务器的，Eclipse平台决定将JDK11作为最低要求，所以该扩展不在支持jdk1.8了 解决方案： 在 settings.json 中添加 java.configuration.runtimes ，其中配置jdk1.8作为默认运行环境，同时依旧需要将 jdk11 作为 java home 如果安装了spring，还需要配置 spring-boot.ls.java.home，否则会出现 Error trying to find JVM: TypeError: Cannot read property 'isJdk' of null 电脑没有配置JAVA_HOME或不想使用电脑环境遍历里配置的JAVA_HOME，这种情况无法在vscode的终端中执行maven package等命令，若不希望在电脑环境遍历中配置JAVA_HOME，可以在settings.json中配置 \"terminal.integrated.env.windows\": { \"JAVA_HOME\": \"D:\\\\Develop\\\\Java\\\\jdk1.8.0_202\", \"PATH\": \"D:\\\\Develop\\\\Java\\\\jdk1.8.0_202;${env:PATH}\" }, 这样，vscode的终端就使用了jdk1.8.0_202，而不是电脑配置的jdk vscode执行maven package等命令用的也是jdk1.8.0_202 完整的java配置如下 \"terminal.integrated.env.windows\": { \"JAVA_HOME\": \"D:\\\\Develop\\\\Java\\\\jdk1.8.0_202\", \"PATH\": \"D:\\\\Develop\\\\Java\\\\jdk1.8.0_202;${env:PATH}\" }, \"terminal.integrated.env.osx\": { \"JAVA_HOME\": \"/usr/local/jdk/oraclejdk/jdk1.8.0_202.jdk/Contents/Home\", \"PATH\": \"/usr/local/jdk/oraclejdk/jdk1.8.0_202.jdk/Contents/Home:$PATH\" }, // java \"java.home\": \"D:\\\\Develop\\\\openjdk\\\\jdk-14.0.1\", \"spring-boot.ls.java.home\": \"D:\\\\Develop\\\\Java\\\\jdk1.8.0_202\", \"maven.executable.path\": \"D:\\\\Develop\\\\Maven\\\\apache-maven-3.6.3\\\\bin\\\\mvn.cmd\", \"java.configuration.maven.userSettings\": \"C:\\\\Users\\\\He\\\\.m2\\\\settings.xml\", \"java.project.importOnFirstTimeStartup\": \"automatic\", \"java.configuration.checkProjectSettingsExclusions\": false, \"java.refactor.renameFromFileExplorer\": \"preview\", \"java.configuration.runtimes\": [ { \"name\": \"JavaSE-1.8\", \"path\": \"D:\\\\Develop\\\\Java\\\\jdk1.8.0_202\", \"default\": true }, { \"name\": \"JavaSE-11\", \"path\": \"D:\\\\Develop\\\\Java\\\\jdk-11.0.5\" }, { \"name\": \"JavaSE-14\", \"path\": \"D:\\\\Develop\\\\openjdk\\\\jdk-14.0.1\" } ], \"java.templates.typeComment\": [ \"/**\", \" * ${type_name}\", \" * @author: ${user}\", \" */\" ], \"java.codeGeneration.generateComments\": true, \"java.implementationsCodeLens.enabled\": true, \"java.referencesCodeLens.enabled\": true, \"java.maven.downloadSources\": true, \"java.saveActions.organizeImports\": true, \"java.showBuildStatusOnStart.enabled\": true, \"java.completion.guessMethodArguments\": true, \"java.dependency.showMembers\": true, \"java.dependency.packagePresentation\": \"flat\", //lombok \"java.jdt.ls.vmargs\": \"-XX:+UseParallelGC -XX:GCTimeRatio=4 -XX:AdaptiveSizePolicyWeight=90 -Dsun.zip.disableMemoryMapping=true -Xmx1G -Xms100m -javaagent:\\\"c:\\\\Users\\\\He\\\\.vscode\\\\extensions\\\\gabrielbb.vscode-lombok-1.0.1\\\\server\\\\lombok.jar\\\"\", \"settingsSync.ignoredSettings\": [ \"spring-boot.ls.java.home\", \"java.jdt.ls.vmargs\", \"java.configuration.maven.userSettings\", ], ","date":"2021-01-27","objectID":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/vscode%E6%94%AF%E6%8C%81jdk8/:0:0","tags":null,"title":"Vscode支持jdk8","uri":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/vscode%E6%94%AF%E6%8C%81jdk8/"},{"categories":null,"content":"原因是当前执行的 class 不在 classpath 路径下，原因可能是环境变量 CLASSPATH 中没有当前路径 .，建议不用配置 CLASSPATH 找不到或无法加载主类： https://blog.csdn.net/ncc1995/article/details/84932759 测试 [root@c48-1 test]# cat Test.java import java.net.InetAddress; import java.net.UnknownHostException; public class Test { public static void main(String[] args)throws UnknownHostException { System.out.println(InetAddress.getLocalHost().getHostAddress()); } } [root@c48-1 test]# echo $CLASSPATH /usr/local/jdk1.7.0_80/lib/dt.jar:/usr/local/jdk1.7.0_80/lib/tools.jar [root@c48-1 test]# javac Test.java [root@c48-1 test]# java Test 错误: 找不到或无法加载主类 Test [root@c48-1 test]# vi /etc/profile [root@c48-1 test]# source /etc/profile [root@c48-1 test]# echo $CLASSPATH .:/usr/local/jdk1.7.0_80/lib/dt.jar:/usr/local/jdk1.7.0_80/lib/tools.jar [root@c48-1 test]# java Test 192.168.15.161 [root@c48-1 test]# vi /etc/profile [root@c48-1 test]# source /etc/profile [root@c48-1 test]# echo $CLASSPATH [root@c48-1 test]# java Test 192.168.15.161 ","date":"2021-01-27","objectID":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/%E6%89%BE%E4%B8%8D%E5%88%B0%E6%88%96%E6%97%A0%E6%B3%95%E5%8A%A0%E8%BD%BD%E4%B8%BB%E7%B1%BB/:0:0","tags":null,"title":"找不到或无法加载主类","uri":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/%E6%89%BE%E4%B8%8D%E5%88%B0%E6%88%96%E6%97%A0%E6%B3%95%E5%8A%A0%E8%BD%BD%E4%B8%BB%E7%B1%BB/"},{"categories":null,"content":"下载kubectl 和 helm 二进制文件 kubectl 二进制文件下载地址： https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.18.md#client-binaries 得到适用于win10的kubectl下载链接： https://dl.k8s.io/v1.18.12/kubernetes-client-windows-amd64.tar.gz helm 二进制文件下载地址 https://github.com/helm/helm/tags 得到适用于win10的helm下载链接： https://get.helm.sh/helm-v3.4.1-windows-amd64.zip 将下载后的 kubectl 和 helm 二进制文件解压到某个目录，如D:Develop/k8s/bin 将k8s集群中的/root/.kube/config文件拷贝到D:Develop/k8s/conf vscode配置kubernetes vscode安装kubernetes插件 在首选项 - 设置中，选择vs-kubernetes，点击在settings.json中编辑 编辑后的内容如下 \"vs-kubernetes\": { \"vs-kubernetes.namespace\": \"\", \"vs-kubernetes.kubectl-path\": \"d:\\\\Develop\\\\k8s\\\\bin\\\\kubectl.exe\", \"vs-kubernetes.helm-path\": \"d:\\\\Develop\\\\k8s\\\\bin\\\\helm.exe\", \"vs-kubernetes.draft-path\": \"\", \"vs-kubernetes.minikube-path\": \"\", \"vs-kubernetes.kubectlVersioning\": \"user-provided\", \"vs-kubernetes.outputFormat\": \"yaml\", \"vs-kubernetes.kubeconfig\": \"d:\\\\Develop\\\\k8s\\\\conf\\\\config\", \"vs-kubernetes.knownKubeconfigs\": [ \"d:\\\\Develop\\\\k8s\\\\conf\\\\config\" ], \"vs-kubernetes.autoCleanupOnDebugTerminate\": false, \"vs-kubernetes.nodejs-autodetect-remote-root\": true, \"vs-kubernetes.nodejs-remote-root\": \"\", \"vs-kubernetes.nodejs-debug-port\": 9229, \"checkForMinikubeUpgrade\": true, \"logsDisplay\": \"webview\", \"imageBuildTool\": \"Docker\", }, ","date":"2021-01-27","objectID":"/posts/vscode/vscode%E9%85%8D%E7%BD%AEkubernetes/:0:0","tags":null,"title":"Vscode配置kubernetes","uri":"/posts/vscode/vscode%E9%85%8D%E7%BD%AEkubernetes/"},{"categories":null,"content":" 参考： https://www.cnblogs.com/niceyoo/p/13270224.html https://docs.docker.com/engine/reference/commandline/dockerd/ https://docs.docker.com/engine/security/https/ docker 开启2375端口远程访问 编辑 /usr/lib/systemd/system/docker.service 找到 [Service] 节点，修改 ExecStart 属性，增加 -H tcp://0.0.0.0:2375 修改后 Service 节点如下 ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock -H tcp://0.0.0.0:2375 重新加载docker配置 systemctl daemon-reload systemctl restart docker 通过浏览器访问 2375 测试，格式为：http://ip:2375/version 如果无法访问，检查是否防火墙2375端口被禁止，如果是，使用如下命令开启防火墙2375端口 firewall-cmd --zone=public --add-port=2375/tcp --permanent firewall-cmd --reload 如果还是不能访问，如果使用的是云主机，需要到服务器安全组规则查看是否开放了2375端口，如未配置，增加端口配置即可 IDEA连接docker idea 安装 docker 插件 配置docker tcp socket 为 tcp://ip:2375 vscode配置docker vscode 安装 docker 插件 在 settings.json 中添加 \"docker.host\": \"http://192.168.15.7:2375\", docker 远程端口2375和2376端口的区别： It is conventional to use port 2375 for un-encrypted, and port 2376 for encrypted communication with the daemon. 参考：https://docs.docker.com/engine/reference/commandline/dockerd/ ","date":"2021-01-27","objectID":"/posts/k8s/%E9%85%8D%E7%BD%AEdocker%E8%BF%9C%E7%A8%8B%E8%AE%BF%E9%97%AE/:0:0","tags":null,"title":"配置docker远程访问","uri":"/posts/k8s/%E9%85%8D%E7%BD%AEdocker%E8%BF%9C%E7%A8%8B%E8%AE%BF%E9%97%AE/"},{"categories":null,"content":"helm chart 官网 sonarqube chart 地址 https://github.com/helm/charts/tree/master/stable/sonarqube 该chart已经废弃，新的地址如下 https://github.com/Oteemo/charts/tree/master/charts/sonarqube 添加 chart 仓库 helm repo add oteemocharts https://oteemo.github.io/charts 搜索chart helm search repo sonar 下载chart helm fetch oteemocharts/sonarqube 根据chart生成yaml部署文件（安装中文语言包插件，中文语言包：https://github.com/xuhuisheng/sonar-l10n-zh） helm template sonar-community oteemocharts/sonarqube --set service.type=NodePort --set plugins.install[0]=\"https://github.com/xuhuisheng/sonar-l10n-zh/releases/download/sonar-l10n-zh-plugin-8.5/sonar-l10n-zh-plugin-8.5.jar\" \u003e sonar-community.yaml 注意：plugins.install 的参数是数组 部署 kubectl create ns sonar-community kubectl create -f sonar-community1.yaml -n sonar-community ","date":"2021-01-27","objectID":"/posts/k8s/%E4%BD%BF%E7%94%A8helm%E5%AE%89%E8%A3%85sonarqube/:0:0","tags":null,"title":"使用helm安装sonarqube","uri":"/posts/k8s/%E4%BD%BF%E7%94%A8helm%E5%AE%89%E8%A3%85sonarqube/"},{"categories":null,"content":" 实例变量：类中声明的，且在类的其他成员方法之外的非静态的变量；有默认值 示例 public class Person { private String name; //实例变量 private int age; //实例变量 private String address; //实例变量 @Override public String toString() { return \"Person{\" + \"name='\" + name + '\\'' + \", age=\" + age + \", address='\" + address + '\\'' + '}'; } } 类变量：类中声明的，且在类的其他成员方法之外的静态的变量；有默认值 示例： public class Person { private String name; //实例变量 private int age; //实例变量 private String address; //实例变量 private static String country; //类变量 @Override public String toString() { return \"Person{\" + \"name='\" + name + '\\'' + \", age=\" + age + \", address='\" + address + '\\'' + '}'; } } 常量：在程序运行中保持不变的量 示例： public class HelloWorld { // 静态成员常量 public static final double PI = 3.14; // 声明成员常量 final int y = 10; public static void main(String[] args) { // 声明局部常量 final double x = 3.3; } } 全局变量，又叫成员变量，包括实例变量、类变量、成员常量 局部变量，是指在方法内定义的变量，没有初始值，需要手工赋值后使用。 ","date":"2021-01-27","objectID":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/java%E4%B8%AD%E7%9A%84%E5%8F%98%E9%87%8F/:0:0","tags":null,"title":"Java中的变量","uri":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/java%E4%B8%AD%E7%9A%84%E5%8F%98%E9%87%8F/"},{"categories":null,"content":"lastUpdated文件 原因：依赖没有下载完整，查看 maven 本地仓库，发现依赖为：spring-boot-starter-parent-2.4.0.pom.lastUpdated 解决：删除该 lastUpdated 文件，然后重新下载依赖 ","date":"2021-01-27","objectID":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/maven-jar%E5%8C%85%E4%B8%8B%E8%BD%BD%E4%B8%8D%E5%AE%8C%E6%95%B4%E5%87%BA%E7%8E%B0lastupdated%E7%BB%93%E5%B0%BE%E6%96%87%E4%BB%B6/:1:0","tags":null,"title":"Maven-jar包下载不完整，出现lastUpdated结尾文件","uri":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/maven-jar%E5%8C%85%E4%B8%8B%E8%BD%BD%E4%B8%8D%E5%AE%8C%E6%95%B4%E5%87%BA%E7%8E%B0lastupdated%E7%BB%93%E5%B0%BE%E6%96%87%E4%BB%B6/"},{"categories":null,"content":"win10批量删除lastUpdated文件 在 CMD 窗口中打开 maven 本地仓库目录，如 D:\\Develop\\mvnResp 执行如下命令 D:\\Develop\\mvnRespo\u003e for /r %i in (*.lastUpdated) do del %i ","date":"2021-01-27","objectID":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/maven-jar%E5%8C%85%E4%B8%8B%E8%BD%BD%E4%B8%8D%E5%AE%8C%E6%95%B4%E5%87%BA%E7%8E%B0lastupdated%E7%BB%93%E5%B0%BE%E6%96%87%E4%BB%B6/:2:0","tags":null,"title":"Maven-jar包下载不完整，出现lastUpdated结尾文件","uri":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/maven-jar%E5%8C%85%E4%B8%8B%E8%BD%BD%E4%B8%8D%E5%AE%8C%E6%95%B4%E5%87%BA%E7%8E%B0lastupdated%E7%BB%93%E5%B0%BE%E6%96%87%E4%BB%B6/"},{"categories":null,"content":"idea配置数据库连接时，提示如下错误 Server returns invalid timezone. Need to set 'serverTimezone' property. 点击 set timezone 默认的 serverTimezone 为 UTC utc 为协调世界时，又称世界统一时间、世界标准时间，不属于任何时区，这套时间系统被应用于许多互联网和万维网的标准中，例如，网络时间协议就是协调世界时在互联网中使用的一种方式。 中国大陆、中国香港、中国澳门、中国台湾、蒙古国、新加坡、马来西亚、菲律宾、西澳大利亚州的时间与UTC的时差均为+8，也就是UTC+8。 但是，配置成 UTC+8 会报错： [S1009] No timezone mapping entry for 'UTC+8' com.mysql.cj.exceptions.WrongArgumentException: No timezone mapping entry for 'UTC+8' 查看mysql文档中对时区的支持：https://dev.mysql.com/doc/refman/8.0/en/time-zone-support.html 查看到mysql支持的timezone是根据操作系统判断的 [root@vultrguest ~]# ls /usr/share/zoneinfo/ Africa Australia Cuba Etc GMT-0 Indian Kwajalein MST7MDT Portugal ROC Universal zone.tab America Brazil EET Europe GMT0 Iran leapseconds Navajo posix ROK US Zulu Antarctica Canada Egypt GB Greenwich iso3166.tab Libya NZ posixrules Singapore UTC Arctic CET Eire GB-Eire Hongkong Israel MET NZ-CHAT PRC Turkey WET Asia Chile EST GMT HST Jamaica Mexico Pacific PST8PDT tzdata.zi W-SU Atlantic CST6CDT EST5EDT GMT+0 Iceland Japan MST Poland right UCT zone1970.tab [root@vultrguest ~]# ls /usr/share/zoneinfo/Asia/ Aden Bahrain Chongqing Gaza Jerusalem Kuala_Lumpur Novokuznetsk Rangoon Tashkent Ulan_Bator Almaty Baku Chungking Harbin Kabul Kuching Novosibirsk Riyadh Tbilisi Urumqi Amman Bangkok Colombo Hebron Kamchatka Kuwait Omsk Saigon Tehran Ust-Nera Anadyr Barnaul Dacca Ho_Chi_Minh Karachi Macao Oral Sakhalin Tel_Aviv Vientiane Aqtau Beirut Damascus Hong_Kong Kashgar Macau Phnom_Penh Samarkand Thimbu Vladivostok Aqtobe Bishkek Dhaka Hovd Kathmandu Magadan Pontianak Seoul Thimphu Yakutsk Ashgabat Brunei Dili Irkutsk Katmandu Makassar Pyongyang Shanghai Tokyo Yangon Ashkhabad Calcutta Dubai Istanbul Khandyga Manila Qatar Singapore Tomsk Yekaterinburg Atyrau Chita Dushanbe Jakarta Kolkata Muscat Qostanay Srednekolymsk Ujung_Pandang Yerevan Baghdad Choibalsan Famagusta Jayapura Krasnoyarsk Nicosia Qyzylorda Taipei Ulaanbaatar [root@vultrguest ~]# 使用Aisa/Shanghai就可以了 扩展： jdbc中的url里也有时区配置，示例如下 url: jdbc:mysql://127.0.0.1:3306/simple_demo?useSSL=false\u0026useUnicode=true\u0026characterEncoding=utf8\u0026serverTimezone=Asia/Shanghai 此处serverTimezone也是指的mysql时区，也可以写为GMT+8，即 url: jdbc:mysql://127.0.0.1:3306/simple_demo?useSSL=false\u0026useUnicode=true\u0026characterEncoding=utf8\u0026serverTimezone=GMT%2B8 GMT时间就是英国格林威治时间，也就是世界标准时间，是本初子午线上的地方时，是0时区的区时，与我国的标准时间北京时间（东八区）相差8小时，即晚8小时，故可以使用GMT+8表示北京时间。 问题来了，Aisa/Shanghai 和 GMT+8 的时间相同吗？ 参考：https://www.zhihu.com/question/270709626/answer/357054228 public static void main(String[] args) throws ParseException { SimpleDateFormat sdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\"); sdf.setTimeZone(TimeZone.getTimeZone(\"Asia/Shanghai\")); Date date = sdf.parse(\"1988-08-13 13:13:13\"); TimeZone.setDefault(TimeZone.getTimeZone(\"Asia/Shanghai\")); System.out.println(\"date = \" + date.toString()); //date = Sat Aug 13 13:13:13 CDT 1988 System.out.println(\"date = \" + date.getTime()); //date = 587448793000 sdf.setTimeZone(TimeZone.getTimeZone(\"GMT+8\")); date = sdf.parse(\"1988-08-13 13:13:13\"); TimeZone.setDefault(TimeZone.getTimeZone(\"GMT+8\")); System.out.println(\"date = \" + date.toString()); //date = Sat Aug 13 13:13:13 GMT+08:00 1988 System.out.println(\"date = \" + date.getTime()); //date = 587452393000 /* 时间戳相差一个小时 587452393000 - 587448793000 = 3600000 毫秒 = 1 小时 中国曾经实行过夏令时，使用Asia/Shanghai时会处理夏令时。使用GMT+8时不会处理夏令时，因为GMT+8不能确定是哪个国家 1992年以后就没有这个问题了 */ } 所以，可以认为 Aisa/Shanghai 和 GMT+8 的时间是相同的。 ","date":"2021-01-27","objectID":"/posts/idea/idea%E9%85%8D%E7%BD%AE%E6%95%B0%E6%8D%AE%E5%BA%93%E8%BF%9E%E6%8E%A5/:0:0","tags":null,"title":"Idea配置数据库连接","uri":"/posts/idea/idea%E9%85%8D%E7%BD%AE%E6%95%B0%E6%8D%AE%E5%BA%93%E8%BF%9E%E6%8E%A5/"},{"categories":null,"content":"使用java8 新特性 stream 实现 list 的并集、差集、交集 public static void main(String[] args) { List\u003cString\u003e list1 = new ArrayList\u003c\u003e(); List\u003cString\u003e list2 = new ArrayList\u003c\u003e(); list1.add(\"0.00\"); list1.add(\"0.03\"); list1.add(\"0.05\"); list1.add(\"0.06\"); list1.add(\"0.09\"); list2.add(\"0.00\"); list2.add(\"0.03\"); list2.add(\"0.05\"); list2.add(\"0.06\"); list2.add(\"0.08\"); System.out.println(\"---交集---\"); List list3 = list1.stream().filter(t -\u003e list2.contains(t)).collect(Collectors.toList()); list3.stream().forEach(System.out::println); System.out.println(\"---差集---\"); List list4 = new ArrayList(); if(list1.size() \u003e list2.size()){ list4 = list1.stream().filter(t -\u003e !list2.contains(t)).collect(Collectors.toList()); }else if(list1.size() \u003c list2.size()){ list4 = list2.stream().filter(t -\u003e !list1.contains(t)).collect(Collectors.toList()); }else{ List list41 = list1.stream().filter(t -\u003e !list2.contains(t)).collect(Collectors.toList()); List list42 = list2.stream().filter(t -\u003e !list1.contains(t)).collect(Collectors.toList()); list4.addAll(list41); list4.addAll(list42); } list4.stream().forEach(System.out::println); System.out.println(\"---并集---\"); List list5 = new ArrayList(); list5.addAll(list1); list5.addAll(list2); List list6 = (List) list5.stream().distinct().collect(Collectors.toList()); list6.stream().forEach(System.out::println); } 输出结果： ---交集--- 0.00 0.03 0.05 0.06 ---差集--- 0.09 0.08 ---并集--- 0.00 0.03 0.05 0.06 0.09 0.08 ","date":"2021-01-26","objectID":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/java8%E5%AE%9E%E7%8E%B0%E9%9B%86%E5%90%88%E7%9A%84%E5%B9%B6%E9%9B%86%E5%B7%AE%E9%9B%86%E4%BA%A4%E9%9B%86/:0:0","tags":null,"title":"Java8实现集合的并集、差集、交集","uri":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/java8%E5%AE%9E%E7%8E%B0%E9%9B%86%E5%90%88%E7%9A%84%E5%B9%B6%E9%9B%86%E5%B7%AE%E9%9B%86%E4%BA%A4%E9%9B%86/"},{"categories":null,"content":"想将swagger集成到我的项目里了，项目比较老了，是使用的servlet和webservice暴漏的服务 pom.xml中加入swagger依赖后，启动报错， Caused by: java.lang.NoClassDefFoundError: com/fasterxml/jackson/databind/ObjectMapper swagger依赖jackson，所以需要添加jackson依赖 加上jackson依赖后，报错如下 org.springframework.context.ApplicationContextException: Failed to start bean 'documentationPluginsBootstrapper'; nested exception is java.lang.NoClassDefFoundError: org/springframework/http/codec/multipart/FilePart 因为当前项目使用的spring版本是4.3.30.RELEASE，但是org/springframework/http/codec/multipart/FilePart是在spring5.0开始加的，所以需要使用低版本的swagger。 swagger集成好了之后，发现webservice无法使用swagger，虽然servlet可以使用，但是配置比较复杂，不友好，所以放弃了 servlet配置swagger可以参考官方demo：https://github.com/swagger-api/swagger-samples/tree/master/java/java-servlet Swagger 是一个规范和完整的框架，用于生成、描述、调用和可视化 RESTful 风格的 Web 服务。 适用于restful风格的web服务，不适用于servlet和webservice服务。 ","date":"2021-01-25","objectID":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/swagger%E7%9A%84%E4%BD%BF%E7%94%A8/:0:0","tags":null,"title":"Swagger的使用","uri":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/swagger%E7%9A%84%E4%BD%BF%E7%94%A8/"},{"categories":null,"content":"公司里很多项目都使用了dubbox，并且用其暴漏rest服务，查看文档了解下，地址：http://dangdangdotcom.github.io/dubbox/rest.html ","date":"2021-01-25","objectID":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/dubbox%E6%9A%B4%E6%BC%8Frest%E6%9C%8D%E5%8A%A1/:0:0","tags":null,"title":"Dubbox暴漏rest服务","uri":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/dubbox%E6%9A%B4%E6%BC%8Frest%E6%9C%8D%E5%8A%A1/"},{"categories":null,"content":"idea无法引用某个本地的类，如下所示 解决： File - Invalidate and ","date":"2021-01-25","objectID":"/posts/idea/idea%E6%97%A0%E6%B3%95%E5%AF%BC%E5%85%A5%E6%9F%90%E4%B8%AA%E7%B1%BB/:0:0","tags":null,"title":"Idea无法导入某个类","uri":"/posts/idea/idea%E6%97%A0%E6%B3%95%E5%AF%BC%E5%85%A5%E6%9F%90%E4%B8%AA%E7%B1%BB/"},{"categories":null,"content":"运行springboot 测试类时，启动失败了，错误信息如下 java.lang.IllegalStateException: Unable to find a @SpringBootConfiguration, you need to use @ContextConfiguration or @SpringBootTest(classes=...) with your test at org.springframework.util.Assert.state(Assert.java:76) at org.springframework.boot.test.context.SpringBootTestContextBootstrapper.getOrFindConfigurationClasses(SpringBootTestContextBootstrapper.java:236) at org.springframework.boot.test.context.SpringBootTestContextBootstrapper.processMergedContextConfiguration(SpringBootTestContextBootstrapper.java:152) at org.springframework.test.context.support.AbstractTestContextBootstrapper.buildMergedContextConfiguration(AbstractTestContextBootstrapper.java:392) at org.springframework.test.context.support.AbstractTestContextBootstrapper.buildDefaultMergedContextConfiguration(AbstractTestContextBootstrapper.java:309) at org.springframework.test.context.support.AbstractTestContextBootstrapper.buildMergedContextConfiguration(AbstractTestContextBootstrapper.java:262) at org.springframework.test.context.support.AbstractTestContextBootstrapper.buildTestContext(AbstractTestContextBootstrapper.java:107) at org.springframework.boot.test.context.SpringBootTestContextBootstrapper.buildTestContext(SpringBootTestContextBootstrapper.java:102) at org.springframework.test.context.TestContextManager.\u003cinit\u003e(TestContextManager.java:137) at org.springframework.test.context.TestContextManager.\u003cinit\u003e(TestContextManager.java:122) at org.junit.jupiter.engine.execution.ExtensionValuesStore.lambda$getOrComputeIfAbsent$4(ExtensionValuesStore.java:86) at org.junit.jupiter.engine.execution.ExtensionValuesStore$MemoizingSupplier.get(ExtensionValuesStore.java:205) at org.junit.jupiter.engine.execution.ExtensionValuesStore$StoredValue.evaluate(ExtensionValuesStore.java:182) at org.junit.jupiter.engine.execution.ExtensionValuesStore$StoredValue.access$100(ExtensionValuesStore.java:171) at org.junit.jupiter.engine.execution.ExtensionValuesStore.getOrComputeIfAbsent(ExtensionValuesStore.java:89) at org.junit.jupiter.engine.execution.ExtensionValuesStore.getOrComputeIfAbsent(ExtensionValuesStore.java:93) at org.junit.jupiter.engine.execution.NamespaceAwareStore.getOrComputeIfAbsent(NamespaceAwareStore.java:61) at org.springframework.test.context.junit.jupiter.SpringExtension.getTestContextManager(SpringExtension.java:294) at org.springframework.test.context.junit.jupiter.SpringExtension.beforeAll(SpringExtension.java:113) at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$invokeBeforeAllCallbacks$8(ClassBasedTestDescriptor.java:368) at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73) at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.invokeBeforeAllCallbacks(ClassBasedTestDescriptor.java:368) at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.before(ClassBasedTestDescriptor.java:192) at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.before(ClassBasedTestDescriptor.java:78) at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:136) at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73) at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129) at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137) at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127) at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73) at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126) at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84) at java.util.ArrayList.forEach(ArrayList.java:1257) at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorServ","date":"2021-01-25","objectID":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/springboot%E6%B5%8B%E8%AF%95%E7%B1%BB%E5%90%AF%E5%8A%A8%E5%A4%B1%E8%B4%A5/:0:0","tags":null,"title":"Springboot测试类启动失败","uri":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/springboot%E6%B5%8B%E8%AF%95%E7%B1%BB%E5%90%AF%E5%8A%A8%E5%A4%B1%E8%B4%A5/"},{"categories":null,"content":"maven多模块项目，如 项目A是根项目，项目B是项目A的子模块，项目C是项目B的子模块，简单示例如下 project A module B module C 项目A中pom.xml配置了dependencyManagement，如下 \u003cdependencyManagement\u003e \u003cdependencies\u003e \u003c!-- SpringBoot的依赖配置--\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-dependencies\u003c/artifactId\u003e \u003cversion\u003e${spring-boot.version}\u003c/version\u003e \u003ctype\u003epom\u003c/type\u003e \u003cscope\u003eimport\u003c/scope\u003e \u003c/dependency\u003e \u003c/dependencies\u003e \u003c/dependencyManagement\u003e 项目B中pom.xml也配置了dependencyManagement，如下 \u003cdependencyManagement\u003e \u003cdependencies\u003e \u003c!-- Apache Dubbo --\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.apache.dubbo\u003c/groupId\u003e \u003cartifactId\u003edubbo-dependencies-bom\u003c/artifactId\u003e \u003cversion\u003e${dubbo.version}\u003c/version\u003e \u003ctype\u003epom\u003c/type\u003e \u003cscope\u003eimport\u003c/scope\u003e \u003c/dependency\u003e \u003c!-- Dubbo Spring Boot Starter --\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.apache.dubbo\u003c/groupId\u003e \u003cartifactId\u003edubbo-spring-boot-starter\u003c/artifactId\u003e \u003cversion\u003e${dubbo.version}\u003c/version\u003e \u003cexclusions\u003e \u003cexclusion\u003e \u003cgroupId\u003eorg.springframework\u003c/groupId\u003e \u003cartifactId\u003espring-context\u003c/artifactId\u003e \u003c/exclusion\u003e \u003c/exclusions\u003e \u003c/dependency\u003e \u003c!-- Zookeeper dependencies --\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.apache.dubbo\u003c/groupId\u003e \u003cartifactId\u003edubbo-dependencies-zookeeper\u003c/artifactId\u003e \u003cversion\u003e${dubbo.version}\u003c/version\u003e \u003ctype\u003epom\u003c/type\u003e \u003cexclusions\u003e \u003cexclusion\u003e \u003cgroupId\u003eorg.slf4j\u003c/groupId\u003e \u003cartifactId\u003eslf4j-log4j12\u003c/artifactId\u003e \u003c/exclusion\u003e \u003c/exclusions\u003e \u003c/dependency\u003e \u003c/dependencies\u003e \u003c/dependencyManagement\u003e 项目C的pom.xml没有配置dependencyManagement，直接配置了dependencies，如下 \u003cdependencies\u003e \u003c!-- Spring Boot dependencies--\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-web\u003c/artifactId\u003e \u003c/dependency\u003e \u003c/dependencies\u003e 此时启动项目C，会失败，错误信息如下： Exception in thread \"main\" java.lang.NoClassDefFoundError: org/springframework/core/metrics/ApplicationStartup at org.springframework.boot.SpringApplication.\u003cinit\u003e(SpringApplication.java:251) at org.springframework.boot.SpringApplication.\u003cinit\u003e(SpringApplication.java:264) at org.springframework.boot.SpringApplication.run(SpringApplication.java:1311) at org.springframework.boot.SpringApplication.run(SpringApplication.java:1300) at com.example.simple.dubbo_demo_provider.ProviderApplication.main(ProviderApplication.java:10) Caused by: java.lang.ClassNotFoundException: org.springframework.core.metrics.ApplicationStartup at java.net.URLClassLoader.findClass(URLClassLoader.java:382) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ... 5 more 由此现象推断：在pom.xml中，dependencyManagement是会覆盖的，假设有多层父子module，则某项目父级的dependencyManagement会覆盖祖父级的dependencyManagement，是全部覆盖，祖父pom中dependencyManagement定义的元素对其孙module是不可见的。 解决项目C启动失败问题： 方案1：将项目B的denpendencyManagement内容移动到项目A中 方案2：在项目B的dependencyManagement中添加spring-boot-dependencies 所以，创建多层级module项目时，需要注意dependencyManagement的覆盖问题。查看网上的项目，多层级module项目，一般都是在最高级声明dependencyManagement，子项目里不写dependencyManagement的。 如果在祖父项目的子项目里写上了空的dependencyManagement，和不写的效果时一样的，不会导致覆盖祖父项目的dependencyManagement。 \u003cdependencyManagement\u003e \u003cdependencies\u003e \u003c/dependencies\u003e \u003c/dependencyManagement\u003e 当然，最好还是不要在祖父项目的子项目写dependencyManagement 如果想在同个项目里使用多个dubbo版本，可以在祖父项目的子项目里定义dependencyManagement，不同的子项目使用不同的dubbo版本，注意，由于子项目的dependencyManagement覆盖了祖父项目的dependencyMangement，需要把祖父项目里dependencyMangement中的关键依赖同时添加到子项目的dependencyManagement中。 如果是project-dependencies-dependency元素呢？ 祖父项目A添加依赖 \u003cdependencies\u003e \u003cdependency\u003e \u003cgroupId\u003ecn.hutool\u003c/groupId\u003e \u003cartifactId\u003ehutool-all\u003c/artifactId\u003e \u003cversion\u003e4.5.7\u003c/version\u003e \u003c/dependency\u003e \u003c/dependencies\u003e 祖父项目的子项目B，添加依赖如下 \u003cdependencies\u003e \u003cdependency\u003e \u003cgroupId\u003ecom.alibaba\u003c/groupId\u003e \u003cartifactId\u003efastjson\u003c/artifactId\u003e \u003c/dependency\u003e \u003c/dependencies\u003e 查看祖父项目的孙项目C，继承了祖父项目的依赖和父项目的依赖，孙项目里同时依赖了hutool和fastjson ","date":"2021-01-25","objectID":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/maven_dependencymanagement%E5%A4%9A%E6%A8%A1%E5%9D%97%E8%A6%86%E7%9B%96%E9%97%AE%E9%A2%98/:0:0","tags":null,"title":"Maven_dependencyManagement多模块覆盖问题","uri":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/maven_dependencymanagement%E5%A4%9A%E6%A8%A1%E5%9D%97%E8%A6%86%E7%9B%96%E9%97%AE%E9%A2%98/"},{"categories":null,"content":" 参考： https://maven.apache.org/guides/introduction/introduction-to-dependency-mechanism.html#dependency-scope Dependency scope is used to limit the transitivity of a dependency and to determine when a dependency is included in a classpath. dependency scope 参数用于限制依赖关系的传递性，并用于确定是否将一个依赖包含在classpath中。 There are 6 scopes: scope参数共有6个可选值： compile This is the default scope, used if none is specified. Compile dependencies are available in all classpaths of a project. Furthermore, those dependencies are propagated to dependent projects. compile是scope属性的默认值。scope类型的dependency在项目的所有类路径中都可以使用。此外，compile类型的依赖是可以传递的（比如项目A有scope为compile类型的依赖fastjson，若项目B依赖项目A，那么项目B也依赖了fastjson，项目B的classpath也会有fastjson） 举例： \u003cdependency\u003e \u003cgroupId\u003ecom.alibaba\u003c/groupId\u003e \u003cartifactId\u003efastjson\u003c/artifactId\u003e \u003cversion\u003e1.2.70\u003c/version\u003e \u003c/dependency\u003e provided This is much like compile, but indicates you expect the JDK or a container to provide the dependency at runtime. For example, when building a web application for the Java Enterprise Edition, you would set the dependency on the Servlet API and related Java EE APIs to scope provided because the web container provides those classes. A dependency with this scope is added to the classpath used for compilation and test, but not the runtime classpath. It is not transitive. scope属性值为provided时很像compile，但是，若依赖scope 为 provided，说明你期望该依赖在项目可以在项目运行时由JDK或者容器运行时（tomcat等）提供。简言之，scope属性值为provided的依赖会用于编译和测试，不会包含在运行时的类路径中，即打包时不会包含该依赖，该依赖有servlet容器或jdk提供。并且，该依赖是不可传递的。 举例： \u003cdependency\u003e \u003cgroupId\u003ejavax.servlet\u003c/groupId\u003e \u003cartifactId\u003ejavax.servlet-api\u003c/artifactId\u003e \u003cversion\u003e3.1.0\u003c/version\u003e \u003cscope\u003eprovided\u003c/scope\u003e \u003c/dependency\u003e runtime This scope indicates that the dependency is not required for compilation, but is for execution. Maven includes a dependency with this scope in the runtime and test classpaths, but not the compile classpath. scope属性值为runtime表示该依赖不是编译必须的，但是运行时必须的。maven在运行时和测试时的类路径中包含该依赖，在编译类路径中不包含该依赖。 举例： \u003cdependency\u003e \u003cgroupId\u003emysql\u003c/groupId\u003e \u003cartifactId\u003emysql-connector-java\u003c/artifactId\u003e \u003cversion\u003e8.0.23\u003c/version\u003e \u003cscope\u003eruntime\u003c/scope\u003e \u003c/dependency\u003e test This scope indicates that the dependency is not required for normal use of the application, and is only available for the test compilation and execution phases. This scope is not transitive. Typically this scope is used for test libraries such as JUnit and Mockito. It is also used for non-test libraries such as Apache Commons IO if those libraries are used in unit tests (src/test/java) but not in the model code (src/main/java). scope属性值为test表示该依赖对于应用的正常运行不是必须的，并且仅在测试的编译和执行阶段使用。该属性的依赖是不可传递的。scope值为test的依赖仅可用于src/test/java包中。 举例： \u003cdependency\u003e \u003cgroupId\u003ejunit\u003c/groupId\u003e \u003cartifactId\u003ejunit\u003c/artifactId\u003e \u003cversion\u003e4.12\u003c/version\u003e \u003cscope\u003etest\u003c/scope\u003e \u003c/dependency\u003e system This scope is similar to provided except that you have to provide the JAR which contains it explicitly. The artifact is always available and is not looked up in a repository. scope属性值为system的依赖与值为provided的规则相似，除了必须显示提供对应的jar包。该依赖是引用自本地的依赖jar包，并且不会去依赖仓库中查找。 举例： \u003cdependency\u003e \u003cgroupId\u003ecom.jacob\u003c/groupId\u003e \u003cartifactId\u003ejacob\u003c/artifactId\u003e \u003cversion\u003e1.0\u003c/version\u003e \u003cscope\u003esystem\u003c/scope\u003e \u003csystemPath\u003e${project.basedir}/src/main/webapp/WEB-INF/lib/jacob.jar\u003c/systemPath\u003e \u003c/dependency\u003e import This scope is only supported on a dependency of type pom in the section. It indicates the dependency is to be replaced with the effective list of dependencies in the specified POM’s section. Since they are replaced, dependencies with a scope of import do not actually participate in limiting the transitivity of a dependency. scope属性值为import仅支持在\u003cdependencyManagement\u003e属性中定义的type为 pom 的 dependency，它表示该dependency会被该dependency内部pom.xml中的\u003cdependencyManagement\u003e替代。由于被替代了，dependency为import的依赖不会参与限制依赖的可传递性。简言之，若denpendency的scope配置为scope，那么该dependency会被其内部pom.xml中声明的\u003cdependencyManagement\u003e替代。 举例： 父项目pom.xml中配置摘要如下 \u003cdependencyManagement\u003e \u003cdependencies\u003e \u003c!-- SpringBoot的依赖配置--\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-dependencie","date":"2021-01-25","objectID":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/maven_scope%E5%B1%9E%E6%80%A7/:0:0","tags":null,"title":"Maven_scope属性","uri":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/maven_scope%E5%B1%9E%E6%80%A7/"},{"categories":null,"content":"java 多module项目，父module的pom.xml摘要如下 \u003cdependencyManagement\u003e \u003cdependencies\u003e \u003c!-- Zookeeper dependencies --\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.apache.dubbo\u003c/groupId\u003e \u003cartifactId\u003edubbo-dependencies-zookeeper\u003c/artifactId\u003e \u003cversion\u003e${dubbo.version}\u003c/version\u003e \u003ctype\u003epom\u003c/type\u003e \u003c!--\u003cscope\u003eimport\u003c/scope\u003e--\u003e \u003cexclusions\u003e \u003cexclusion\u003e \u003cgroupId\u003eorg.slf4j\u003c/groupId\u003e \u003cartifactId\u003eslf4j-api\u003c/artifactId\u003e \u003c/exclusion\u003e \u003cexclusion\u003e \u003cgroupId\u003eorg.slf4j\u003c/groupId\u003e \u003cartifactId\u003eslf4j-log4j12\u003c/artifactId\u003e \u003c/exclusion\u003e \u003cexclusion\u003e \u003cgroupId\u003elog4j\u003c/groupId\u003e \u003cartifactId\u003elog4j\u003c/artifactId\u003e \u003c/exclusion\u003e \u003c/exclusions\u003e \u003c/dependency\u003e \u003c/dependencies\u003e \u003c/dependencyManagement\u003e 子模块想要依赖 dubbo-dependencies-zookeeper ，若子模块的pom.xml配置如下 \u003cdependencies\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.apache.dubbo\u003c/groupId\u003e \u003cartifactId\u003edubbo-dependencies-zookeeper\u003c/artifactId\u003e \u003c/dependency\u003e \u003c/dependencies\u003e idea maven一栏点击 reload project，发现依赖没有引入，而且因为这个依赖没引入，导致其他修改的依赖不能更新（比如另一个依赖修改了名字，这个名字无法更新） 正确的引入方式为 \u003cdependencies\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.apache.dubbo\u003c/groupId\u003e \u003cartifactId\u003edubbo-dependencies-zookeeper\u003c/artifactId\u003e \u003ctype\u003epom\u003c/type\u003e \u003c/dependency\u003e \u003c/dependencies\u003e 注意，需要将父模块里的\u003cscope\u003eimport\u003c/scope\u003e注释掉，不然子模块加了\u003ctype\u003epom\u003c/type\u003e也无法引入 问题：加与不加 \u003cscope\u003eimport\u003c/scope\u003e 有什么区别呢？ 由 maven_scope属性 文章可知，scope属性为import，表示将该dependency替换为其内部定义的dependencyManagement，所以加上scope import，对子模块来说就无法直接引用该artifactId了。 子模块里想引入的是dubbo-dependencies-zookeeper本身，而不是 dubbo-dependencies-zookeeper里包含的依赖，所以注释掉scope import，注释掉scope import后，子模块既可以引用dubbo-dependencies-zookeeper本身，也可以引用dubbo-dependencies-zookeeper的子依赖，比如zookeeper dubbo-dependencies-zookeeper包的pom.xml内容摘要如下 \u003cdependencyManagement\u003e \u003cdependencies\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.apache.dubbo\u003c/groupId\u003e \u003cartifactId\u003edubbo-dependencies-bom\u003c/artifactId\u003e \u003cversion\u003e${project.version}\u003c/version\u003e \u003ctype\u003epom\u003c/type\u003e \u003cscope\u003eimport\u003c/scope\u003e \u003c/dependency\u003e \u003c/dependencies\u003e \u003c/dependencyManagement\u003e \u003cdependencies\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.apache.curator\u003c/groupId\u003e \u003cartifactId\u003ecurator-recipes\u003c/artifactId\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.apache.zookeeper\u003c/groupId\u003e \u003cartifactId\u003ezookeeper\u003c/artifactId\u003e \u003c/dependency\u003e \u003c/dependencies\u003e 可以看到，里面既有dependencyManagement，也有不包含在dependencyManagement里的dependencies，若项目A引入该依赖时使用scope import，则实际上会将该pom.xml中的dependencyManagement替换到项目A的dependencyManagement中。 ","date":"2021-01-25","objectID":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/java_%E5%AD%90module%E4%BE%9D%E8%B5%96pom%E7%B1%BB%E5%9E%8B%E7%9A%84dependency/:0:0","tags":null,"title":"Java_子module依赖pom类型的dependency","uri":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/java_%E5%AD%90module%E4%BE%9D%E8%B5%96pom%E7%B1%BB%E5%9E%8B%E7%9A%84dependency/"},{"categories":null,"content":"win10安装wsl2文档：https://docs.microsoft.com/zh-cn/windows/wsl/install-win10 win10安装 wsl2 后，就可以在商店里直接安装linux主机了，不用再借助vmware或virtualbox之类的虚拟化软件了 在商店里安装ubuntu。 ubuntu不允许使用用户名密码远程连接 解决：编辑/etc/ssh/sshd_config，去除PasswordAuthentication yes前面的注释，然后重启sshd（service ssh restart） ubuntu上root密码为空，使用如下命令配置root用户密码 sudo passwd root 无法使用root用户远程连接ubuntu 解决：编辑/etc/ssh/sshd_config，在#PermitRootLogin prohibit-password一行下手动加入PermitRootLogin yes，然后重启ssh（service ssh restart） ssh启动失败 root@HK-DESKTOP:/etc/netplan# vi /etc/ssh/sshd_config root@HK-DESKTOP:/etc/netplan# service ssh start * Starting OpenBSD Secure Shell server sshd sshd: no hostkeys available -- exiting. [fail] 解决：执行 ssh-keygen -A 命令 root@HK-DESKTOP:/etc/netplan# ssh-keygen -A ssh-keygen: generating new host keys: RSA DSA ECDSA ED25519 root@HK-DESKTOP:/etc/netplan# service ssh start * Starting OpenBSD Secure Shell server sshd [ OK ] wsl ubuntu 的ip是会动态变化的，可以在win10上使用127.0.0.1:22进行远程连接 wsl ubuntu 怎么关机？ 参考：https://blog.csdn.net/flynetcn/article/details/108133340 以管理员运行如下命令 net stop LxssManager 总结：可能是使用vmware习惯了，感觉wsl2 linux还是不太方便，没办法替代vmware。 ","date":"2021-01-25","objectID":"/posts/linux/windos10%E5%AE%89%E8%A3%85wsl%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/:0:0","tags":null,"title":"Windos10安装wsl问题汇总","uri":"/posts/linux/windos10%E5%AE%89%E8%A3%85wsl%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/"},{"categories":null,"content":" 新增 java.lang.AutoCloseable 接口 二进制字面量 jdk7开始，可以使用二进制来表示整数（byte, short, int 和 long） 只要在二进制数值前面加上 0b 或 0B 即可。 /** * jdk7新特性：二进制字面量表示整数 */ @Test public void testBinaryLiteral(){ int bin = 0b010101; //二进制，jdk7开始，以0b或0B开头表示 System.out.println(bin); //输出该二进制值对应的十进制值：21 int oct = 0177; //八进制，以0开头表示 System.out.println(oct); //输出该八进制值对应的十进制值：127 int dec = 123; //十进制 System.out.println(dec); //输出十进制值：123 int hex = 0XFD10; //十六进制，以0x或0X开头表示 System.out.println(hex); //输出该16进制值对应的十进制值：64784 } 数字字面量可以出现下划线 为了增强对数值的阅读性，JDK7和以后的版本可以使用 _ 对数值分隔。 注意： 不能出现在进制标识和数值之间 不能出现在数值开头和结尾 不能出现在小数点旁边 @Test public void testNumericLiteral(){ byte a = 127; short b = 10_000; int c = 100_000; float d = 12.34_56F; double e = 12.34_56D; //float f = 12.3456_F; //error //float f = _12.3456F; //error //float f = 12.3456F_; //error //float f = 12._3456F; //error //int j = 0_x52; //error //int j = 0x_52; //error } switch 语句可以使用字符串 switch 语句能否作用在byte、long、String上？ JDK7之前，在switch(expr1)中，expr1只能是一个整数表达式或者枚举常量，整数表达式可以是int基本类型或Integer包装类型，由于byte、short、char都可以隐式转换为int，所以这些类型及对应的包装类可以作为switch的参数。显然，由于long和String类型都不符合switch的语法规定，并且不能隐式转为int类型，所以，他们不能用于switch语句中。 JDK7开始，已经支持String类型作为switch的参数 @Test public void testSwitch(){ String season = \"春天\"; switch (season) { case \"春天\": System.out.println(\"春天来了\"); break; case \"夏天\": System.out.println(\"夏天来了\"); break; case \"秋天\": System.out.println(\"秋天来了\"); break; case \"冬天\": System.out.println(\"冬天来了\"); break; } } 泛型简化（泛型实例化类型推断） 泛型实例的创建可以通过类型推断来简化，可以去掉后面new部分的泛型类型，只用\u003c\u003e就可以了。 @Test public void testGeberic(){ //jdk7之前 List\u003cString\u003e listPreJDK7 = new ArrayList\u003cString\u003e(); //jdk7及以后 List\u003cString\u003e listForJDK7 = new ArrayList\u003c\u003e(); } catch语句可以捕获多个异常 JDK7之前，一个catch语句只能捕获一个异常，JDK7之后，一个catch语句可以捕获多个异常 @Test public void testDeserializationForJDK7() { File file = new File(\"tempFile\"); try (ObjectInputStream ois = new ObjectInputStream(new FileInputStream(file))) { User user = (User) ois.readObject(); System.out.println(user); } catch (IOException | ClassNotFoundException e) { e.printStackTrace(); } } try-with-resource语句 jdk7之前，使用try-catch-finally语句进行异常捕获和资源释放。 jdk7开始，支持try-with-resource语句，try-with-resources语句是一种声明了一种或多种资源的try语句。资源是指在程序用完了之后必须要关闭的对象。try-with-resources语句保证了每个声明了的资源在语句结束的时候都会被关闭。任何实现了java.lang.AutoCloseable接口的对象实现了java .io .Closeable接口的对象，都可以当做资源使用。 jdk7之前使用try-catch-finally语句代码示例 /** * java对象序列化测试 */ @Test public void testSerialization() { User user = new User(null, \"jack\", 23, \"Shanghai\"); ObjectOutputStream oos = null; try { oos = new ObjectOutputStream(new FileOutputStream(\"tempFile\")); oos.writeObject(user); } catch (IOException e) { e.printStackTrace(); } finally { try { if (oos != null) { oos.close(); } } catch (IOException e) { e.printStackTrace(); } } } jdk7使用try-with-resource语句代码示例 /** * 使用jdk7 try-with-resources 新特性 * java对象序列化测试 */ @Test public void testSerializationForJDK7() { User user = new User(null, \"jack\", 23, \"Shanghai\"); try (ObjectOutputStream oos = new ObjectOutputStream(new FileOutputStream(\"tempFile\"))) { oos.writeObject(user); } catch (IOException e) { e.printStackTrace(); } } 参考： https://blog.csdn.net/csdnlijingran/article/details/88855000 https://www.cnblogs.com/excellencesy/p/8613743.html ","date":"2021-01-25","objectID":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/jdk7%E6%96%B0%E7%89%B9%E6%80%A7/:0:0","tags":null,"title":"Jdk7新特性","uri":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/jdk7%E6%96%B0%E7%89%B9%E6%80%A7/"},{"categories":null,"content":" 参考： https://jingyan.baidu.com/article/656db918c36534e381249c83.html Java的序列化机制是通过判断serialVersionUID来验证版本的一致性，在反序列化的时候与本地类的serialVersionUID进行比较，若一致，则可以进行反序列化，若不一致则会抛出InvalidCastException。 使用idea时，可以配置在类实现java.io.Serializable接口时，自动生成serialVersionUID，步骤如下 File–\u003eSettings–\u003eEditor–\u003eInspections–\u003e在搜索框输入serialVersionUID，然后勾选 Serialzable class without serialVersionUID –\u003eApply 然后，新建类，实现 Serialiable接口，然后光标放在该类名上，点击Alt+Insert，就可以实现自动生成SerialVersionUID了。 public class User implements Serializable { private static final long serialVersionUID = 6151537955659372302L; private Integer id; private String name; private Integer age; private String address; } 查看Serializable接口文档 If a serializable class does not explicitly declare a serialVersionUID, then the serialization runtime will calculate a default serialVersionUID value for that class based on various aspects of the class, as described in the Java(TM) Object Serialization Specification. However, it is strongly recommended that all serializable classes explicitly declare serialVersionUID values, since the default serialVersionUID computation is highly sensitive to class details that may vary depending on compiler implementations, and can thus result in unexpected InvalidClassExceptions during deserialization. Therefore, to guarantee a consistent serialVersionUID value across different java compiler implementations, a serializable class must declare an explicit serialVersionUID value. It is also strongly advised that explicit serialVersionUID declarations use the private modifier where possible, since such declarations apply only to the immediately declaring class--serialVersionUID fields are not useful as inherited members. Array classes cannot declare an explicit serialVersionUID, so they always have the default computed value, but the requirement for matching serialVersionUID values is waived for array classes. 上面说如果一个类实现了Serialiable接口，没有配置实现serialVersionUID，则序列化运行时会自动生成一个serialVersionUID，但是，不推荐使用这种方式，因为序列化运行时自动生成的serialVersionUID每次都是不同的，不能保证反序列化的时候和本地类的serialVersionUID一致。 如果一个类不实现Serializable接口，但是在该类中，定义serialVersionUID，此时有作用吗？ 不行，这种情况如果进行序列化时，会抛出 java.io.NotSerializableException 异常，对序列化文件进行反序列化时会抛出 java.io.WriteAbortedException: writing aborted; java.io.NotSerializableException 异常。 测试类： @Data @AllArgsConstructor @NoArgsConstructor public class User implements Serializable { private static final long serialVersionUID = 6151537955659372302L; private Integer id; private String name; private Integer age; private String address; } @SpringBootTest public class SerializationTest { /** * 使用jdk7 try-with-resources 新特性 * java对象序列化测试 */ @Test public void testSerializationForJDK7() { User user = new User(null, \"jack\", 23, \"Shanghai\"); try (ObjectOutputStream oos = new ObjectOutputStream(new FileOutputStream(\"tempFile\"))) { oos.writeObject(user); } catch (IOException e) { e.printStackTrace(); } } /** * 使用jdk7 try-with-resources 新特性 * java对象反序列化测试 */ @Test public void testDeserializationForJDK7() { File file = new File(\"tempFile\"); try (ObjectInputStream ois = new ObjectInputStream(new FileInputStream(file))) { User user = (User) ois.readObject(); System.out.println(user); } catch (IOException | ClassNotFoundException e) { e.printStackTrace(); } } 序列化相关文章 https://www.cnblogs.com/junzi2099/p/9014814.html https://blog.csdn.net/qq_37552636/article/details/110651198 ","date":"2021-01-25","objectID":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/idea%E9%85%8D%E7%BD%AE%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90serialversionuid/:0:0","tags":null,"title":"Idea配置自动生成serialVersionUID","uri":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/idea%E9%85%8D%E7%BD%AE%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90serialversionuid/"},{"categories":null,"content":" 参考： https://blog.csdn.net/CF779/article/details/112269347 在 idea java maven项目中新建名为 demo 的module，将该 module删除后，再次新建名为 demo 的 module，该 module的 pom.xml 会显示为 ingored pom.xml 解决：进入 File–\u003eSettings–\u003eBuild,Execution,Deployment–\u003eBuild Tools–\u003eMaven–\u003eIgnored Files，将demo module前的勾去掉即可。 ","date":"2021-01-25","objectID":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/idea-maven%E9%A1%B9%E7%9B%AE%E5%87%BA%E7%8E%B0ignored_pom.xml/:0:0","tags":null,"title":"IDEA Maven项目出现ignored pom","uri":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/idea-maven%E9%A1%B9%E7%9B%AE%E5%87%BA%E7%8E%B0ignored_pom.xml/"},{"categories":null,"content":" 参考： https://maven.aliyun.com/mvn/guide https://www.cnblogs.com/default/p/11856188.html 在java maven项目中，配置阿里云依赖jar包仓库和maven插件仓库，在 pom.xml中添加如下配置 \u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003cproject xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://maven.apache.org/POM/4.0.0\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"\u003e \u003cmodelVersion\u003e4.0.0\u003c/modelVersion\u003e \u003crepositories\u003e \u003c!--maven仓库--\u003e \u003crepository\u003e \u003cid\u003ealiyun\u003c/id\u003e \u003cname\u003ealiyun repo\u003c/name\u003e \u003curl\u003ehttps://maven.aliyun.com/repository/public\u003c/url\u003e \u003creleases\u003e \u003cenabled\u003etrue\u003c/enabled\u003e \u003c/releases\u003e \u003csnapshots\u003e \u003cenabled\u003efalse\u003c/enabled\u003e \u003c/snapshots\u003e \u003c/repository\u003e \u003c/repositories\u003e \u003cpluginRepositories\u003e \u003c!--maven插件仓库--\u003e \u003cpluginRepository\u003e \u003cid\u003ealiyun-plugin\u003c/id\u003e \u003cname\u003ealiyun plugin\u003c/name\u003e \u003curl\u003ehttps://maven.aliyun.com/repository/public\u003c/url\u003e \u003creleases\u003e \u003cenabled\u003etrue\u003c/enabled\u003e \u003c/releases\u003e \u003csnapshots\u003e \u003cenabled\u003efalse\u003c/enabled\u003e \u003c/snapshots\u003e \u003c/pluginRepository\u003e \u003c/pluginRepositories\u003e \u003c/project\u003e ","date":"2021-01-25","objectID":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/java%E9%A1%B9%E7%9B%AE%E9%85%8D%E7%BD%AE%E9%98%BF%E9%87%8C%E4%BA%91%E4%BB%93%E5%BA%93/:0:0","tags":null,"title":"Java项目配置阿里云仓库","uri":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/java%E9%A1%B9%E7%9B%AE%E9%85%8D%E7%BD%AE%E9%98%BF%E9%87%8C%E4%BA%91%E4%BB%93%E5%BA%93/"},{"categories":null,"content":"spring-boot-starter-parent 的 pom.xml 文件中有如下内容 \u003cproperties\u003e \u003cjava.version\u003e1.8\u003c/java.version\u003e \u003cspring-boot.version\u003e2.4.2\u003c/spring-boot.version\u003e \u003cmaven-compiler-plugin.version\u003e3.8.1\u003c/maven-compiler-plugin.version\u003e \u003cproject.encoding\u003eUTF-8\u003c/project.encoding\u003e \u003cresource.delimiter\u003e@\u003c/resource.delimiter\u003e \u003cmaven.compiler.source\u003e${java.version}\u003c/maven.compiler.source\u003e \u003cmaven.compiler.target\u003e${java.version}\u003c/maven.compiler.target\u003e \u003cproject.build.sourceEncoding\u003e${project.encoding}\u003c/project.build.sourceEncoding\u003e \u003cproject.reporting.outputEncoding\u003e${project.encoding}\u003c/project.reporting.outputEncoding\u003e \u003crevision\u003e1.0.0\u003c/revision\u003e \u003c/properties\u003e \u003cbuild\u003e \u003cresources\u003e \u003cresource\u003e \u003cdirectory\u003e${basedir}/src/main/resources\u003c/directory\u003e \u003cfiltering\u003etrue\u003c/filtering\u003e \u003cincludes\u003e \u003cinclude\u003e**/application*.yml\u003c/include\u003e \u003cinclude\u003e**/application*.yaml\u003c/include\u003e \u003cinclude\u003e**/application*.properties\u003c/include\u003e \u003c/includes\u003e \u003c/resource\u003e \u003cresource\u003e \u003cdirectory\u003e${basedir}/src/main/resources\u003c/directory\u003e \u003cexcludes\u003e \u003cexclude\u003e**/application*.yml\u003c/exclude\u003e \u003cexclude\u003e**/application*.yaml\u003c/exclude\u003e \u003cexclude\u003e**/application*.properties\u003c/exclude\u003e \u003c/excludes\u003e \u003c/resource\u003e \u003c/resources\u003e \u003c/build\u003e 为什么把${basedir}/src/main/resources目录下的**/application*.yml文件include到classpath后又exclude了呢？ resource 标签是按照顺序进行加载的 第一个resource标签，表示加载项目路径/src/main/resources下的application*.yml文件 然后再加载第二个 resource 标签，表示加载除了第一个resource标签加载内容（项目路径/src/main/resources下的application*.yml文件）之外的其他文件 \u003cfiltering\u003etrue\u003c/filtering\u003e的作用是什么呢？ 参考：https://www.cnblogs.com/wangxuejian/p/13551292.html SpringEL表达式的取值一般使用 ${var} 方式，如 application.properties 和 @Value(\"${var}\") 中的取值 maven的pom.xml文件也有类似的取值方式，也是使用 ${var} 方式取值。 然而，它们并不是一个东西 SpringEL表达式取值适用于配置文件和代码中的注解 maven的占位符取值表达式默认仅仅适用于pom.xml文件中 所以，默认情况下是不能在application.properties里获取pom.xml中定义的值的。 如果我们想打通二者的交流，该怎么实现呢？这时filtering就派上用场了 maven的占位符解析表达式默认只在pom.xml文件内使用，如果想扩大它的活动范围，就必须指定需要扩大到哪些文件，然后指定 filtering = true，这样，maven的占位符解析表达式就可以用于那些文件里进行表达式解析了。 如上，springboot parent中的pom.xml，配置 \u003cresource.delimiter\u003e@\u003c/resource.delimiter\u003e 将maven的占位符置为@，同时，在第一个resource标签中指定maven占位符的使用范围扩大到项目路径/src/main/resources下的application*.yml文件，这样，application.yml文件就能使用@var@的形式获取maven的参数了。 ","date":"2021-01-25","objectID":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/springboot-pom%E6%96%87%E4%BB%B6%E4%B8%AD%E7%9A%84resources%E6%A0%87%E7%AD%BE/:0:0","tags":null,"title":"Springboot Pom文件中的resources标签","uri":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/springboot-pom%E6%96%87%E4%BB%B6%E4%B8%AD%E7%9A%84resources%E6%A0%87%E7%AD%BE/"},{"categories":null,"content":"web.xml 示例如下： \u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003cweb-app xmlns=\"http://java.sun.com/xml/ns/javaee\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_3_0.xsd\" version=\"3.0\"\u003e \u003c/web-app\u003e web.xml文件中的版本号（version=“3.0”）指的是servlet的版本 servlet版本与tomcat版本、jdk版本的对应关系可以查看tomcat官网的 whichversion 页面 http://tomcat.apache.org/whichversion.html ","date":"2021-01-25","objectID":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/web.xml%E7%89%88%E6%9C%AC%E4%B8%8Etomcat%E7%89%88%E6%9C%ACjdk%E7%89%88%E6%9C%AC%E7%9A%84%E5%AF%B9%E5%BA%94%E5%85%B3%E7%B3%BB/:0:0","tags":null,"title":"Web.xml版本与tomcat版本、jdk版本的对应关系","uri":"/posts/java/%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/web.xml%E7%89%88%E6%9C%AC%E4%B8%8Etomcat%E7%89%88%E6%9C%ACjdk%E7%89%88%E6%9C%AC%E7%9A%84%E5%AF%B9%E5%BA%94%E5%85%B3%E7%B3%BB/"},{"categories":["算法"],"content":"需求 如何判定一堆不重复的字符串是否以某个前缀开头？ 用Set\\Map存储字符串 遍历所有字符串进行判断 时间复杂度 O(n) 有没有更优的数据结构实现前缀搜索？ Trie ","date":"2020-09-24","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-18-trie/:1:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-18-Trie","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-18-trie/"},{"categories":["算法"],"content":"Trie Trie 也叫做字典树、前缀树（Prefix Tree）、单词查找树 Trie 搜索字符串的效率主要跟字符串的长度有关 假设使用 Trie 存储 cat、dog、doggy、does、cast、add 六个单词 ","date":"2020-09-24","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-18-trie/:2:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-18-Trie","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-18-trie/"},{"categories":["算法"],"content":"Trie 接口设计参考 只存储字符串 int size(); boolean isEmpty(); void clear(); boolean contains(String str); void add(String str); void remove(String str); boolean startsWith(String prefix) 存储字符串和值 int size(); boolean isEmpty(); void clear(); boolean contains(String str); V add(String str, V value); V remove(String str); boolean startsWith(String prefix) ","date":"2020-09-24","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-18-trie/:3:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-18-Trie","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-18-trie/"},{"categories":["算法"],"content":"实现 package com.mj; import java.util.HashMap; public class Trie\u003cV\u003e { private int size; private Node\u003cV\u003e root; public int size() { return size; } public boolean isEmpty() { return size == 0; } public void clear() { size = 0; root = null; } public V get(String key) { Node\u003cV\u003e node = node(key); return node != null \u0026\u0026 node.word ? node.value : null; } public boolean contains(String key) { Node\u003cV\u003e node = node(key); return node != null \u0026\u0026 node.word; } public V add(String key, V value) { keyCheck(key); // 创建根节点 if (root == null) { root = new Node\u003c\u003e(null); } Node\u003cV\u003e node = root; int len = key.length(); for (int i = 0; i \u003c len; i++) { char c = key.charAt(i); boolean emptyChildren = node.children == null; Node\u003cV\u003e childNode = emptyChildren ? null : node.children.get(c); if (childNode == null) { childNode = new Node\u003c\u003e(node); childNode.character = c; node.children = emptyChildren ? new HashMap\u003c\u003e() : node.children; node.children.put(c, childNode); } node = childNode; } if (node.word) { // 已经存在这个单词 V oldValue = node.value; node.value = value; return oldValue; } // 新增一个单词 node.word = true; node.value = value; size++; return null; } public V remove(String key) { // 找到最后一个节点 Node\u003cV\u003e node = node(key); // 如果不是单词结尾，不用作任何处理 if (node == null || !node.word) return null; size--; V oldValue = node.value; // 如果还有子节点 if (node.children != null \u0026\u0026 !node.children.isEmpty()) { node.word = false; node.value = null; return oldValue; } // 如果没有子节点 Node\u003cV\u003e parent = null; while ((parent = node.parent) != null) { parent.children.remove(node.character); if (parent.word || !parent.children.isEmpty()) break; node = parent; } return oldValue; } public boolean startsWith(String prefix) { return node(prefix) != null; } private Node\u003cV\u003e node(String key) { keyCheck(key); Node\u003cV\u003e node = root; int len = key.length(); for (int i = 0; i \u003c len; i++) { if (node == null || node.children == null || node.children.isEmpty()) return null; char c = key.charAt(i); node = node.children.get(c); } return node; } private void keyCheck(String key) { if (key == null || key.length() == 0) { throw new IllegalArgumentException(\"key must not be empty\"); } } private static class Node\u003cV\u003e { Node\u003cV\u003e parent; HashMap\u003cCharacter, Node\u003cV\u003e\u003e children; Character character; V value; boolean word; // 是否为单词的结尾（是否为一个完整的单词） public Node(Node\u003cV\u003e parent) { this.parent = parent; } } } ","date":"2020-09-24","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-18-trie/:4:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-18-Trie","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-18-trie/"},{"categories":["算法"],"content":"总结 Trie的优点：搜索前缀的效率主要跟前缀的长度有关 Trie的缺点：需要耗费大量内存，因此还有待改进 更多Trie相关的数据结构和算法 Double-array Trie、Suffix Tree、Patricia Tree、Crit-bit Tree、AC自动机 ","date":"2020-09-24","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-18-trie/:5:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-18-Trie","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-18-trie/"},{"categories":["算法"],"content":"数据结构总结 复杂度 时间复杂度 空间复杂度 线性数据结构 动态数组（ArrayList） 链表（LinkedList） 单向链表 双向链表 循环链表 静态链表 栈（Stack） 队列（Queue） 双端队列（Deque） 循环队列 哈希表（HashTable） 树形数据结构 二叉树（BinaryTree）、二叉搜索树（BinarySearchTree、BST） 平衡二叉搜索树（BalancedBinarySearchTree、BBST） AVL树（AVLTree）、红黑树（RebBlackTree） B树（B-Tree） 集合（TreeSet）、映射（TreeMap） 哈夫曼树 Trie 线性+树形数据结构 集合（HashSet） 映射（HashMap、LinkedHashMap） 二叉堆（BinaryHeap） 优先级队列（PriorityQueue） ","date":"2020-09-24","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-18-trie/:6:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-18-Trie","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-18-trie/"},{"categories":["算法"],"content":"哈夫曼编码 哈夫曼编码，又称为霍夫曼编码，它是现代压缩算法的基础 假设要把字符串 ABBBCCCCCCCCDDDDDDEE 转成二进制编码进行传输 可以转为 ASCII 编码（65-69， 1000001~1000101），但是有点冗余，如果希望编码更短呢？ 可以先约定 5 个字母对应的二进制 A B C D E 000 001 010 011 100 对应的二进制编码：000001001001010010010010010010010010011011011011011011100100 一共20个字母，转成了60个二进制位 如果使用哈夫曼编码，可以压缩至41个二进制位，约为原来长度的68.3% ","date":"2020-09-24","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-17-%E5%93%88%E5%A4%AB%E6%9B%BC%E6%A0%91/:1:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-17-哈夫曼树","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-17-%E5%93%88%E5%A4%AB%E6%9B%BC%E6%A0%91/"},{"categories":["算法"],"content":"哈夫曼树 先计算出每个字母的出现频率（权值，这里直接用出现次数）， ABBBCCCCCCCCDDDDDDEE A B C D E 1 3 8 6 2 利用这些权值，构建一棵哈夫曼树（又称为霍夫曼树、最优二叉树） 如何构建一棵哈夫曼树？（假设有 n 个权值） 以权值作为根节点构建 n 棵二叉树，组成森林 在森林中选出 2 个根节点最小的树合并，作为一棵新树的左右子树，且新树的根节点为其左右子树根节点之和 从森林中删除刚才选取的2棵树，并将新树加入森林 重复步骤2、3，直到森林只剩一棵树为止，该树即为哈夫曼树 ","date":"2020-09-24","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-17-%E5%93%88%E5%A4%AB%E6%9B%BC%E6%A0%91/:2:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-17-哈夫曼树","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-17-%E5%93%88%E5%A4%AB%E6%9B%BC%E6%A0%91/"},{"categories":["算法"],"content":"构建哈夫曼编码 left 为 0， right 为 1，可以得出 5 个字母对应的哈夫曼编码 A B C D E 1110 110 0 10 1111 ABBBCCCCCCCCDDDDDDEE 的哈夫曼编码是 1110110110110000000001010101010101111 总结 n 个权值构建出来的哈夫曼树拥有 n 个叶子节点 每个哈夫曼编码都不是另一个哈夫曼编码的前缀 哈夫曼树是带权路径长度最短的树，权值较大的节点离根节点较近 带权路径长度：树中所有的叶子节点的权值乘上其到根节点的路径长度。与最终的哈夫曼编码总长度成正比关系 ","date":"2020-09-24","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-17-%E5%93%88%E5%A4%AB%E6%9B%BC%E6%A0%91/:3:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-17-哈夫曼树","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-17-%E5%93%88%E5%A4%AB%E6%9B%BC%E6%A0%91/"},{"categories":["算法"],"content":"优先级队列（Priority Queue） 优先级队列也是个队列，因此提供如下接口 int size(); //元素的数量 boolean isEmpty(); //是否为空 void enQueue(E element); //入队 E deQueue(); //出队 E front(); //获取队列的头元素 void clear(); //清空 普通的队列是 FIFO 原则，也就是先进先出 优先级队列则是按照优先级高低进行出队，比如将优先级最高的元素作为队头优先出队 ","date":"2020-09-24","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-16-%E4%BC%98%E5%85%88%E7%BA%A7%E9%98%9F%E5%88%97/:1:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-16-优先级队列","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-16-%E4%BC%98%E5%85%88%E7%BA%A7%E9%98%9F%E5%88%97/"},{"categories":["算法"],"content":"优先级队列的底层实现 使用二叉堆实现优先级队列 可以通过 Comparator 或 Comparable 去自定义优先级高低 package com.mj.queue; import java.util.Comparator; import com.mj.heap.BinaryHeap; public class PriorityQueue\u003cE\u003e { private BinaryHeap\u003cE\u003e heap; public PriorityQueue(Comparator\u003cE\u003e comparator) { heap = new BinaryHeap\u003c\u003e(comparator); } public PriorityQueue() { this(null); } public int size() { return heap.size(); } public boolean isEmpty() { return heap.isEmpty(); } public void clear() { heap.clear(); } public void enQueue(E element) { heap.add(element); } public E deQueue() { return heap.remove(); } public E front() { return heap.get(); } } 测试 public static void main(String[] args) { PriorityQueue\u003cPerson\u003e queue = new PriorityQueue\u003c\u003e(); queue.enQueue(new Person(\"Jack\", 2)); queue.enQueue(new Person(\"Rose\", 10)); queue.enQueue(new Person(\"Jake\", 5)); queue.enQueue(new Person(\"James\", 15)); while (!queue.isEmpty()) { System.out.println(queue.deQueue()); } } public class Person implements Comparable\u003cPerson\u003e { private String name; private int boneBreak; public Person(String name, int boneBreak) { this.name = name; this.boneBreak = boneBreak; } @Override public int compareTo(Person person) { return this.boneBreak - person.boneBreak; } @Override public String toString() { return \"Person [name=\" + name + \", boneBreak=\" + boneBreak + \"]\"; } } ","date":"2020-09-24","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-16-%E4%BC%98%E5%85%88%E7%BA%A7%E9%98%9F%E5%88%97/:2:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-16-优先级队列","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-16-%E4%BC%98%E5%85%88%E7%BA%A7%E9%98%9F%E5%88%97/"},{"categories":["算法"],"content":"思考 设计一种数据结构，用来存放整数，要求提供 3 个接口 添加元素 获取最大值 删除最大值 有没有更优的数据结构？ 堆 获取最大值：O(1)、删除最大值：O(log n)，添加元素：O(log n) ","date":"2020-09-24","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-15-%E4%BA%8C%E5%8F%89%E5%A0%86/:1:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-15-二叉堆","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-15-%E4%BA%8C%E5%8F%89%E5%A0%86/"},{"categories":["算法"],"content":"Top K 问题 什么是 Top K 问题 从海量数据中找出前 K 个数据 比如 从 100 万个整数中找出最大的 100 个整数 Top K 问题的解法之一：可以用数据结构“堆”来解决 ","date":"2020-09-24","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-15-%E4%BA%8C%E5%8F%89%E5%A0%86/:2:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-15-二叉堆","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-15-%E4%BA%8C%E5%8F%89%E5%A0%86/"},{"categories":["算法"],"content":"堆（Heap） 堆（Heap）也是一种树状的数据结构（不要跟内存模型中的“堆空间”混淆），常见的堆实现有 二叉堆（Binary Heap，完全二叉堆） 多叉堆（D-heap、D-ary Heap） 索引堆（Index Heap） 二项堆（Binomial Heap） 斐波那契堆（Fibonacci Heap） 左倾堆（Leftist Heap，左式堆） 斜堆（Skew Heap） 堆的一个重要性质：任意节点的值总是 \u003e= （\u003c=） 子节点的值 如果任意节点的值总是大于等于子节点的值，称为：最大堆、大根堆、大项堆 如果任意节点的值总是小于等于子节点的值，称为：最小堆、小根堆、小项堆 由此可见，堆中的元素必须具备可比较性（跟二叉搜索树一样） ","date":"2020-09-24","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-15-%E4%BA%8C%E5%8F%89%E5%A0%86/:3:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-15-二叉堆","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-15-%E4%BA%8C%E5%8F%89%E5%A0%86/"},{"categories":["算法"],"content":"堆的基本接口设计 int size(); //元素的数量 boolean isEmpty(); //是否为空 void clear(); //清空 void add(E element); //添加元素 E get(); //获得堆顶元素 E remove(); //删除堆顶元素 E replace(E element); //删除堆顶元素的同时插入一个新元素 ","date":"2020-09-24","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-15-%E4%BA%8C%E5%8F%89%E5%A0%86/:4:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-15-二叉堆","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-15-%E4%BA%8C%E5%8F%89%E5%A0%86/"},{"categories":["算法"],"content":"二叉堆（Binary Heap） 二叉堆的逻辑结构就是一棵完全二叉树，所以也叫完全二叉堆 鉴于完全二叉树的一些特性，二叉堆的底层（物理结构）一般用数组实现 索引 i 的规律（n 是元素数量） 如果 i = 0，它是根节点 如果 i \u003e 0，它的父节点的索引为 floor((i-1)/2) 如果 2i + 1 \u003c= n - 1，它的左子节点的索引为 2i + 1 如果 2i + 1 \u003e n - 1，它无左子节点 如果 2i + 2 \u003c= n - 1，它的右子节点的索引为 2i + 2 如果 2i + 2 \u003e n - 1，它无右子节点 ","date":"2020-09-24","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-15-%E4%BA%8C%E5%8F%89%E5%A0%86/:5:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-15-二叉堆","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-15-%E4%BA%8C%E5%8F%89%E5%A0%86/"},{"categories":["算法"],"content":"二叉堆的实现 package com.mj.heap; public interface Heap\u003cE\u003e { int size(); // 元素的数量 boolean isEmpty(); // 是否为空 void clear(); // 清空 void add(E element); // 添加元素 E get(); // 获得堆顶元素 E remove(); // 删除堆顶元素 E replace(E element); // 删除堆顶元素的同时插入一个新元素 } package com.mj.heap; import java.util.Comparator; @SuppressWarnings(\"unchecked\") public abstract class AbstractHeap\u003cE\u003e implements Heap\u003cE\u003e { protected int size; protected Comparator\u003cE\u003e comparator; public AbstractHeap(Comparator\u003cE\u003e comparator) { this.comparator = comparator; } public AbstractHeap() { this(null); } @Override public int size() { return size; } @Override public boolean isEmpty() { return size == 0; } protected int compare(E e1, E e2) { return comparator != null ? comparator.compare(e1, e2) : ((Comparable\u003cE\u003e)e1).compareTo(e2); } } package com.mj.heap; import java.util.Comparator; import com.mj.printer.BinaryTreeInfo; /** * 二叉堆（最大堆） * @author MJ Lee * * @param \u003cE\u003e */ @SuppressWarnings(\"unchecked\") public class BinaryHeap\u003cE\u003e extends AbstractHeap\u003cE\u003e implements BinaryTreeInfo { private E[] elements; private static final int DEFAULT_CAPACITY = 10; public BinaryHeap(E[] elements, Comparator\u003cE\u003e comparator) { super(comparator); if (elements == null || elements.length == 0) { this.elements = (E[]) new Object[DEFAULT_CAPACITY]; } else { size = elements.length; int capacity = Math.max(elements.length, DEFAULT_CAPACITY); this.elements = (E[]) new Object[capacity]; for (int i = 0; i \u003c elements.length; i++) { this.elements[i] = elements[i]; } heapify(); } } public BinaryHeap(E[] elements) { this(elements, null); } public BinaryHeap(Comparator\u003cE\u003e comparator) { this(null, comparator); } public BinaryHeap() { this(null, null); } @Override public void clear() { for (int i = 0; i \u003c size; i++) { elements[i] = null; } size = 0; } @Override public void add(E element) { elementNotNullCheck(element); ensureCapacity(size + 1); elements[size++] = element; siftUp(size - 1); } @Override public E get() { emptyCheck(); return elements[0]; } @Override public E remove() { emptyCheck(); int lastIndex = --size; E root = elements[0]; elements[0] = elements[lastIndex]; elements[lastIndex] = null; siftDown(0); return root; } @Override public E replace(E element) { elementNotNullCheck(element); E root = null; if (size == 0) { elements[0] = element; size++; } else { root = elements[0]; elements[0] = element; siftDown(0); } return root; } /** * 批量建堆 */ private void heapify() { // 自上而下的上滤 // for (int i = 1; i \u003c size; i++) { // siftUp(i); // } // 自下而上的下滤 for (int i = (size \u003e\u003e 1) - 1; i \u003e= 0; i--) { siftDown(i); } } /** * 让index位置的元素下滤 * @param index */ private void siftDown(int index) { E element = elements[index]; int half = size \u003e\u003e 1; // 第一个叶子节点的索引 == 非叶子节点的数量 // index \u003c 第一个叶子节点的索引 // 必须保证index位置是非叶子节点 while (index \u003c half) { // index的节点有2种情况 // 1.只有左子节点 // 2.同时有左右子节点 // 默认为左子节点跟它进行比较 int childIndex = (index \u003c\u003c 1) + 1; E child = elements[childIndex]; // 右子节点 int rightIndex = childIndex + 1; // 选出左右子节点最大的那个 if (rightIndex \u003c size \u0026\u0026 compare(elements[rightIndex], child) \u003e 0) { child = elements[childIndex = rightIndex]; } if (compare(element, child) \u003e= 0) break; // 将子节点存放到index位置 elements[index] = child; // 重新设置index index = childIndex; } elements[index] = element; } /** * 让index位置的元素上滤 * @param index */ private void siftUp(int index) { // E e = elements[index]; // while (index \u003e 0) { // int pindex = (index - 1) \u003e\u003e 1; // E p = elements[pindex]; // if (compare(e, p) \u003c= 0) return; // // // 交换index、pindex位置的内容 // E tmp = elements[index]; // elements[index] = elements[pindex]; // elements[pindex] = tmp; // // // 重新赋值index // index = pindex; // } E element = elements[index]; while (index \u003e 0) { int parentIndex = (index - 1) \u003e\u003e 1; E parent = elements[parentIndex]; if (compare(element, parent) \u003c= 0) break; // 将父元素存储在index位置 elements[index] = parent; // 重新赋值index index = parentIndex; } elements[index] = element; } private void ensureCapacity(int capacity) { int oldCapacity = elements.length; if (oldCapacity \u003e= capacity) return; // 新容量为","date":"2020-09-24","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-15-%E4%BA%8C%E5%8F%89%E5%A0%86/:6:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-15-二叉堆","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-15-%E4%BA%8C%E5%8F%89%E5%A0%86/"},{"categories":["算法"],"content":"最大堆 - 添加 循环执行以下操作（图中的 80 简称为 node） 如果 node \u003e 父节点 与父节点交换位置 如果 node \u003c 父节点，或者 node 没有父节点 退出循环 这个过程，叫做上滤（Sift Up） 时间复杂度：O(log n) 交换位置的优化 实现 @Override public void add(E element) { elementNotNullCheck(element); ensureCapacity(size + 1); elements[size++] = element; siftUp(size - 1); } /** * 让index位置的元素上滤 * @param index */ private void siftUp(int index) { // E e = elements[index]; // while (index \u003e 0) { // int pindex = (index - 1) \u003e\u003e 1; // E p = elements[pindex]; // if (compare(e, p) \u003c= 0) return; // // // 交换index、pindex位置的内容 // E tmp = elements[index]; // elements[index] = elements[pindex]; // elements[pindex] = tmp; // // // 重新赋值index // index = pindex; // } E element = elements[index]; while (index \u003e 0) { int parentIndex = (index - 1) \u003e\u003e 1; E parent = elements[parentIndex]; if (compare(element, parent) \u003c= 0) break; // 将父元素存储在index位置 elements[index] = parent; // 重新赋值index index = parentIndex; } elements[index] = element; } ","date":"2020-09-24","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-15-%E4%BA%8C%E5%8F%89%E5%A0%86/:6:1","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-15-二叉堆","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-15-%E4%BA%8C%E5%8F%89%E5%A0%86/"},{"categories":["算法"],"content":"最大堆 - 删除 删除堆顶元素 用最后一个节点覆盖根节点 删除最后一个节点 循环执行以下操作（图中的43简称为node） 如果 node \u003c 最大的子节点 与最大的子节点交换位置 如果 node \u003e= 最大的子节点，或者 node 没有子节点 退出循环 这个过程，叫做下滤（Sift Down），时间复杂度：O(log n) 同样的，交换位置的操作可以像添加那样进行优化 实现 @Override public E remove() { emptyCheck(); int lastIndex = --size; E root = elements[0]; elements[0] = elements[lastIndex]; elements[lastIndex] = null; siftDown(0); return root; } /** * 让index位置的元素下滤 * @param index */ private void siftDown(int index) { E element = elements[index]; int half = size \u003e\u003e 1; // 第一个叶子节点的索引 == 非叶子节点的数量 // index \u003c 第一个叶子节点的索引 // 必须保证index位置是非叶子节点 while (index \u003c half) { // index的节点有2种情况 // 1.只有左子节点 // 2.同时有左右子节点 // 默认为左子节点跟它进行比较 int childIndex = (index \u003c\u003c 1) + 1; E child = elements[childIndex]; // 右子节点 int rightIndex = childIndex + 1; // 选出左右子节点最大的那个 if (rightIndex \u003c size \u0026\u0026 compare(elements[rightIndex], child) \u003e 0) { child = elements[childIndex = rightIndex]; } if (compare(element, child) \u003e= 0) break; // 将子节点存放到index位置 elements[index] = child; // 重新设置index index = childIndex; } elements[index] = element; } ","date":"2020-09-24","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-15-%E4%BA%8C%E5%8F%89%E5%A0%86/:6:2","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-15-二叉堆","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-15-%E4%BA%8C%E5%8F%89%E5%A0%86/"},{"categories":["算法"],"content":"最大堆 - 替换 @Override public E replace(E element) { elementNotNullCheck(element); E root = null; if (size == 0) { elements[0] = element; size++; } else { root = elements[0]; elements[0] = element; siftDown(0); } return root; } ","date":"2020-09-24","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-15-%E4%BA%8C%E5%8F%89%E5%A0%86/:6:3","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-15-二叉堆","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-15-%E4%BA%8C%E5%8F%89%E5%A0%86/"},{"categories":["算法"],"content":"最大堆 - 批量建堆（Heapify） 批量建堆，有2种做法 自上而下的上滤 自下而上的下滤 最大堆 - 批量建堆 - 自上而下的上滤 最大堆 - 批量建堆 - 自下而上的下滤 叶子节点下面没有节点了，所以叶子节点没必要下滤 批量建堆效率对比 自上而下的上滤相当于一个一个添加，它的效率低于自下而上的下滤 所有节点的深度之和 仅仅是叶子节点，就有近 n/2 个，而且每一个叶子节点的深度都是 O(log n) 级别的 因此，在叶子节点这一块，就达到了 O(nlog n) 级别 O(nlog n) 的时间复杂度足以利用排序算法对所有节点进行全排序 所有节点的高度之和 假设是满树，节点总个数为 n，树高为 h，那么 n = 2h − 1 所有节点的树高之和 H(n) = 20 ∗ (h − 0) + 21 ∗ (h − 1) + 22 ∗ (h − 2) + … + 2h-1 ∗ [h − (h − 1)] H(n) = h ∗ (20 + 21 + 22 + … + 2h-1) - [1 ∗ 21 + 2 ∗ 22 + 3 ∗ 23 + … + (h − 1) ∗ 2h-1] H(n) = h ∗ (2h − 1) − [(h − 2) ∗ 2h + 2] H(n) = h ∗ 2h − h − h ∗ 2h + 2h+1 − 2 H(n) = 2h+1 − h − 2 = 2 ∗ (2h − 1) − h = 2n − h = 2n − log2(n + 1) = O(n) 公式推导 S(h) = 1 ∗ 21 + 2 ∗ 22 + 3 ∗ 23 + … + (h − 2) ∗ 2h-2 + (h − 1) ∗ 2h-1 2S(h) = 1 ∗ 22 + 2 ∗ 23 + 3 ∗ 24 + … + (h − 2) ∗ 2h-1 + (h − 1) ∗ 2h S(h) – 2S(h) = [21 + 22 + 23 + … + 2h-1] − (h − 1) ∗ 2h = (2h − 2) − (h − 1) ∗ 2h S(h) = (h − 1) ∗ 2h − (2h − 2) = (h − 2) ∗ 2h + 2 疑惑 以下方法可以批量建堆吗？ 自上而下的下滤 自下而上的上滤 上述方法不可行 批量建堆的实现 public BinaryHeap(E[] elements, Comparator\u003cE\u003e comparator) { super(comparator); if (elements == null || elements.length == 0) { this.elements = (E[]) new Object[DEFAULT_CAPACITY]; } else { size = elements.length; int capacity = Math.max(elements.length, DEFAULT_CAPACITY); this.elements = (E[]) new Object[capacity]; for (int i = 0; i \u003c elements.length; i++) { this.elements[i] = elements[i]; } heapify(); } } public BinaryHeap(E[] elements) { this(elements, null); } /** * 批量建堆 */ private void heapify() { // 自上而下的上滤 // for (int i = 1; i \u003c size; i++) { // siftUp(i); // } // 自下而上的下滤 for (int i = (size \u003e\u003e 1) - 1; i \u003e= 0; i--) { siftDown(i); } } 如何构建小顶堆 修改比较的逻辑即可 Top K 问题 从 n 个整数种，找出最大的前 k 个数（k 远远小于 n） 如果使用排序算法进行全排序，需要 O(nlog n) 的时间复杂度 如果使用二叉堆来解决，可以使用 O(nlog k) 的时间复杂度解决 新建一个小顶堆 扫描 n 个整数 先将遍历得到的前 k 个数放入堆中 从第 k + 1 个数开始，如果大于堆顶元素，就用 replace 操作（删除堆顶元素，将第k+1个数添加到堆中） 扫描完毕后，堆中剩下的就是最大的前 k 个数 如果是找出最小的前 k 个数呢？ 用大顶堆 如果小于堆顶元素，就使用 replace 操作 ","date":"2020-09-24","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-15-%E4%BA%8C%E5%8F%89%E5%A0%86/:6:4","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-15-二叉堆","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-15-%E4%BA%8C%E5%8F%89%E5%A0%86/"},{"categories":["算法"],"content":"哈希表 ","date":"2020-09-22","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-14-%E5%93%88%E5%B8%8C%E8%A1%A8/:1:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-14-哈希表","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-14-%E5%93%88%E5%B8%8C%E8%A1%A8/"},{"categories":["算法"],"content":"TreeMap 分析 时间复杂度（平均） 添加、删除、搜索：O(log n) 特点 key 必须具备可比较性（TreeMap是由红黑树实现的） 元素的分布是有顺序的 在实际应用中，很多时候的需求 Map 中存储的元素不需要讲究顺序 Map 中的 key 不需要具备可比较性 不考虑顺序、不考虑 key 的可比较性，Map 有更好的实现方案，平均复杂度可以达到 O(1) 那就是采取哈希表来实现 Map ","date":"2020-09-22","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-14-%E5%93%88%E5%B8%8C%E8%A1%A8/:1:1","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-14-哈希表","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-14-%E5%93%88%E5%B8%8C%E8%A1%A8/"},{"categories":["算法"],"content":"需求 设计一个写字楼通讯录，存放所有公司的通讯信息 座机号码作为 key（假设座机号码长度是8位），公司详情（名称、地址等）作为 value 添加、删除、搜索的时间复杂度要求是 O(1) 模拟实现 private Company[] companies = new Company[100000000]; public void add(int phone, Company company){ companies[phone] = company; } public void remove(int phone){ companies[phone] = null; } public Company get(int phone){ return companies[phone]; } 存在什么问题？ 空间复杂度非常大 空间利用率极其低，非常浪费内存空间 其实数组 companies 就是一个哈希表，典型的空间换时间 ","date":"2020-09-22","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-14-%E5%93%88%E5%B8%8C%E8%A1%A8/:1:2","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-14-哈希表","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-14-%E5%93%88%E5%B8%8C%E8%A1%A8/"},{"categories":["算法"],"content":"哈希表（Hash Table） 哈希表也叫做散列表（hash 有“剁碎”的意思） 它是如何实现高效处理数据的？ put(“Jack”, 666); put(“Rose”, 777); put(“Kate”, 888); 添加、搜索、删除的流程都是类似的 利用哈希函数生成 key 对应的 index，时间复杂度 O(1) 根据 index 操作定位数组元素，时间复杂度 O(1) 哈希表是空间换时间的典型应用 哈希函数，也叫做散列函数 哈希表内部的数组元素，很多地方也叫 Bucket（桶），整个数组叫 Buckets 或者 Bucket Array ","date":"2020-09-22","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-14-%E5%93%88%E5%B8%8C%E8%A1%A8/:1:3","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-14-哈希表","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-14-%E5%93%88%E5%B8%8C%E8%A1%A8/"},{"categories":["算法"],"content":"哈希冲突（Hash Collision） 哈希冲突也叫做哈希碰撞 2 个不同的 key，经过哈希函数计算出相同的结果 key1 ≠ key2， hash(key1) = hash(key2) 解决哈希冲突的常见方法 开放定址法（Open Addressing） 按照一定规则向其他地址探测，直到遇到空桶 再哈希法（Re-Hashing） 设计多个哈希函数 链地址法（Separate Chaining） 比如通过链表将同一 index 的元素串起来 ","date":"2020-09-22","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-14-%E5%93%88%E5%B8%8C%E8%A1%A8/:1:4","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-14-哈希表","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-14-%E5%93%88%E5%B8%8C%E8%A1%A8/"},{"categories":["算法"],"content":"JDK1.8 的哈希冲突解决方案 默认使用单向链表将元素串起来 在添加元素时，可能会由单向链表转为红黑树来存储元素 比如当哈希表容量 \u003e= 64 且单向链表的节点数量大于 8 时 当红黑树节点数量少到一定程度时，又会转为单向链表 JDK1.8中的哈希表是使用链表+红黑树解决哈希冲突的 思考：这里为什么使用单链表？ 每次都是从头节点开始遍历 单向链表比双向链表少一个指针，可以节省内存空间 ","date":"2020-09-22","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-14-%E5%93%88%E5%B8%8C%E8%A1%A8/:1:5","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-14-哈希表","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-14-%E5%93%88%E5%B8%8C%E8%A1%A8/"},{"categories":["算法"],"content":"哈希函数 哈希表中哈希函数的实现步骤大概如下 先生成 key 的哈希值（必须是整数） 再让 key 的哈希值跟数组的大小进行相关运算，生成一个索引值 public int hash(Object key){ return hash_code(key) % table.length; } 为了提高效率，可以使用 \u0026 位运算取代 % 运算（前提：将数组长度设计为 2 的幂，即 2n） public int hash(Object key){ return hash_code(key) \u0026 (table.length - 1); } 良好的哈希函数 让哈希值更加均匀分布 -\u003e 减少哈希冲突次数 -\u003e 提升哈希表的性能 ","date":"2020-09-22","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-14-%E5%93%88%E5%B8%8C%E8%A1%A8/:1:6","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-14-哈希表","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-14-%E5%93%88%E5%B8%8C%E8%A1%A8/"},{"categories":["算法"],"content":"如何生成 key 的哈希值 key 的常见种类有 整数、浮点数、字符串、自定义对象 不同种类的 key，哈希值的生成方式不一样，但目标是一致的 尽量让每个 key 的哈希值是唯一的 尽量让 key 的所有信息参与运算 在 Java 中，HashMap 的 key 必须实现 hashCode、equals 方法，也允许 key 为 null Java 平台的哈希值必须是 int 类型，即 4 个字节，32 位 整数的哈希值 整数值当作哈希值 比如 10 的哈希值就是 10 public static int hashCode(int value){ return value; } 浮点数的哈希值 将存储的二进制格式转为整数值 public static int hashCode(float value){ return floatToIntBits(value); } Long 的哈希值 Long 类型是 8 字节，64 位，超过了 int 的长度 public static int hashCode(long value){ return (int)(value ^ (value \u003e\u003e\u003e 32)); } \u003e\u003e\u003e : 无符号右移 \u003e\u003e\u003e 和 ^ 的作用是？ 高 32 bit 和 低 32 bit 混合计算出 32 bit 的哈希值 充分利用所有信息计算出哈希值 Double 的哈希值 Long 类型是 8 字节，64 位，超过了 int 的长度 public static int hashCode(double value){ long bits = doubleToLongBits(value); return (int)(bits ^ (bits \u003e\u003e\u003e 32)); } 字符串的哈希值 整数 5489 是如何计算出来的？ 5 * 103 + 4 * 102 + 8 * 101 + 9 * 100 字符串是由若干字符串组成的 比如字符串 jack，由 j、a、c、k 四个字符组成（字符的本质就是一个整数） 因此，jack 的哈希值可以表示为 j * n3 + a * n2 + c * n1 + k * n0 在 JDK 中，乘数 n 为 31，为什么使用 31 ？ 31 是一个奇素数，jvm 会将 31 * i 优化成 (i \u003c\u003c 5) - i String string = \"jack\"; int hashCode = 0; int len = string.length(); for(int i=0; i\u003clen; i++){ char c = string.charAt(i); hashCode = 31 * hashCode + c; } String string = \"jack\"; int hashCode = 0; int len = string.length(); for(int i=0; i\u003clen; i++){ char c = string.charAt(i); hashCode = (hashCode \u003c\u003c 5) - hashCode + c; } 关于 31 的探讨 31 * i = (2^5 - 1) * i = i * 2^5 - i = (i«5) - i 31 不仅仅是符合 2^n - 1 ，它是个奇素数（既是奇数，又是素数，也就是质数） 素数和其他数相乘的结果比其他方式更容易产生唯一性，减少哈希冲突 最终选择31是经过观测分布结果后的选择 自定义对象的哈希值 public class Person { private int age; // 10 20 private float height; // 1.55 1.67 private String name; // \"jack\" \"rose\" public Person(int age, float height, String name) { this.age = age; this.height = height; this.name = name; } @Override /** * 用来比较2个对象是否相等 */ public boolean equals(Object obj) { // 内存地址 if (this == obj) return true; if (obj == null || obj.getClass() != getClass()) return false; // if (obj == null || !(obj instanceof Person)) return false; // 比较成员变量 Person person = (Person) obj; return person.age == age \u0026\u0026 person.height == height \u0026\u0026 (person.name == null ? name == null : person.name.equals(name)); } @Override public int hashCode() { int hashCode = Integer.hashCode(age); hashCode = hashCode * 31 + Float.hashCode(height); hashCode = hashCode * 31 + (name != null ? name.hashCode() : 0); return hashCode; } } 思考：哈希值太大，整型溢出怎么办？ 不用作任何处理 自定义对象作为 key 自定义对象作为 key，最好同时重写 hashCode、equals 方法 equals: 用以判定 2 个 key 是否为同一个 key 自反性：对于任何非 null 的 x， x.equals(x) 必须返回 true 对称性：对于任何非 null 的 x、y，如果 y.equals(x) 返回 true，x.equals(y) 必须返回 true 传递性：对于任何非 null 的 x、y、z，如果 x.equals(y)、y.equals(z) 返回 true，那么 x.equals(z) 必须返回 true 一致性：对于任何非 null 的 x、y，只要 equals 的比较操作在对象中所用的信息没有被修改，多次调用 x.equals(y) 就会一致地返回 true，或者一致地返回 false 对于任何非 null 的 x，x.equals(null) 必须返回 false hashCode: 必须保证 equals 为 true 的 2 个 key 的哈希值一样 返回来， hashCode 相等的 key，不一定 equals 为 true 不重写 hashCode 方法只重写 equals 方法会有什么结果？ 可能导致 2 个 equals 为 true 的 key 同时存在哈希表中 hashCode 在计算索引的时候使用，equals 在哈希冲突时比较节点 key 的时候使用 如果两个对象 equals 为 true，那么它们的 hashCode 必须相等（本例中 equals 和 hashCode 都是使用了 age、height、name 计算，所以能保证该性质） 返回来， hashCode 相等的 key，不一定 equals 为 true 哈希值的进一步处理：扰动计算 private int hash(K key){ if(key == null) return 0; int h = key.hashCode(); return (h ^ (h \u003e\u003e\u003e 16)) \u0026 (table.length - 1); } ","date":"2020-09-22","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-14-%E5%93%88%E5%B8%8C%E8%A1%A8/:1:7","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-14-哈希表","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-14-%E5%93%88%E5%B8%8C%E8%A1%A8/"},{"categories":["算法"],"content":"自定义 HashMap 实现 使用数组+红黑树实现自定义 HashMap package com.mj.map; import java.util.LinkedList; import java.util.Objects; import java.util.Queue; import com.mj.printer.BinaryTreeInfo; import com.mj.printer.BinaryTrees; @SuppressWarnings({\"unchecked\", \"rawtypes\"}) public class HashMap_v0\u003cK, V\u003e implements Map\u003cK, V\u003e { private static final boolean RED = false; private static final boolean BLACK = true; private int size; private Node\u003cK, V\u003e[] table; private static final int DEFAULT_CAPACITY = 1 \u003c\u003c 4; public HashMap_v0() { table = new Node[DEFAULT_CAPACITY]; } @Override public int size() { return size; } @Override public boolean isEmpty() { return size == 0; } @Override public void clear() { if (size == 0) return; size = 0; for (int i = 0; i \u003c table.length; i++) { table[i] = null; } } @Override public V put(K key, V value) { int index = index(key); // 取出index位置的红黑树根节点 Node\u003cK, V\u003e root = table[index]; if (root == null) { root = new Node\u003c\u003e(key, value, null); table[index] = root; size++; afterPut(root); return null; } // 添加新的节点到红黑树上面 Node\u003cK, V\u003e parent = root; Node\u003cK, V\u003e node = root; int cmp = 0; K k1 = key; int h1 = k1 == null ? 0 : k1.hashCode(); Node\u003cK, V\u003e result = null; boolean searched = false; // 是否已经搜索过这个key do { parent = node; K k2 = node.key; int h2 = node.hash; if (h1 \u003e h2) { cmp = 1; } else if (h1 \u003c h2) { cmp = -1; } else if (Objects.equals(k1, k2)) { cmp = 0; } else if (k1 != null \u0026\u0026 k2 != null \u0026\u0026 k1.getClass() == k2.getClass() \u0026\u0026 k1 instanceof Comparable \u0026\u0026 (cmp = ((Comparable) k1).compareTo(k2)) != 0) { } else if (searched) { // 已经扫描了 cmp = System.identityHashCode(k1) - System.identityHashCode(k2); } else { // searched == false; 还没有扫描，然后再根据内存地址大小决定左右 if ((node.left != null \u0026\u0026 (result = node(node.left, k1)) != null) || (node.right != null \u0026\u0026 (result = node(node.right, k1)) != null)) { // 已经存在这个key node = result; cmp = 0; } else { // 不存在这个key searched = true; cmp = System.identityHashCode(k1) - System.identityHashCode(k2); } } if (cmp \u003e 0) { node = node.right; } else if (cmp \u003c 0) { node = node.left; } else { // 相等 V oldValue = node.value; node.key = key; node.value = value; node.hash = h1; return oldValue; } } while (node != null); // 看看插入到父节点的哪个位置 Node\u003cK, V\u003e newNode = new Node\u003c\u003e(key, value, parent); if (cmp \u003e 0) { parent.right = newNode; } else { parent.left = newNode; } size++; // 新添加节点之后的处理 afterPut(newNode); return null; } @Override public V get(K key) { Node\u003cK, V\u003e node = node(key); return node != null ? node.value : null; } @Override public V remove(K key) { return remove(node(key)); } @Override public boolean containsKey(K key) { return node(key) != null; } @Override public boolean containsValue(V value) { if (size == 0) return false; Queue\u003cNode\u003cK, V\u003e\u003e queue = new LinkedList\u003c\u003e(); for (int i = 0; i \u003c table.length; i++) { if (table[i] == null) continue; queue.offer(table[i]); while (!queue.isEmpty()) { Node\u003cK, V\u003e node = queue.poll(); if (Objects.equals(value, node.value)) return true; if (node.left != null) { queue.offer(node.left); } if (node.right != null) { queue.offer(node.right); } } } return false; } @Override public void traversal(Visitor\u003cK, V\u003e visitor) { if (size == 0 || visitor == null) return; Queue\u003cNode\u003cK, V\u003e\u003e queue = new LinkedList\u003c\u003e(); for (int i = 0; i \u003c table.length; i++) { if (table[i] == null) continue; queue.offer(table[i]); while (!queue.isEmpty()) { Node\u003cK, V\u003e node = queue.poll(); if (visitor.visit(node.key, node.value)) return; if (node.left != null) { queue.offer(node.left); } if (node.right != null) { queue.offer(node.right); } } } } public void print() { if (size == 0) return; for (int i = 0; i \u003c table.length; i++) { final Node\u003cK, V\u003e root = table[i]; System.out.println(\"【index = \" + i + \"】\"); BinaryTrees.println(new BinaryTreeInfo() { @Override public Object string(Object node) { return node; } @Override public Object root() { return root; } @Override public Object right(Object node) { return ((Node\u003cK, V\u003e)node).right; } @Override public Object left(Object node) { return ((Node\u003cK, V\u003e)node).left; } }); System.o","date":"2020-09-22","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-14-%E5%93%88%E5%B8%8C%E8%A1%A8/:1:8","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-14-哈希表","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-14-%E5%93%88%E5%B8%8C%E8%A1%A8/"},{"categories":["算法"],"content":"扩容 ","date":"2020-09-22","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-14-%E5%93%88%E5%B8%8C%E8%A1%A8/:2:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-14-哈希表","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-14-%E5%93%88%E5%B8%8C%E8%A1%A8/"},{"categories":["算法"],"content":"装填因子 装填因子（Load Factor）：节点总数量/哈希表桶数组长度，也叫做负载因子 在 jdk1.8 的 HashMap 中，如果装填因子超过0.75，就扩容为原来的2倍 ","date":"2020-09-22","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-14-%E5%93%88%E5%B8%8C%E8%A1%A8/:2:1","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-14-哈希表","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-14-%E5%93%88%E5%B8%8C%E8%A1%A8/"},{"categories":["算法"],"content":"具有扩容功能的哈希表 package com.mj.map; import java.util.LinkedList; import java.util.Objects; import java.util.Queue; import com.mj.printer.BinaryTreeInfo; import com.mj.printer.BinaryTrees; @SuppressWarnings({\"unchecked\", \"rawtypes\"}) public class HashMap\u003cK, V\u003e implements Map\u003cK, V\u003e { private static final boolean RED = false; private static final boolean BLACK = true; private int size; private Node\u003cK, V\u003e[] table; private static final int DEFAULT_CAPACITY = 1 \u003c\u003c 4; private static final float DEFAULT_LOAD_FACTOR = 0.75f; public HashMap() { table = new Node[DEFAULT_CAPACITY]; } @Override public int size() { return size; } @Override public boolean isEmpty() { return size == 0; } @Override public void clear() { if (size == 0) return; size = 0; for (int i = 0; i \u003c table.length; i++) { table[i] = null; } } @Override public V put(K key, V value) { resize(); int index = index(key); // 取出index位置的红黑树根节点 Node\u003cK, V\u003e root = table[index]; if (root == null) { root = createNode(key, value, null); table[index] = root; size++; fixAfterPut(root); return null; } // 添加新的节点到红黑树上面 Node\u003cK, V\u003e parent = root; Node\u003cK, V\u003e node = root; int cmp = 0; K k1 = key; int h1 = hash(k1); Node\u003cK, V\u003e result = null; boolean searched = false; // 是否已经搜索过这个key do { parent = node; K k2 = node.key; int h2 = node.hash; if (h1 \u003e h2) { cmp = 1; } else if (h1 \u003c h2) { cmp = -1; } else if (Objects.equals(k1, k2)) { cmp = 0; } else if (k1 != null \u0026\u0026 k2 != null \u0026\u0026 k1 instanceof Comparable \u0026\u0026 k1.getClass() == k2.getClass() \u0026\u0026 (cmp = ((Comparable)k1).compareTo(k2)) != 0) { } else if (searched) { // 已经扫描了 cmp = System.identityHashCode(k1) - System.identityHashCode(k2); } else { // searched == false; 还没有扫描，然后再根据内存地址大小决定左右 if ((node.left != null \u0026\u0026 (result = node(node.left, k1)) != null) || (node.right != null \u0026\u0026 (result = node(node.right, k1)) != null)) { // 已经存在这个key node = result; cmp = 0; } else { // 不存在这个key searched = true; cmp = System.identityHashCode(k1) - System.identityHashCode(k2); } } if (cmp \u003e 0) { node = node.right; } else if (cmp \u003c 0) { node = node.left; } else { // 相等 V oldValue = node.value; node.key = key; node.value = value; node.hash = h1; return oldValue; } } while (node != null); // 看看插入到父节点的哪个位置 Node\u003cK, V\u003e newNode = createNode(key, value, parent); if (cmp \u003e 0) { parent.right = newNode; } else { parent.left = newNode; } size++; // 新添加节点之后的处理 fixAfterPut(newNode); return null; } @Override public V get(K key) { Node\u003cK, V\u003e node = node(key); return node != null ? node.value : null; } @Override public V remove(K key) { return remove(node(key)); } @Override public boolean containsKey(K key) { return node(key) != null; } @Override public boolean containsValue(V value) { if (size == 0) return false; Queue\u003cNode\u003cK, V\u003e\u003e queue = new LinkedList\u003c\u003e(); for (int i = 0; i \u003c table.length; i++) { if (table[i] == null) continue; queue.offer(table[i]); while (!queue.isEmpty()) { Node\u003cK, V\u003e node = queue.poll(); if (Objects.equals(value, node.value)) return true; if (node.left != null) { queue.offer(node.left); } if (node.right != null) { queue.offer(node.right); } } } return false; } @Override public void traversal(Visitor\u003cK, V\u003e visitor) { if (size == 0 || visitor == null) return; Queue\u003cNode\u003cK, V\u003e\u003e queue = new LinkedList\u003c\u003e(); for (int i = 0; i \u003c table.length; i++) { if (table[i] == null) continue; queue.offer(table[i]); while (!queue.isEmpty()) { Node\u003cK, V\u003e node = queue.poll(); if (visitor.visit(node.key, node.value)) return; if (node.left != null) { queue.offer(node.left); } if (node.right != null) { queue.offer(node.right); } } } } public void print() { if (size == 0) return; for (int i = 0; i \u003c table.length; i++) { final Node\u003cK, V\u003e root = table[i]; System.out.println(\"【index = \" + i + \"】\"); BinaryTrees.println(new BinaryTreeInfo() { @Override public Object string(Object node) { return node; } @Override public Object root() { return root; } @Override public Object right(Object node) { return ((Node\u003cK, V\u003e)node).right; } @Override public Object left(Object node) { return ((Node\u003cK, V\u003e)node).lef","date":"2020-09-22","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-14-%E5%93%88%E5%B8%8C%E8%A1%A8/:2:2","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-14-哈希表","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-14-%E5%93%88%E5%B8%8C%E8%A1%A8/"},{"categories":["算法"],"content":"TreeMap vs HashMap 何时选择 TreeMap 元素具备可比较性且要求升序遍历（元素从小到大） 何时选择 HashMap 无序遍历 ","date":"2020-09-22","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-14-%E5%93%88%E5%B8%8C%E8%A1%A8/:3:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-14-哈希表","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-14-%E5%93%88%E5%B8%8C%E8%A1%A8/"},{"categories":["算法"],"content":"关于使用%计算索引 如何使用%来计算索引 建议把哈希表的长度设计为素数（质数） 可以大大减少哈希冲突 下表列出了不同数据规模对应的最佳素数，特点如下 每个素数略小于前一个素数的2倍 每个素数尽可能接近2的幂（22） ","date":"2020-09-22","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-14-%E5%93%88%E5%B8%8C%E8%A1%A8/:4:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-14-哈希表","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-14-%E5%93%88%E5%B8%8C%E8%A1%A8/"},{"categories":["算法"],"content":"LinkedHashMap 在HashMap的基础上维护元素的添加顺序，使得遍历的结果是遵从添加顺序的 假设添加顺序是 37、21、31、41、97、95、52、42、83 删除度为2的节点node时 需要注意更换node与前驱\\后继节点的连接位置 ","date":"2020-09-22","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-14-%E5%93%88%E5%B8%8C%E8%A1%A8/:5:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-14-哈希表","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-14-%E5%93%88%E5%B8%8C%E8%A1%A8/"},{"categories":["算法"],"content":"实现 package com.mj.map; import java.util.Objects; @SuppressWarnings({ \"rawtypes\", \"unchecked\" }) public class LinkedHashMap\u003cK, V\u003e extends HashMap\u003cK, V\u003e { private LinkedNode\u003cK, V\u003e first; private LinkedNode\u003cK, V\u003e last; @Override public void clear() { super.clear(); first = null; last = null; } @Override public boolean containsValue(V value) { LinkedNode\u003cK, V\u003e node = first; while (node != null) { if (Objects.equals(value, node.value)) return true; node = node.next; } return false; } @Override public void traversal(Visitor\u003cK, V\u003e visitor) { if (visitor == null) return; LinkedNode\u003cK, V\u003e node = first; while (node != null) { if (visitor.visit(node.key, node.value)) return; node = node.next; } } @Override protected void afterRemove(Node\u003cK, V\u003e willNode, Node\u003cK, V\u003e removedNode) { LinkedNode\u003cK, V\u003e node1 = (LinkedNode\u003cK, V\u003e) willNode; LinkedNode\u003cK, V\u003e node2 = (LinkedNode\u003cK, V\u003e) removedNode; if (node1 != node2) { // 交换linkedWillNode和linkedRemovedNode在链表中的位置 // 交换prev LinkedNode\u003cK, V\u003e tmp = node1.prev; node1.prev = node2.prev; node2.prev = tmp; if (node1.prev == null) { first = node1; } else { node1.prev.next = node1; } if (node2.prev == null) { first = node2; } else { node2.prev.next = node2; } // 交换next tmp = node1.next; node1.next = node2.next; node2.next = tmp; if (node1.next == null) { last = node1; } else { node1.next.prev = node1; } if (node2.next == null) { last = node2; } else { node2.next.prev = node2; } } LinkedNode\u003cK, V\u003e prev = node2.prev; LinkedNode\u003cK, V\u003e next = node2.next; if (prev == null) { first = next; } else { prev.next = next; } if (next == null) { last = prev; } else { next.prev = prev; } } @Override protected Node\u003cK, V\u003e createNode(K key, V value, Node\u003cK, V\u003e parent) { LinkedNode node = new LinkedNode(key, value, parent); if (first == null) { first = last = node; } else { last.next = node; node.prev = last; last = node; } return node; } private static class LinkedNode\u003cK, V\u003e extends Node\u003cK, V\u003e { LinkedNode\u003cK, V\u003e prev; LinkedNode\u003cK, V\u003e next; public LinkedNode(K key, V value, Node\u003cK, V\u003e parent) { super(key, value, parent); } } } ","date":"2020-09-22","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-14-%E5%93%88%E5%B8%8C%E8%A1%A8/:5:1","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-14-哈希表","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-14-%E5%93%88%E5%B8%8C%E8%A1%A8/"},{"categories":["算法"],"content":"映射 Map 在有些编程语言中也叫做字典（dictionary，比如 Python、Objective-C、Swift 等） Map 的每一个 key 是唯一的 ","date":"2020-09-22","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-13-%E6%98%A0%E5%B0%84/:1:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-13-映射","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-13-%E6%98%A0%E5%B0%84/"},{"categories":["算法"],"content":"Map 的接口设计 public interface Map\u003cK, V\u003e { int size(); boolean isEmpty(); void clear(); V put(K key, V value); V get(K key); V remove(K key); boolean containsKey(K key); boolean containsValue(V value); void traversal(Visitor\u003cK, V\u003e visitor); public static abstract class Visitor\u003cK, V\u003e { boolean stop; public abstract boolean visit(K key, V value); } } 类似 Set，Map 可以直接利用之前学习的链表、二叉搜索树（AVL树、红黑树）等数据结构来实现 使用红黑树实现 TreeMap package com.mj.map; import java.util.Comparator; import java.util.LinkedList; import java.util.Queue; @SuppressWarnings({\"unchecked\", \"unused\"}) public class TreeMap\u003cK, V\u003e implements Map\u003cK, V\u003e { private static final boolean RED = false; private static final boolean BLACK = true; private int size; private Node\u003cK, V\u003e root; private Comparator\u003cK\u003e comparator; public TreeMap() { this(null); } public TreeMap(Comparator\u003cK\u003e comparator) { this.comparator = comparator; } public int size() { return size; } public boolean isEmpty() { return size == 0; } public void clear() { root = null; size = 0; } @Override public V put(K key, V value) { keyNotNullCheck(key); // 添加第一个节点 if (root == null) { root = new Node\u003c\u003e(key, value, null); size++; // 新添加节点之后的处理 afterPut(root); return null; } // 添加的不是第一个节点 // 找到父节点 Node\u003cK, V\u003e parent = root; Node\u003cK, V\u003e node = root; int cmp = 0; do { cmp = compare(key, node.key); parent = node; if (cmp \u003e 0) { node = node.right; } else if (cmp \u003c 0) { node = node.left; } else { // 相等 node.key = key; V oldValue = node.value; node.value = value; return oldValue; } } while (node != null); // 看看插入到父节点的哪个位置 Node\u003cK, V\u003e newNode = new Node\u003c\u003e(key, value, parent); if (cmp \u003e 0) { parent.right = newNode; } else { parent.left = newNode; } size++; // 新添加节点之后的处理 afterPut(newNode); return null; } @Override public V get(K key) { Node\u003cK, V\u003e node = node(key); return node != null ? node.value : null; } @Override public V remove(K key) { return remove(node(key)); } @Override public boolean containsKey(K key) { return node(key) != null; } @Override public boolean containsValue(V value) { if (root == null) return false; Queue\u003cNode\u003cK, V\u003e\u003e queue = new LinkedList\u003c\u003e(); queue.offer(root); while (!queue.isEmpty()) { Node\u003cK, V\u003e node = queue.poll(); if (valEquals(value, node.value)) return true; if (node.left != null) { queue.offer(node.left); } if (node.right != null) { queue.offer(node.right); } } return false; } @Override public void traversal(Visitor\u003cK, V\u003e visitor) { if (visitor == null) return; traversal(root, visitor); } private void traversal(Node\u003cK, V\u003e node, Visitor\u003cK, V\u003e visitor) { if (node == null || visitor.stop) return; traversal(node.left, visitor); if (visitor.stop) return; visitor.visit(node.key, node.value); traversal(node.right, visitor); } private boolean valEquals(V v1, V v2) { return v1 == null ? v2 == null : v1.equals(v2); } private V remove(Node\u003cK, V\u003e node) { if (node == null) return null; size--; V oldValue = node.value; if (node.hasTwoChildren()) { // 度为2的节点 // 找到后继节点 Node\u003cK, V\u003e s = successor(node); // 用后继节点的值覆盖度为2的节点的值 node.key = s.key; node.value = s.value; // 删除后继节点 node = s; } // 删除node节点（node的度必然是1或者0） Node\u003cK, V\u003e replacement = node.left != null ? node.left : node.right; if (replacement != null) { // node是度为1的节点 // 更改parent replacement.parent = node.parent; // 更改parent的left、right的指向 if (node.parent == null) { // node是度为1的节点并且是根节点 root = replacement; } else if (node == node.parent.left) { node.parent.left = replacement; } else { // node == node.parent.right node.parent.right = replacement; } // 删除节点之后的处理 afterRemove(replacement); } else if (node.parent == null) { // node是叶子节点并且是根节点 root = null; } else { // node是叶子节点，但不是根节点 if (node == node.parent.left) { node.parent.left = null; } else { // node == node.parent.right node.parent.right = null; } // 删除节点之后的处理 afterRemove(node); } return oldValue; } private void afterRemove(Node\u003cK, V\u003e node) { // 如果删除的节点是红色 // 或者 用以取代删除节点的子节点是红色 if (isRed(node)) { black(node); return; } Node\u003cK, V\u003e parent = node.parent; if (parent == null) return; // 删除的是黑色叶子节点【下溢】 // 判断被删除的node是左还是右 boolean lef","date":"2020-09-22","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-13-%E6%98%A0%E5%B0%84/:2:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-13-映射","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-13-%E6%98%A0%E5%B0%84/"},{"categories":["算法"],"content":"Map 与 Set Map 的所有 key 组合在一起，其实就是一个 Set 因此，Set 可以间接利用 Map 来作内部实现 使用 TreeMap 实现 TreeSet package com.mj.set; public interface Set\u003cE\u003e { int size(); boolean isEmpty(); void clear(); boolean contains(E element); void add(E element); void remove(E element); void traversal(Visitor\u003cE\u003e visitor); public static abstract class Visitor\u003cE\u003e { boolean stop; public abstract boolean visit(E element); } } package com.mj.set; import com.mj.map.Map; import com.mj.map.TreeMap; public class TreeSet\u003cE\u003e implements Set\u003cE\u003e { Map\u003cE, Object\u003e map = new TreeMap\u003c\u003e(); @Override public int size() { return map.size(); } @Override public boolean isEmpty() { return map.isEmpty(); } @Override public void clear() { map.clear(); } @Override public boolean contains(E element) { return map.containsKey(element); } @Override public void add(E element) { map.put(element, null); } @Override public void remove(E element) { map.remove(element); } @Override public void traversal(Visitor\u003cE\u003e visitor) { map.traversal(new Map.Visitor\u003cE, Object\u003e() { public boolean visit(E key, Object value) { return visitor.visit(key); } }); } } java 自带的 TreeSet 的内部实现也是使用了 TreeMap ","date":"2020-09-22","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-13-%E6%98%A0%E5%B0%84/:3:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-13-映射","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-13-%E6%98%A0%E5%B0%84/"},{"categories":["算法"],"content":"集合（set） 集合的特点 不存放重复的元素 常用于去重 存放新增IP，统计新增IP量 存放词汇，统计词汇量 …… 集合的接口 public interface Set\u003cE\u003e { int size(); boolean isEmpty(); void clear(); boolean contains(E element); void add(E element); void remove(E element); void traversal(Visitor\u003cE\u003e visitor); public static abstract class Visitor\u003cE\u003e { boolean stop; public abstract boolean visit(E element); } } 思考：集合的内部实现能否直接利用以前学过的数据结构？ 动态数组 链表 二叉搜索树（AVL树、红黑树）: 元素必须具备可比较性（如果元素不具备可比较性，又想要红黑树的性能，可以使用哈希表） ","date":"2020-09-22","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-12-%E9%9B%86%E5%90%88/:1:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-12-集合","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-12-%E9%9B%86%E5%90%88/"},{"categories":["算法"],"content":"集合的实现 集合的接口 package com.mj.set; public interface Set\u003cE\u003e { int size(); boolean isEmpty(); void clear(); boolean contains(E element); void add(E element); void remove(E element); void traversal(Visitor\u003cE\u003e visitor); public static abstract class Visitor\u003cE\u003e { boolean stop; public abstract boolean visit(E element); } } ","date":"2020-09-22","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-12-%E9%9B%86%E5%90%88/:2:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-12-集合","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-12-%E9%9B%86%E5%90%88/"},{"categories":["算法"],"content":"使用链表实现集合 自定义链表相关代码 package com.mj.list; public interface List\u003cE\u003e { static final int ELEMENT_NOT_FOUND = -1; /** * 清除所有元素 */ void clear(); /** * 元素的数量 * @return */ int size(); /** * 是否为空 * @return */ boolean isEmpty(); /** * 是否包含某个元素 * @param element * @return */ boolean contains(E element); /** * 添加元素到尾部 * @param element */ void add(E element); /** * 获取index位置的元素 * @param index * @return */ E get(int index); /** * 设置index位置的元素 * @param index * @param element * @return 原来的元素ֵ */ E set(int index, E element); /** * 在index位置插入一个元素 * @param index * @param element */ void add(int index, E element); /** * 删除index位置的元素 * @param index * @return */ E remove(int index); /** * 查看元素的索引 * @param element * @return */ int indexOf(E element); } package com.mj.list; public abstract class AbstractList\u003cE\u003e implements List\u003cE\u003e { /** * 元素的数量 */ protected int size; /** * 元素的数量 * @return */ public int size() { return size; } /** * 是否为空 * @return */ public boolean isEmpty() { return size == 0; } /** * 是否包含某个元素 * @param element * @return */ public boolean contains(E element) { return indexOf(element) != ELEMENT_NOT_FOUND; } /** * 添加元素到尾部 * @param element */ public void add(E element) { add(size, element); } protected void outOfBounds(int index) { throw new IndexOutOfBoundsException(\"Index:\" + index + \", Size:\" + size); } protected void rangeCheck(int index) { if (index \u003c 0 || index \u003e= size) { outOfBounds(index); } } protected void rangeCheckForAdd(int index) { if (index \u003c 0 || index \u003e size) { outOfBounds(index); } } } package com.mj.list; public class LinkedList\u003cE\u003e extends AbstractList\u003cE\u003e { private Node\u003cE\u003e first; private Node\u003cE\u003e last; private static class Node\u003cE\u003e { E element; Node\u003cE\u003e prev; Node\u003cE\u003e next; public Node(Node\u003cE\u003e prev, E element, Node\u003cE\u003e next) { this.prev = prev; this.element = element; this.next = next; } @Override public String toString() { StringBuilder sb = new StringBuilder(); if (prev != null) { sb.append(prev.element); } else { sb.append(\"null\"); } sb.append(\"_\").append(element).append(\"_\"); if (next != null) { sb.append(next.element); } else { sb.append(\"null\"); } return sb.toString(); } } @Override public void clear() { size = 0; first = null; last = null; } @Override public E get(int index) { return node(index).element; } @Override public E set(int index, E element) { Node\u003cE\u003e node = node(index); E old = node.element; node.element = element; return old; } @Override public void add(int index, E element) { rangeCheckForAdd(index); // size == 0 // index == 0 if (index == size) { // 往最后面添加元素 Node\u003cE\u003e oldLast = last; last = new Node\u003c\u003e(oldLast, element, null); if (oldLast == null) { // 这是链表添加的第一个元素 first = last; } else { oldLast.next = last; } } else { Node\u003cE\u003e next = node(index); Node\u003cE\u003e prev = next.prev; Node\u003cE\u003e node = new Node\u003c\u003e(prev, element, next); next.prev = node; if (prev == null) { // index == 0 first = node; } else { prev.next = node; } } size++; } @Override public E remove(int index) { rangeCheck(index); Node\u003cE\u003e node = node(index); Node\u003cE\u003e prev = node.prev; Node\u003cE\u003e next = node.next; if (prev == null) { // index == 0 first = next; } else { prev.next = next; } if (next == null) { // index == size - 1 last = prev; } else { next.prev = prev; } size--; return node.element; } @Override public int indexOf(E element) { if (element == null) { Node\u003cE\u003e node = first; for (int i = 0; i \u003c size; i++) { if (node.element == null) return i; node = node.next; } } else { Node\u003cE\u003e node = first; for (int i = 0; i \u003c size; i++) { if (element.equals(node.element)) return i; node = node.next; } } return ELEMENT_NOT_FOUND; } /** * 获取index位置对应的节点对象 * @param index * @return */ private Node\u003cE\u003e node(int index) { rangeCheck(index); if (index \u003c (size \u003e\u003e 1)) { Node\u003cE\u003e node = first; for (int i = 0; i \u003c index; i++) { node = node.next; } return node; } else { Node\u003cE\u003e node = last; for (int i = size - 1; i \u003e index; i--) { node = node.prev; } return node; } } @Override public String toString() { StringBuilder string = new StringBuilder(); string.append(\"size=\").append(size","date":"2020-09-22","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-12-%E9%9B%86%E5%90%88/:2:1","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-12-集合","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-12-%E9%9B%86%E5%90%88/"},{"categories":["算法"],"content":"使用红黑树实现集合 自定义红黑树相关代码 package com.mj.tree; import java.util.LinkedList; import java.util.Queue; public class BinaryTree\u003cE\u003e { protected int size; protected Node\u003cE\u003e root; public int size() { return size; } public boolean isEmpty() { return size == 0; } public void clear() { root = null; size = 0; } public void preorder(Visitor\u003cE\u003e visitor) { if (visitor == null) return; preorder(root, visitor); } private void preorder(Node\u003cE\u003e node, Visitor\u003cE\u003e visitor) { if (node == null || visitor.stop) return; visitor.stop = visitor.visit(node.element); preorder(node.left, visitor); preorder(node.right, visitor); } public void inorder(Visitor\u003cE\u003e visitor) { if (visitor == null) return; inorder(root, visitor); } private void inorder(Node\u003cE\u003e node, Visitor\u003cE\u003e visitor) { if (node == null || visitor.stop) return; inorder(node.left, visitor); if (visitor.stop) return; visitor.stop = visitor.visit(node.element); inorder(node.right, visitor); } public void postorder(Visitor\u003cE\u003e visitor) { if (visitor == null) return; postorder(root, visitor); } private void postorder(Node\u003cE\u003e node, Visitor\u003cE\u003e visitor) { if (node == null || visitor.stop) return; postorder(node.left, visitor); postorder(node.right, visitor); if (visitor.stop) return; visitor.stop = visitor.visit(node.element); } public void levelOrder(Visitor\u003cE\u003e visitor) { if (root == null || visitor == null) return; Queue\u003cNode\u003cE\u003e\u003e queue = new LinkedList\u003c\u003e(); queue.offer(root); while (!queue.isEmpty()) { Node\u003cE\u003e node = queue.poll(); if (visitor.visit(node.element)) return; if (node.left != null) { queue.offer(node.left); } if (node.right != null) { queue.offer(node.right); } } } public boolean isComplete() { if (root == null) return false; Queue\u003cNode\u003cE\u003e\u003e queue = new LinkedList\u003c\u003e(); queue.offer(root); boolean leaf = false; while (!queue.isEmpty()) { Node\u003cE\u003e node = queue.poll(); if (leaf \u0026\u0026 !node.isLeaf()) return false; if (node.left != null) { queue.offer(node.left); } else if (node.right != null) { return false; } if (node.right != null) { queue.offer(node.right); } else { // 后面遍历的节点都必须是叶子节点 leaf = true; } } return true; } public int height() { if (root == null) return 0; // 树的高度 int height = 0; // 存储着每一层的元素数量 int levelSize = 1; Queue\u003cNode\u003cE\u003e\u003e queue = new LinkedList\u003c\u003e(); queue.offer(root); while (!queue.isEmpty()) { Node\u003cE\u003e node = queue.poll(); levelSize--; if (node.left != null) { queue.offer(node.left); } if (node.right != null) { queue.offer(node.right); } if (levelSize == 0) { // 意味着即将要访问下一层 levelSize = queue.size(); height++; } } return height; } public int height2() { return height(root); } private int height(Node\u003cE\u003e node) { if (node == null) return 0; return 1 + Math.max(height(node.left), height(node.right)); } protected Node\u003cE\u003e createNode(E element, Node\u003cE\u003e parent) { return new Node\u003c\u003e(element, parent); } protected Node\u003cE\u003e predecessor(Node\u003cE\u003e node) { if (node == null) return null; // 前驱节点在左子树当中（left.right.right.right....） Node\u003cE\u003e p = node.left; if (p != null) { while (p.right != null) { p = p.right; } return p; } // 从父节点、祖父节点中寻找前驱节点 while (node.parent != null \u0026\u0026 node == node.parent.left) { node = node.parent; } // node.parent == null // node == node.parent.right return node.parent; } protected Node\u003cE\u003e successor(Node\u003cE\u003e node) { if (node == null) return null; // 前驱节点在左子树当中（right.left.left.left....） Node\u003cE\u003e p = node.right; if (p != null) { while (p.left != null) { p = p.left; } return p; } // 从父节点、祖父节点中寻找前驱节点 while (node.parent != null \u0026\u0026 node == node.parent.right) { node = node.parent; } return node.parent; } public static abstract class Visitor\u003cE\u003e { boolean stop; /** * @return 如果返回true，就代表停止遍历 */ public abstract boolean visit(E element); } protected static class Node\u003cE\u003e { E element; Node\u003cE\u003e left; Node\u003cE\u003e right; Node\u003cE\u003e parent; public Node(E element, Node\u003cE\u003e parent) { this.element = element; this.parent = parent; } public boolean isLeaf() { return left == null \u0026\u0026 right == null; } public boolean hasTwoChildren() { return left != null \u0026\u0026 right != null; } public boolean isLeftChild() { return parent != nul","date":"2020-09-22","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-12-%E9%9B%86%E5%90%88/:2:2","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-12-集合","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-12-%E9%9B%86%E5%90%88/"},{"categories":["算法"],"content":"红黑树（Red Black Tree） 红黑树也是一种自平衡的二叉搜索树 以前也叫做平衡二叉 B 树（Symmetric Binary B-tree） 红黑树必须满足以下 5 条性质 节点是 RED 或者 BLACK 根节点是 BLACK 叶子节点（外部节点、空节点）都是 BLACK RED 节点的子节点都是 BLACK RED 节点的 parent 都是 BLACK 从根节点到叶子节点的所有路径上不能有 2 个连续的 RED 节点 从任一节点到叶子节点的所有路径都包含相同数目的 BLACK 节点 为何在这些规则下，就能保证平衡？ 请问下面这棵是红黑树吗？ 不是红黑树，不符合第 5 条规律，38 节点右侧有个空的叶子节点，该路径只有 3 个黑色节点（包含空的叶子节点），其他路径都是 4 个黑色节点 ","date":"2020-09-21","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-11-%E7%BA%A2%E9%BB%91%E6%A0%91/:1:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-11-红黑树","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-11-%E7%BA%A2%E9%BB%91%E6%A0%91/"},{"categories":["算法"],"content":"红黑树的等价变换 红黑树 和 4阶B树（2-3-4树）具有等价性 BLACK 节点与它的 RED 子节点融合在一起，形成 1 个 B 树节点 红黑树的 BLACK 节点个数 与 4阶B树 的节点总个数相等 网上有些教程：用2-3树 与 红黑树 进行类比，这是极其不严谨的，2-3树 并不能完美匹配红黑树的各种情况 注意：后面展示的红黑树都会省略 NULL 节点 ","date":"2020-09-21","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-11-%E7%BA%A2%E9%BB%91%E6%A0%91/:2:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-11-红黑树","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-11-%E7%BA%A2%E9%BB%91%E6%A0%91/"},{"categories":["算法"],"content":"红黑树 vs 2-3-4树 思考：如果上图最底层的 BLACK 节点是不存在的，在 B 树中是什么样的情形？ 整棵 B 树只有 1 个节点，而且是超级节点 ","date":"2020-09-21","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-11-%E7%BA%A2%E9%BB%91%E6%A0%91/:3:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-11-红黑树","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-11-%E7%BA%A2%E9%BB%91%E6%A0%91/"},{"categories":["算法"],"content":"红黑树中单词概念 parent: 父节点 sibling: 兄弟节点 uncle: 叔父节点（ parent 的兄弟节点） grand: 祖父节点（ parent 的父节点） ","date":"2020-09-21","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-11-%E7%BA%A2%E9%BB%91%E6%A0%91/:4:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-11-红黑树","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-11-%E7%BA%A2%E9%BB%91%E6%A0%91/"},{"categories":["算法"],"content":"添加 已知 B 树中，新元素必定是添加到叶子节点中 4阶B树所有节点的元素个数 x 都符合 1 \u003c= x \u003c= 3 建议新添加的节点默认为 RED，这样能够让红黑树的性质尽快满足（性质 1、2、3、5 都满足，性质 4 不一定） 如果添加的是根节点，染成 BLACK 即可 红黑树添加节点的所有情况 有 4 种情况满足红黑树的性质 4：parent 为 BLACK 同样也满足 4阶B树 的性质 因此不用做任何额外处理 有 8 种情况不满足红黑树的性质 4：parent 为 RED （Double RED） 其中前 4 种属于 B 树节点上溢的情况 注意：红黑树的平衡不是靠平衡因子保证的，是靠红黑树5条性质保证的，红黑树和AVL树没关系，红黑树节点没有高度的属性。 ","date":"2020-09-21","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-11-%E7%BA%A2%E9%BB%91%E6%A0%91/:5:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-11-红黑树","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-11-%E7%BA%A2%E9%BB%91%E6%A0%91/"},{"categories":["算法"],"content":"添加 - 修复性质4 - LL\\RR 判定条件：uncle 不是 RED parent 染成 BLACK，grand 染成 RED grand 进行单旋操作（对46进行左旋转，对76进行右旋转） LL: 右旋转 RR: 左旋转 ","date":"2020-09-21","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-11-%E7%BA%A2%E9%BB%91%E6%A0%91/:5:1","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-11-红黑树","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-11-%E7%BA%A2%E9%BB%91%E6%A0%91/"},{"categories":["算法"],"content":"添加 - 修复性质4 - LR\\RL 判定条件：uncle 不是 RED 自己染成 BLACK，grand 染成 RED 进行双旋操作 LR: parent 左旋转，grand 右旋转（72左旋转，76右旋转，74成为子树根节点） RL: parent 右旋转，grand 左旋转（50右旋转，46左旋转，48成为子树根节点） ","date":"2020-09-21","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-11-%E7%BA%A2%E9%BB%91%E6%A0%91/:5:2","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-11-红黑树","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-11-%E7%BA%A2%E9%BB%91%E6%A0%91/"},{"categories":["算法"],"content":"添加 - 修复性质4 - 上溢 - LL 判定条件：uncle 是 RED parent、uncle 染成 BLACK grand 向上合并 染成 RED，当作是新添加的节点进行处理 grand 向上合并时，可能继续发生上溢 若上溢持续到根节点，只需将根节点染成 BLACK ","date":"2020-09-21","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-11-%E7%BA%A2%E9%BB%91%E6%A0%91/:5:3","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-11-红黑树","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-11-%E7%BA%A2%E9%BB%91%E6%A0%91/"},{"categories":["算法"],"content":"添加 - 修复性质4 - 上溢 - RR 判定条件：uncle 是 RED parent、uncle 染成 BLACK grand 向上合并 染成 RED，当做是新添加的节点进行处理 ","date":"2020-09-21","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-11-%E7%BA%A2%E9%BB%91%E6%A0%91/:5:4","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-11-红黑树","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-11-%E7%BA%A2%E9%BB%91%E6%A0%91/"},{"categories":["算法"],"content":"添加 - 修复性质4 - 上溢 - LR 判定条件：uncle 是 RED parent、uncle 染成 BLACK grand 向上合并 染成 RED，当作是新添加的节点进行处理 ","date":"2020-09-21","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-11-%E7%BA%A2%E9%BB%91%E6%A0%91/:5:5","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-11-红黑树","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-11-%E7%BA%A2%E9%BB%91%E6%A0%91/"},{"categories":["算法"],"content":"添加 - 修复性质4 - 上溢 - RL 判定条件：uncle 是 RED parent、uncle 染成 BLACK grand 向上合并 染成 RED，当作是新添加的节点进行处理 ","date":"2020-09-21","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-11-%E7%BA%A2%E9%BB%91%E6%A0%91/:5:6","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-11-红黑树","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-11-%E7%BA%A2%E9%BB%91%E6%A0%91/"},{"categories":["算法"],"content":"添加的实现 @Override protected void afterAdd(Node\u003cE\u003e node) { Node\u003cE\u003e parent = node.parent; // 添加的是根节点 或者 上溢到达了根节点 if (parent == null) { black(node); return; } // 如果父节点是黑色，直接返回 if (isBlack(parent)) return; // 叔父节点 Node\u003cE\u003e uncle = parent.sibling(); // 祖父节点 Node\u003cE\u003e grand = red(parent.parent); if (isRed(uncle)) { // 叔父节点是红色【B树节点上溢】 black(parent); black(uncle); // 把祖父节点当做是新添加的节点 afterAdd(grand); return; } // 叔父节点不是红色 if (parent.isLeftChild()) { // L if (node.isLeftChild()) { // LL black(parent); } else { // LR black(node); rotateLeft(parent); } rotateRight(grand); } else { // R if (node.isLeftChild()) { // RL black(node); rotateRight(parent); } else { // RR black(parent); } rotateLeft(grand); } } 继承结构的调整 二叉树 —\u003e BST(二叉搜索树) -\u003e BBST(平衡二叉搜索树) -\u003e AVL树或红黑树 二叉树整体代码如下 package org.msdemt.demo.tree; import org.msdemt.demo.printer.BinaryTreeInfo; import java.util.LinkedList; import java.util.Queue; /** * 二叉树 * * @param \u003cE\u003e */ @SuppressWarnings(\"unchecked\") public class BinaryTree\u003cE\u003e implements BinaryTreeInfo { protected int size; protected Node\u003cE\u003e root; public int size() { return size; } public boolean isEmpty() { return size == 0; } public void clear() { root = null; size = 0; } public void preorder(Visitor\u003cE\u003e visitor) { if (visitor == null) return; preorder(root, visitor); } private void preorder(Node\u003cE\u003e node, Visitor\u003cE\u003e visitor) { if (node == null || visitor.stop) return; visitor.stop = visitor.visit(node.element); preorder(node.left, visitor); preorder(node.right, visitor); } public void inorder(Visitor\u003cE\u003e visitor) { if (visitor == null) return; inorder(root, visitor); } private void inorder(Node\u003cE\u003e node, Visitor\u003cE\u003e visitor) { if (node == null || visitor.stop) return; inorder(node.left, visitor); if (visitor.stop) return; visitor.stop = visitor.visit(node.element); inorder(node.right, visitor); } public void postorder(Visitor\u003cE\u003e visitor) { if (visitor == null) return; postorder(root, visitor); } private void postorder(Node\u003cE\u003e node, Visitor\u003cE\u003e visitor) { if (node == null || visitor.stop) return; postorder(node.left, visitor); postorder(node.right, visitor); if (visitor.stop) return; visitor.stop = visitor.visit(node.element); } public void levelOrder(Visitor\u003cE\u003e visitor) { if (root == null || visitor == null) return; Queue\u003cNode\u003cE\u003e\u003e queue = new LinkedList\u003c\u003e(); queue.offer(root); while (!queue.isEmpty()) { Node\u003cE\u003e node = queue.poll(); if (visitor.visit(node.element)) return; if (node.left != null) { queue.offer(node.left); } if (node.right != null) { queue.offer(node.right); } } } public boolean isComplete() { if (root == null) return false; Queue\u003cNode\u003cE\u003e\u003e queue = new LinkedList\u003c\u003e(); queue.offer(root); boolean leaf = false; while (!queue.isEmpty()) { Node\u003cE\u003e node = queue.poll(); if (leaf \u0026\u0026 !node.isLeaf()) return false; if (node.left != null) { queue.offer(node.left); } else if (node.right != null) { return false; } if (node.right != null) { queue.offer(node.right); } else { // 后面遍历的节点都必须是叶子节点 leaf = true; } } return true; } public int height() { if (root == null) return 0; // 树的高度 int height = 0; // 存储着每一层的元素数量 int levelSize = 1; Queue\u003cNode\u003cE\u003e\u003e queue = new LinkedList\u003c\u003e(); queue.offer(root); while (!queue.isEmpty()) { Node\u003cE\u003e node = queue.poll(); levelSize--; if (node.left != null) { queue.offer(node.left); } if (node.right != null) { queue.offer(node.right); } if (levelSize == 0) { // 意味着即将要访问下一层 levelSize = queue.size(); height++; } } return height; } public int height2() { return height(root); } private int height(Node\u003cE\u003e node) { if (node == null) return 0; return 1 + Math.max(height(node.left), height(node.right)); } protected Node\u003cE\u003e createNode(E element, Node\u003cE\u003e parent) { return new Node\u003c\u003e(element, parent); } protected Node\u003cE\u003e predecessor(Node\u003cE\u003e node) { if (node == null) return null; // 前驱节点在左子树当中（left.right.right.right....） Node\u003cE\u003e p = node.left; if (p != null) { while (p.right != null) { p = p.right; } return p; } // 从父节点、祖父节点中寻找前驱节点 while (node.parent != null \u0026\u0026 node == node.parent.left) { node = node.parent; } // node.parent == null // ","date":"2020-09-21","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-11-%E7%BA%A2%E9%BB%91%E6%A0%91/:5:7","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-11-红黑树","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-11-%E7%BA%A2%E9%BB%91%E6%A0%91/"},{"categories":["算法"],"content":"删除 B 树中，最后真正被删除的元素都在叶子节点中 ","date":"2020-09-21","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-11-%E7%BA%A2%E9%BB%91%E6%A0%91/:6:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-11-红黑树","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-11-%E7%BA%A2%E9%BB%91%E6%A0%91/"},{"categories":["算法"],"content":"删除 - RED 节点 直接删除，不用作任何调整 ","date":"2020-09-21","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-11-%E7%BA%A2%E9%BB%91%E6%A0%91/:6:1","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-11-红黑树","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-11-%E7%BA%A2%E9%BB%91%E6%A0%91/"},{"categories":["算法"],"content":"删除 - BLACK 节点 有 3 中情况 拥有 2 个 RED 子节点的 BLACK 节点 不可能被直接删除，因为会找它的前驱或后继（本例为子节点）替代删除 因此不用考虑这种情况 拥有 1 个 RED 子节点的 BLACK 节点 BLACK 叶子节点 删除 - 拥有 1 个 RED 子节点的 BLACK 节点 判定条件：用以替代的子节点是 RED 将替代的子节点染成 BLACK 即可保持红黑树性质 删除 - BLACK叶子节点 - sibling为BLACK 兄弟节点能借一个节点的情况 BLACK 叶子节点被删除后，会导致B树节点下溢（比如删除88） 如果 sibling 至少有 1 个 RED 子节点 进行旋转操作 旋转之后的中心节点继承 parent 的颜色 旋转之后的左右节点染成 BLACK 第三幅图有两种情况，LL 或 LR 兄弟节点不能借一个节点的情况 判定条件：sibling 没有 1 个 RED 子节点 将 sibling 染成 RED，parent 染成 BLACK 即可修复红黑树性质 如果 parent 是 BLACK 会导致 parent 也下溢 这时只需要把 parent 当作被删除的节点处理即可 删除 - BLACK叶子节点 - sibling为RED 如果 sibling 是 RED sibling 染成 BLACK，parent 染成 RED，进行旋转 于是又回到了 sibling 是 BLACK 的情况 ","date":"2020-09-21","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-11-%E7%BA%A2%E9%BB%91%E6%A0%91/:6:2","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-11-红黑树","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-11-%E7%BA%A2%E9%BB%91%E6%A0%91/"},{"categories":["算法"],"content":"红黑树的平衡 最初遗留的困惑：为何那 5 条性质，就能保证红黑树是平衡的？ 那 5 条性质，可以保证 红黑树 等价于 4阶B树 相比 AVL树，红黑树的平衡标准比较宽松：没有一条路径会大于其他路径的 2 倍 红黑树的平衡是一种弱平衡，黑高度平衡 红黑树的最大高度是 2 * log2(n+1)，依然是 O(log n) 级别 ","date":"2020-09-21","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-11-%E7%BA%A2%E9%BB%91%E6%A0%91/:7:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-11-红黑树","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-11-%E7%BA%A2%E9%BB%91%E6%A0%91/"},{"categories":["算法"],"content":"红黑树的平均时间复杂度 搜索：O(log n) 添加：O(log n)，O(1)次的旋转操作 删除：O(log n)，O(1)次的旋转操作 ","date":"2020-09-21","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-11-%E7%BA%A2%E9%BB%91%E6%A0%91/:8:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-11-红黑树","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-11-%E7%BA%A2%E9%BB%91%E6%A0%91/"},{"categories":["算法"],"content":"AVL树 vs 红黑树 AVL树 平衡标准比较严格：每个左右子树的高度差不能超过 1 最大高度是 1.44 * log2(n+2) - 1.328 （100W 个节点，AVL树最大树高28） 搜索、添加、删除都是 O(log n) 复杂度，其中添加仅需 O(1) 次旋转调整，删除最多需要 O(log n) 次旋转调整 红黑树 平衡标准比较宽松：没有一条路径会大于其他路径的 2 倍 最大高度是 2 * log2(n+1) （100W 个节点，红黑树最大树高40） 搜索、添加、删除都是 O(log n) 复杂度，其中添加、删除都仅需 O(1) 次旋转调整 如果搜索的次数远远大于插入和删除，选择AVL树；搜索、插入、删除次数几乎差不多，选择红黑树 相对于AVL树来说，红黑树牺牲了部分平衡性以换取插入/删除操作时少量的选择操作，整体来说性能要优于AVL树 红黑树的平均统计性能优于AVL树，实际应用中更多选择红黑树 ","date":"2020-09-21","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-11-%E7%BA%A2%E9%BB%91%E6%A0%91/:9:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-11-红黑树","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-11-%E7%BA%A2%E9%BB%91%E6%A0%91/"},{"categories":["算法"],"content":"BST vs AVL Tree vs Red Black Tree ","date":"2020-09-21","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-11-%E7%BA%A2%E9%BB%91%E6%A0%91/:10:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-11-红黑树","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-11-%E7%BA%A2%E9%BB%91%E6%A0%91/"},{"categories":["算法"],"content":"B 树（B-tree、B-树） B 树是一种平衡的多路搜索树，多用于文件系统、数据库的实现 仔细观察 B 树，有什么特点？ 1 个节点可以存储超过 2 个元素，可以拥有超过 2 个子节点 拥有二叉搜索树的一些性质 平衡，每个节点的所有子树高度一致 比较矮 ","date":"2020-09-21","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-10-b%E6%A0%91/:1:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-10-B树","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-10-b%E6%A0%91/"},{"categories":["算法"],"content":"m 阶 B 树的性质（m \u003e= 2） 假设一个节点存储的元素个数为 x 根节点的元素个数：1 \u003c= x \u003c= m-1 非根节点的元素个数：⌈ m/2 ⌉ - 1 \u003c= x \u003c= m-1 如果有子节点，子节点个数 y = x + 1 根节点：2 \u003c= y \u003c= m 非根节点: ⌈ m/2 ⌉ \u003c= y \u003c= m 比如 m = 3，2 \u003c= y \u003c= 3，因此可以称为(2, 3)树、2-3树 比如 m = 4，2 \u003c= y \u003c= 4，因此可以称为(2, 4)树、2-3-4树 比如 m = 5，3 \u003c= y \u003c= 5，因此可以称为(3, 5)树 比如 m = 6，3 \u003c= y \u003c= 6，因此可以称为(3, 6)树 比如 m = 7，4 \u003c= y \u003c= 7，因此可以称为(4, 7)树 ","date":"2020-09-21","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-10-b%E6%A0%91/:2:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-10-B树","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-10-b%E6%A0%91/"},{"categories":["算法"],"content":"B 树 vs 二叉搜索树 B 树 和 二叉搜索树，在逻辑上是等价的 多代节点合并，可以获得一个超级节点 2 代合并的超级节点，最多拥有 4 个子节点（至少是 4 阶 B 树） 3 代合并的超级节点，最多拥有 8 个子节点（至少是 8 阶 B 树） n 代合并的超级节点，最多拥有 2n 个子节点（至少是 2n 阶 B 树） m 阶 B 树，最多需要 log2m 代合并 ","date":"2020-09-21","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-10-b%E6%A0%91/:3:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-10-B树","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-10-b%E6%A0%91/"},{"categories":["算法"],"content":"搜索 B 树的搜索和二叉搜索树的搜索类似 先在节点内部从小到大开始搜索元素 如果命中，搜索结束 如果未命中，再去对应的节点中搜索元素，重复步骤 1 ","date":"2020-09-21","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-10-b%E6%A0%91/:4:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-10-B树","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-10-b%E6%A0%91/"},{"categories":["算法"],"content":"添加 B 树添加的元素必定是添加到叶子节点 添加 55 添加 95 再添加 98 呢？ （假设这是一棵 4 阶 B 树） 最右下角的叶子节点的元素个数将超过限制 这种现象称为：上溢（overflow） ","date":"2020-09-21","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-10-b%E6%A0%91/:5:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-10-B树","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-10-b%E6%A0%91/"},{"categories":["算法"],"content":"添加时上溢的解决（假设5阶） 上溢节点的元素个数必然等于 m 假设上溢节点最中间元素的位置为 k 将 k 位置的元素向上与父节点合并 将 [0, k-1] 和 [k+1, m-1] 位置的元素分裂成 2 个节点 这 2 个子节点的元素个数，必然都不会低于最低限制（⌈ m/2 ⌉ - 1） 一次分裂完毕后，有可能导致父节点上溢，依然按照上述方法解决 最极端的情况，有可能一直分裂到根节点 示例： ","date":"2020-09-21","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-10-b%E6%A0%91/:5:1","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-10-B树","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-10-b%E6%A0%91/"},{"categories":["算法"],"content":"删除 ","date":"2020-09-21","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-10-b%E6%A0%91/:6:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-10-B树","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-10-b%E6%A0%91/"},{"categories":["算法"],"content":"删除 - 叶子节点 假如需要删除的元素在叶子节点中，那么直接删除即可 删除 30 ","date":"2020-09-21","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-10-b%E6%A0%91/:6:1","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-10-B树","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-10-b%E6%A0%91/"},{"categories":["算法"],"content":"删除 - 非叶子节点 假如需要删除的元素在非叶子节点中 先找到前驱或后继元素，覆盖所需删除的元素 再把前驱或后继元素删除 删除 60 非叶子节点的前驱或后继元素，必定在叶子节点中 所以这里的删除前驱或后继元素，就是最开始提到的情况：删除的元素在叶子节点中 真正的删除元素都是发生在叶子节点中 上图中，假设这是一棵 5 阶 B 树，如果删除 22 ？ 叶子节点被删掉一个元素后，元素个数可能会低于最低限制（ \u003e= ⌈ m/2 ⌉ - 1 ） 这种现象称为：下溢（underflow） ","date":"2020-09-21","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-10-b%E6%A0%91/:6:2","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-10-B树","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-10-b%E6%A0%91/"},{"categories":["算法"],"content":"删除时下溢的解决 下溢节点的元素数量必然等于 ⌈ m/2 ⌉ - 2 如果下溢节点临近的兄弟节点，有至少 ⌈ m/2 ⌉ 个元素，可以向其借一个元素 将父节点的元素 b 插入到下溢节点的 0 位置（最小位置） 用兄弟节点的元素 a （最大的元素）替代父节点的元素 b 这种操作其实就是：旋转 如果下溢节点临近的兄弟节点，只有 ⌈ m/2 ⌉ - 1 个元素 将父节点的元素 b 挪下来根左右子节点进行合并 合并后的节点元素个数等于 ⌈ m/2 ⌉ + ⌈ m/2 ⌉ - 2 ，不超过 m - 1 这个操作可能导致父节点下溢，依然按照上述方法解决，下溢现象可能会一直往上传播 上溢到根节点，是唯一一种能让 B 树长高的情况 下溢到根节点，是唯一一种能让 B 树变矮的情况 ","date":"2020-09-21","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-10-b%E6%A0%91/:6:3","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-10-B树","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-10-b%E6%A0%91/"},{"categories":["算法"],"content":"4 阶 B 树 如果先学习 4 阶 B 树（2-3-4树），将能更好地学习理解红黑树 4 阶 B 树的性质 所有节点能存储的元素个数 x ： 1 \u003c= x \u003c= 3 所有非叶子节点的子节点个数 y ： 2 \u003c= y \u003c= 4 ","date":"2020-09-21","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-10-b%E6%A0%91/:7:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-10-B树","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-10-b%E6%A0%91/"},{"categories":["算法"],"content":"AVL树 AVL树是最早发明的自平衡二叉搜索树之一 AVL取名与两位发明者的名字 G.M.Adelson-Velsky 和 E.M.Landis （来自苏联的科学家） 平衡因子（Balance Factor）：某结点的左右子树的高度差 AVL树的特点 每个节点的平衡因子只可能是 1、0、-1（绝对值 \u003c= 1，如果超过 1，称之为“失衡”） 每个节点的左右子树高度差不超过 1 搜索、添加、删除的时间复杂度是 O(log n) ","date":"2020-09-18","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-09-avl%E6%A0%91/:1:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-09-AVL树","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-09-avl%E6%A0%91/"},{"categories":["算法"],"content":"平衡对比 输入数据：35、37、34、56、25、62、57、9、74、32、94、80、75、100、16、82 ","date":"2020-09-18","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-09-avl%E6%A0%91/:1:1","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-09-AVL树","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-09-avl%E6%A0%91/"},{"categories":["算法"],"content":"简单的继承结构 二叉树实现 package org.msdemt.demo.tree; import org.msdemt.demo.printer.BinaryTreeInfo; import java.util.LinkedList; import java.util.Queue; public class BinaryTree\u003cE\u003e implements BinaryTreeInfo { protected int size; protected Node\u003cE\u003e root; public int size() { return size; } public boolean isEmpty() { return size == 0; } public void clear() { root = null; size = 0; } public void preorder(Visitor\u003cE\u003e visitor) { if (visitor == null) return; preorder(root, visitor); } private void preorder(Node\u003cE\u003e node, Visitor\u003cE\u003e visitor) { if (node == null || visitor.stop) return; visitor.stop = visitor.visit(node.element); preorder(node.left, visitor); preorder(node.right, visitor); } public void inorder(Visitor\u003cE\u003e visitor) { if (visitor == null) return; inorder(root, visitor); } private void inorder(Node\u003cE\u003e node, Visitor\u003cE\u003e visitor) { if (node == null || visitor.stop) return; inorder(node.left, visitor); if (visitor.stop) return; visitor.stop = visitor.visit(node.element); inorder(node.right, visitor); } public void postorder(Visitor\u003cE\u003e visitor) { if (visitor == null) return; postorder(root, visitor); } private void postorder(Node\u003cE\u003e node, Visitor\u003cE\u003e visitor) { if (node == null || visitor.stop) return; postorder(node.left, visitor); postorder(node.right, visitor); if (visitor.stop) return; visitor.stop = visitor.visit(node.element); } public void levelOrder(Visitor\u003cE\u003e visitor) { if (root == null || visitor == null) return; Queue\u003cNode\u003cE\u003e\u003e queue = new LinkedList\u003c\u003e(); queue.offer(root); while (!queue.isEmpty()) { Node\u003cE\u003e node = queue.poll(); if (visitor.visit(node.element)) return; if (node.left != null) { queue.offer(node.left); } if (node.right != null) { queue.offer(node.right); } } } public boolean isComplete() { if (root == null) return false; Queue\u003cNode\u003cE\u003e\u003e queue = new LinkedList\u003c\u003e(); queue.offer(root); boolean leaf = false; while (!queue.isEmpty()) { Node\u003cE\u003e node = queue.poll(); if (leaf \u0026\u0026 !node.isLeaf()) return false; if (node.left != null) { queue.offer(node.left); } else if (node.right != null) { return false; } if (node.right != null) { queue.offer(node.right); } else { // 后面遍历的节点都必须是叶子节点 leaf = true; } } return true; } public int height() { if (root == null) return 0; int height = 0; int levelSize = 1; Queue\u003cNode\u003cE\u003e\u003e queue = new LinkedList\u003c\u003e(); queue.offer(root); while (!queue.isEmpty()) { Node\u003cE\u003e node = queue.poll(); levelSize--; if (node.left != null) { queue.offer(node.left); } if (node.right != null) { queue.offer(node.right); } if (levelSize == 0) { levelSize = queue.size(); height++; } } return height; } public int height2() { return height(root); } private int height(Node\u003cE\u003e node) { if (node == null) return 0; return 1 + Math.max(height(node.left), height(node.right)); } protected Node\u003cE\u003e createNode(E element, Node\u003cE\u003e parent) { return new Node\u003c\u003e(element, parent); } protected Node\u003cE\u003e predecessor(Node\u003cE\u003e node) { if (node == null) return null; Node\u003cE\u003e p = node.left; if (p != null) { while (p.right != null) { p = p.right; } return p; } while (node.parent != null \u0026\u0026 node == node.parent.left) { node = node.parent; } return node.parent; } protected Node\u003cE\u003e successor(Node\u003cE\u003e node) { if (node == null) return null; Node\u003cE\u003e p = node.right; if (p != null) { while (p.left != null) { p = p.left; } return p; } while (node.parent != null \u0026\u0026 node == node.parent.right) { node = node.parent; } return node.parent; } public static abstract class Visitor\u003cE\u003e { boolean stop; abstract boolean visit(E element); } protected static class Node\u003cE\u003e { E element; Node\u003cE\u003e left; Node\u003cE\u003e right; Node\u003cE\u003e parent; public Node(E element, Node\u003cE\u003e parent) { this.element = element; this.parent = parent; } public boolean isLeaf() { return left == null \u0026\u0026 right == null; } public boolean hasTwoChildren() { return left != null \u0026\u0026 right != null; } public boolean isLeftChild() { return parent != null \u0026\u0026 this == parent.left; } public boolean isRightChild() { return parent != null \u0026\u0026 this == parent.right; } public Node\u003cE\u003e sibling() { if (isLeftChild()) { return parent.right; ","date":"2020-09-18","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-09-avl%E6%A0%91/:1:2","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-09-AVL树","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-09-avl%E6%A0%91/"},{"categories":["算法"],"content":"添加导致的失衡 示例：往下面这棵树中添加 13 最坏情况：可能会导致所有祖先节点都失衡 父节点、非祖先节点，都不可能失衡 LL - 右旋转（单旋） T0 添加一个元素，会导致祖先节点 g 失衡 添加的节点（T1或T2上添加） 位于 失衡节点 g 的 左边、左边 方向，所以称为 LL 这种情况对 g 进行右旋转 解决办法：右旋转 g.left = p.right p.right = g 让 p 成为这棵子树的根节点 仍然是一棵二叉搜索树：T0 \u003c n \u003c T1 \u003c p \u003c T2 \u003c g \u003c T3 整棵树都达到平衡 还需要注意维护的内容 T2、p、g 的 parent 属性 先后更新 g、p 的高度 有些教程里，将右旋转叫做 zig，旋转之后的状态叫做 zigged RR - 左旋转（单旋） T3 添加一个元素，会导致祖先节点 g 失衡 添加的节点（T2或T3上添加） 位于 失衡节点 g 的 右边、右边 方向，所以称为 RR 这种情况对 g 进行左旋转 解决办法：左旋转 g.right = p.left p.left = g 让 p 成为这棵子树的根节点 仍然是一棵二叉搜索树：T0 \u003c g \u003c T1 \u003c p \u003c T2 \u003c n \u003c T3 整棵树都达到平衡 还需要注意维护的内容 T1、p、g 的 parent 属性 先后更新 g、p 的高度 有些教程里将左旋转叫做 zag，旋转之后的状态叫做 zagged LR - RR左旋转，LL右旋转（双旋） T2 添加元素，导致祖先节点 g 失衡 添加的节点（T1或T2） 位于 失衡节点 g 的 左边、右边 方向，所以称为 LR 这种情况先对 p 进行左旋转，让 n 成为子树的根节点，此时 g 是 LL 的情况，在对 g 进行一次右旋转，最终搞定 RL - LL右旋转，RR左旋转（双旋） 在失衡节点 g 的右方向、左方向添加节点导致的失衡，这种情况先对 p 进行一次右旋转，选择之后 n 成为子树根节点，这时 g 符合 RR 情况，对 g 进行一次左旋转 实现 AVLNode 继承 二叉树的 Node，维护高度的属性 二叉树的 Node protected static class Node\u003cE\u003e { E element; Node\u003cE\u003e left; Node\u003cE\u003e right; Node\u003cE\u003e parent; public Node(E element, Node\u003cE\u003e parent) { this.element = element; this.parent = parent; } public boolean isLeaf() { return left == null \u0026\u0026 right == null; } public boolean hasTwoChildren() { return left != null \u0026\u0026 right != null; } public boolean isLeftChild() { return parent != null \u0026\u0026 this == parent.left; } public boolean isRightChild() { return parent != null \u0026\u0026 this == parent.right; } public Node\u003cE\u003e sibling() { if (isLeftChild()) { return parent.right; } if (isRightChild()) { return parent.left; } return null; } } AVLNode的实现 private static class AVLNode\u003cE\u003e extends Node\u003cE\u003e { int height = 1; public AVLNode(E element, Node\u003cE\u003e parent) { super(element, parent); } public int balanceFactor() { int leftHeight = left == null ? 0 : ((AVLNode\u003cE\u003e) left).height; int rightHeight = right == null ? 0 : ((AVLNode\u003cE\u003e) right).height; return leftHeight - rightHeight; } public void updateHeight() { int leftHeight = left == null ? 0 : ((AVLNode\u003cE\u003e) left).height; int rightHeight = right == null ? 0 : ((AVLNode\u003cE\u003e) right).height; height = 1 + Math.max(leftHeight, rightHeight); } public Node\u003cE\u003e tallerChild() { int leftHeight = left == null ? 0 : ((AVLNode\u003cE\u003e) left).height; int rightHeight = right == null ? 0 : ((AVLNode\u003cE\u003e) right).height; if (leftHeight \u003e rightHeight) return left; if (leftHeight \u003c rightHeight) return right; return isLeftChild() ? left : right; } @Override public String toString() { String parentString = \"null\"; if (parent != null) { parentString = parent.element.toString(); } return element + \"_p(\" + parentString + \")_h(\" + height + \")\"; } } AVL中判断节点是否平衡 private boolean isBalanced(Node\u003cE\u003e node) { return Math.abs(((AVLNode\u003cE\u003e) node).balanceFactor()) \u003c= 1; } AVL中添加节点后更新高度 @Override protected void afterAdd(Node\u003cE\u003e node) { while ((node = node.parent) != null) { if (isBalanced(node)) { // 更新高度 updateHeight(node); } else { // 恢复平衡 rebalance(node); // 整棵树恢复平衡 break; } } } private void updateHeight(Node\u003cE\u003e node) { ((AVLNode\u003cE\u003e) node).updateHeight(); } 恢复平衡 /** * 恢复平衡 * * @param grand 高度最低的那个不平衡节点 */ @SuppressWarnings(\"unused\") private void rebalance(Node\u003cE\u003e grand) { Node\u003cE\u003e parent = ((AVLNode\u003cE\u003e) grand).tallerChild(); Node\u003cE\u003e node = ((AVLNode\u003cE\u003e) parent).tallerChild(); if (parent.isLeftChild()) { // L if (node.isLeftChild()) { // LL rotateRight(grand); } else { // LR rotateLeft(parent); rotateRight(grand); } } else { // R if (node.isLeftChild()) { // RL rotateRight(parent); rotateLeft(grand); } else { // RR rotateLeft(grand); } } } private void rotateLeft(Node\u003cE\u003e grand) { Node\u003cE\u003e parent = grand.right; Node\u003cE\u003e child = parent.left; grand.right = child; parent.left = grand; afterRotate(grand, parent, child); } private void rotateRight(Node\u003cE\u003e grand) { Node\u003cE\u003e parent = grand.left; Node\u003cE\u003e child = parent.right; grand.left = child; parent.right = grand; afterRotate(grand, parent, child); } 恢复平衡的优化，统一旋转的操作 根据二叉搜索树的性质，a \u003c b \u003c c \u003c d \u003c e \u003c f \u003c g 最终恢复平衡的样子都是一样的，所以搞清楚对应的 a、b、c、d、e、f、g 是谁就可以了 /** * 恢复平衡 * * @param grand 高度最低的那个不平衡节点 */ private void rebalance(Node\u003cE\u003e grand) { Node\u003cE\u003e pa","date":"2020-09-18","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-09-avl%E6%A0%91/:1:3","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-09-AVL树","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-09-avl%E6%A0%91/"},{"categories":["算法"],"content":"删除导致的失衡 示例：删除子树中的 16 可能会导致父节点或祖先节点失衡（只有 1 个节点会失衡），其他节点，都不可能失衡 LL - 右旋转（单旋） 假设删除 T3 的红色节点，此时节点 g 会失去平衡，因为此时属于 LL 情况，所以需要对 g 进行右旋转 如果绿色节点不存在，更高层的祖先节点可能也会失衡，需要再次恢复平衡，然后有可能导致更高层的祖先节点失衡… 极端情况下，所有祖先节点都需要进行恢复平衡的操作，共 O(log n) 次调整 RR - 左旋转（单旋） LR - RR左旋转，LL右旋转（双旋） RL - LL右旋转，RR左旋转（双旋） 实现 @Override protected void afterRemove(Node\u003cE\u003e node) { while ((node = node.parent) != null) { if (isBalanced(node)) { // 更新高度 updateHeight(node); } else { // 恢复平衡 rebalance(node); } } } ","date":"2020-09-18","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-09-avl%E6%A0%91/:1:4","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-09-AVL树","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-09-avl%E6%A0%91/"},{"categories":["算法"],"content":"总结 添加 可能导致所有祖先节点都失衡 只要让高度最低的失衡节点恢复平衡，整棵树就恢复平衡（仅需 O(1) 次调整） 删除 可能会导致父节点或祖先节点失衡（只有 1 个节点会失衡） 恢复平衡后，可能会导致更高层的祖先节点失衡（最多需要 O(log n) 次调整） 平均复杂度 搜索：O(log n) 添加：O(log n)，仅需 O(1) 次的旋转操作 删除：O(log n)，最多需要 O(log n) 次的旋转操作 ","date":"2020-09-18","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-09-avl%E6%A0%91/:1:5","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-09-AVL树","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-09-avl%E6%A0%91/"},{"categories":["算法"],"content":"二叉搜索树的复杂度分析 如果是按照7、4、9、2、5、8、11的顺序添加节点 添加、删除、搜索时都是从根节点开始比较，比较的次数和树的高度有关，和元素的个数没关系。 复杂度 O(h) == O(log n) 二叉搜索树对比数组和链表，添加、删除、搜索的效率大大提高，因为它只和树的高度有关系，但是有种最坏的情况，即从小到大添加元素 如果是从小到大添加节点 复杂度 O(h) == O(n) 此时和数组、链表的区别不大，二叉搜索树退化成链表 当 n 比较大时，两者（随机添加元素和从小到大添加元素）的性能差异比较大 比如，当 n = 1000000 时，二叉搜索树的最低高度是 20 删除元素时，也有可能导致二叉搜索树退化成链表 添加、删除节点时，都有可能导致二叉搜索树退化成链表 有没有办法防止二叉搜索树退化成链表？ 让添加、删除、搜索的复杂度维持在O(log n) ","date":"2020-09-18","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-08-%E5%B9%B3%E8%A1%A1%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91/:1:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-08-平衡二叉搜索树","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-08-%E5%B9%B3%E8%A1%A1%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91/"},{"categories":["算法"],"content":"平衡（Balance） 平衡：当节点数量固定时，左右子树的高度越接近，这颗二叉树就越平衡（高度越低） ","date":"2020-09-18","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-08-%E5%B9%B3%E8%A1%A1%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91/:2:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-08-平衡二叉搜索树","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-08-%E5%B9%B3%E8%A1%A1%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91/"},{"categories":["算法"],"content":"理想平衡 最理想的平衡，就是像完全二叉树、满二叉树那样，高度是最小的 ","date":"2020-09-18","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-08-%E5%B9%B3%E8%A1%A1%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91/:3:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-08-平衡二叉搜索树","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-08-%E5%B9%B3%E8%A1%A1%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91/"},{"categories":["算法"],"content":"如何改进二叉搜索树 首先，节点的添加、删除顺序是无法限制的，可以认为是随机的 所以，改进方案是：在节点的添加、删除操作后，想办法让二叉搜索树恢复平衡（减小树的高度） 如果接着继续调整节点的位置，完全可以达到理想平衡，但是付出的代价可能会比较大 比如调整的次数会比较多，反而增加了时间复杂度 总结来说，比较合理的改进方案是：用尽量少的调整次数达到适度平衡即可 一颗达到适度平衡的二叉搜索树，可以称之为：平衡二叉搜索树 ","date":"2020-09-18","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-08-%E5%B9%B3%E8%A1%A1%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91/:4:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-08-平衡二叉搜索树","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-08-%E5%B9%B3%E8%A1%A1%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91/"},{"categories":["算法"],"content":"平衡二叉搜索树（Balanced Binary Search Tree） 英文简称：BBST 经典常见的平衡二叉搜索树有 AVL树 Windows NT 内核中广泛使用 红黑树 C++ STL (比如 map、set) Java 的 TreeMap、TreeSet、HashMap、HashSet Linux 的进程调度 Nginx 的 timer管理 一般也称它们为：自平衡的二叉搜索树（Self-balancing Binary Search Tree） ","date":"2020-09-18","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-08-%E5%B9%B3%E8%A1%A1%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91/:5:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-08-平衡二叉搜索树","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-08-%E5%B9%B3%E8%A1%A1%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91/"},{"categories":["算法"],"content":"思考 在 n 个动态的整数中搜索某个整数？（查看其是否存在） 假设使用动态数组存放元素，从第 0 个位置开始遍历搜索，平均时间复杂度：O(n) 如果维护一个有序的动态数组，使用二分搜索，最坏时间复杂度：O(logn) 但是添加、删除的平均时间复杂度是O(n) 针对这个需求，有没有更好的方案？ 使用二叉搜索树，添加、删除、搜索的最坏时间复杂度均可优化至：O(logn) ","date":"2020-09-16","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-07-%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91/:1:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-07-二叉搜索树","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-07-%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91/"},{"categories":["算法"],"content":"二叉搜索树（Binary Search Tree） 二叉搜索树是二叉树的一种，是应用非常广泛的一种二叉树，英文简称为 BST 又被称为：二叉查找树、二叉排序树 任意一个节点的值都大于其左子树所有节点的值 任意一个节点的值都小于其右子树所有节点的值 它的左右子树也是一颗二叉搜索树 二叉搜索树可以大大提高搜索数据的效率 二叉搜索树存储的元素必须具备可比较性 比如 int、 double 等 如果是自定义类型，需要指定比较方式 不允许为 null ","date":"2020-09-16","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-07-%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91/:2:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-07-二叉搜索树","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-07-%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91/"},{"categories":["算法"],"content":"二叉搜索树的接口设计 int size(); //元素的数量 boolean isEmpty(); //是否为空 void clear(); //清空所有元素 void add(E element); //添加元素 void remove(E element); //删除元素 boolean contains(E element); //是否包含某元素 需要注意的是 对于我们现在使用的二叉树来说，它的元素没有索引的概念 ","date":"2020-09-16","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-07-%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91/:2:1","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-07-二叉搜索树","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-07-%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91/"},{"categories":["算法"],"content":"二叉搜索树的实现 添加节点 找到父节点 parent 创建新节点 node parent.left = node 或者 parent.right = node 遇到值相等的元素该如何处理？ 建议覆盖旧的值 元素的比较方案 允许外界传入一个 Comparator 自定义比较方案 如果没有传入 Comparator，强制认定元素实现了 Comparable 接口 实现 package org.msdemt.demo; import org.msdemt.demo.printer.BinaryTreeInfo; import java.util.Comparator; import java.util.LinkedList; import java.util.Queue; /** * 二叉搜索树的实现 * * 继承BinaryTreeInfo是为了实现打印的功能 * @param \u003cE\u003e */ public class BinarySearchTree\u003cE\u003e implements BinaryTreeInfo { private int size; private Node\u003cE\u003e root; private Comparator\u003cE\u003e comparator; private static class Node\u003cE\u003e { E element; Node\u003cE\u003e left; Node\u003cE\u003e right; Node\u003cE\u003e parent; public Node(E element, Node\u003cE\u003e parent) { this.element = element; this.parent = parent; } public boolean isLeaf() { return left == null \u0026\u0026 right == null; } public boolean hasTwoChildren() { return left != null \u0026\u0026 right != null; } } public BinarySearchTree() { this(null); } public BinarySearchTree(Comparator\u003cE\u003e comparator) { this.comparator = comparator; } public int size() { return size; } public boolean isEmpty() { return size == 0; } public void clear() { root = null; size = 0; } public void add(E element) { elementNotNullCheck(element); //添加第一个节点 if (root == null) { root = new Node\u003c\u003e(element, null); size++; return; } //添加的不是第一个节点 //找到父节点 Node\u003cE\u003e parent = root; Node\u003cE\u003e node = root; int cmp = 0; do { cmp = compare(element, node.element); //保存父节点 parent = node; if (cmp \u003e 0) { node = node.right; } else if (cmp \u003c 0) { node = node.left; } else { node.element = element; return; } } while (node != null); //看看插入到父节点的哪个位置 Node\u003cE\u003e newNode = new Node\u003c\u003e(element, parent); if (cmp \u003e 0) { parent.right = newNode; } else { parent.left = newNode; } size++; } public boolean contains(E element) { return node(element) != null; } public void remove(E element) { remove(node(element)); } private void remove(Node\u003cE\u003e node) { if (node == null) { return; } size--; //度为2的节点 if (node.hasTwoChildren()) { //找到后继节点 Node\u003cE\u003e s = successor(node); //用后继节点的值覆盖度为2的节点的值，前驱或后继节点的度只能为0或1 node.element = s.element; //删除后继节点，将后继节点s赋给node,在后续的流程中删除该度为0或1的节点 node = s; } //删除node节点（此时node的度为1或者0） Node\u003cE\u003e replacement = node.left != null ? node.left : node.right; if (replacement != null) { //node是度为1的节点 //更改parent replacement.parent = node.parent; //更改parent的left、right指向 if (node.parent == null) { //node是度为1的节点并且是根节点 root = replacement; } else if (node == node.parent.left) { node.parent.left = replacement; } else { //此时的情况为 node == node.parent.right node.parent.right = replacement; } } else if (node.parent == null) { //node是叶子节点并且是根节点 root = null; } else { //node是叶子节点，但不是根节点 if (node == node.parent.left) { node.parent.left = null; } else { //此时的情况为 node == node.parent.right node.parent.right = null; } } } /** * 查找node的前驱节点 * \u003cp\u003e * 前驱节点：中序遍历时的前一个节点 * 如果是二叉搜索树，前驱节点就是前一个比它小的节点 * 判断条件： * 1. 如果node的左节点不为空，则node的前驱节点为node的左子树的右节点的右节点的右节点。。。直到右节点为空 * predecessor = node.left.right.right.right... 终止条件 right为null * 2. 如果node的左节点为空，父节点不为空，则node的前驱节点为node的父节点的父节点的父节点。。。直到node在父节点的右子树中 * predecessor = node.parent.parent.parent... 终止条件 node在parent的右子树中 * 3. 如果node的左子树为空且父节点为空，则该node没有前驱节点，例如没有左子树的根节点 * * @param node * @return */ private Node\u003cE\u003e predecessor(Node\u003cE\u003e node) { if (node == null) { return null; } //前驱节点在左子树中（left.right.right.right...） Node\u003cE\u003e p = node.left; if (p != null) { while (p.right != null) { p = p.right; } return p; } //node的左子树为空 //从父节点、祖父节点中寻找前驱节点 while (node.parent != null \u0026\u0026 node == node.parent.left) { node = node.parent; } //上面循环结束时的情况如下 //node.parent == null 或者 node == node.parent.right //此时node.parent为前驱节点 return node.parent; } /** * 查找node的后继节点 * \u003cp\u003e * 后继节点：中序遍历时的后一个节点 * 如果是二叉搜索树，后继节点就是后一个比它大的节点 * 判断条件 * 1. 如果node的右节点不为空，则node的后继节点为node的右子树的左节点的左节点的左节点。。。直到左节点为空 * successor = node.right.left.left.left... 终止条件 left为null * 2. 如果node的右节点为空，父节点不为空，则node的后继节点为node的父节点的父节点的父节点。。。直到node在父节点的左子树中 * successor = node.parent.parent.parent... 终止条件 node在parent的左子树中 * 3. 如果node的右子树为空且父节点为空，则该node没有前驱节点，例如没有右子树的根节点 * * @param node * @return */ private Node\u003cE\u003e successo","date":"2020-09-16","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-07-%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91/:2:2","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-07-二叉搜索树","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-07-%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91/"},{"categories":["算法"],"content":"二叉搜索树的重构 二叉树 package org.msdemt.demo.tree; import org.msdemt.demo.printer.BinaryTreeInfo; import java.util.LinkedList; import java.util.Queue; /** * 二叉树 * * @param \u003cE\u003e */ @SuppressWarnings(\"unchecked\") public class BinaryTree\u003cE\u003e implements BinaryTreeInfo { protected int size; protected Node\u003cE\u003e root; public int size() { return size; } public boolean isEmpty() { return size == 0; } public void clear() { root = null; size = 0; } /** * 前序遍历 * * @param visitor */ public void preorder(Visitor\u003cE\u003e visitor) { if (visitor == null) { return; } preorder(root, visitor); } private void preorder(Node\u003cE\u003e node, Visitor\u003cE\u003e visitor) { if (node == null || visitor.stop) { return; } visitor.stop = visitor.visit(node.element); preorder(node.left, visitor); preorder(node.right, visitor); } /** * 中序遍历 * * @param visitor */ public void inorder(Visitor\u003cE\u003e visitor) { if (visitor == null) { return; } inorder(root, visitor); } private void inorder(Node\u003cE\u003e node, Visitor\u003cE\u003e visitor) { if (node == null || visitor.stop) { return; } inorder(node.left, visitor); if (visitor.stop) { return; } visitor.stop = visitor.visit(node.element); inorder(node.right, visitor); } /** * 后续遍历 * * @param visitor */ public void postorder(Visitor\u003cE\u003e visitor) { if (visitor == null) { return; } postorder(root, visitor); } private void postorder(Node\u003cE\u003e node, Visitor\u003cE\u003e visitor) { if (node == null || visitor.stop) { return; } postorder(node.left, visitor); postorder(node.right, visitor); if (visitor.stop) { return; } visitor.stop = visitor.visit(node.element); } /** * 层序遍历 * * @param visitor */ public void levelOrder(Visitor\u003cE\u003e visitor) { if (root == null || visitor == null) { return; } Queue\u003cNode\u003cE\u003e\u003e queue = new LinkedList\u003c\u003e(); queue.offer(root); while (!queue.isEmpty()) { Node\u003cE\u003e node = queue.poll(); if (visitor.visit(node.element)) { return; } if (node.left != null) { queue.offer(node.left); } if (node.right != null) { queue.offer(node.right); } } } /** * 是否为完全二叉树 * * @return */ public boolean isComplete() { if (root == null) { return false; } Queue\u003cNode\u003cE\u003e\u003e queue = new LinkedList\u003c\u003e(); queue.offer(root); boolean leaf = false; while (!queue.isEmpty()) { Node\u003cE\u003e node = queue.poll(); if (leaf \u0026\u0026 !node.isLeaf()) { return false; } if (node.left != null) { queue.offer(node.left); } else if (node.right != null) { return false; } if (node.right != null) { queue.offer(node.right); } else { // 后面遍历的节点都必须是叶子节点 leaf = true; } } return true; } /** * 二叉树的高度 * * @return */ public int height() { if (root == null) { return 0; } // 树的高度 int height = 0; // 存储着每一层的元素数量 int levelSize = 1; Queue\u003cNode\u003cE\u003e\u003e queue = new LinkedList\u003c\u003e(); queue.offer(root); while (!queue.isEmpty()) { Node\u003cE\u003e node = queue.poll(); levelSize--; if (node.left != null) { queue.offer(node.left); } if (node.right != null) { queue.offer(node.right); } if (levelSize == 0) { // 意味着即将要访问下一层 levelSize = queue.size(); height++; } } return height; } /** * 二叉树的高度 * * @return */ public int height2() { return height(root); } private int height(Node\u003cE\u003e node) { if (node == null) { return 0; } return 1 + Math.max(height(node.left), height(node.right)); } /** * 获取node的前驱节点 * * @param node * @return */ protected Node\u003cE\u003e predecessor(Node\u003cE\u003e node) { if (node == null) { return null; } // 前驱节点在左子树当中（left.right.right.right....） Node\u003cE\u003e p = node.left; if (p != null) { while (p.right != null) { p = p.right; } return p; } // 从父节点、祖父节点中寻找前驱节点 while (node.parent != null \u0026\u0026 node == node.parent.left) { node = node.parent; } // node.parent == null // node == node.parent.right return node.parent; } /** * 获取node的后继节点 * * @param node * @return */ protected Node\u003cE\u003e successor(Node\u003cE\u003e node) { if (node == null) { return null; } // 后继节点在右子树当中（right.left.left.left....） Node\u003cE\u003e p = node.right; if (p != null) { while (p.left != null) { p = p.left; } return p; } // 从父节点、祖父节点中寻找后继节点 while (node.parent != null \u0026\u0026 node == node.parent.right) { node = node.parent; } return node.parent; } /** * 自定义节点访问策略 * * @param \u003cE\u003e */ public static abstract class Visitor\u003cE\u003e { boolean stop; ","date":"2020-09-16","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-07-%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91/:2:3","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-07-二叉搜索树","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-07-%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91/"},{"categories":["算法"],"content":"树（Tree）的基本概念 节点、根节点、父节点、子节点、兄弟节点 一棵树可以没有任何节点，称为空树 一棵树可以只有1个节点，也就是只有根节点 子树、左子树、右子树 节点的度（degree）：子树的个数 树的度：所有节点度中的最大值 叶子节点（leaf）：度为0的节点 非叶子节点：度不为0的节点 层数（level）：根节点在第1层，根节点的子节点在第2层，依此类推（有些教程也从第0层开始计算） 节点的深度（depth）：从根节点到当前节点的唯一路径上的节点总数 节点的高度（height）：从当前节点到最远叶子节点的路径上的节点总数 树的深度：所有节点深度中的最大值 树的高度：所有节点高度中的最大值 树的深度等于树的高度 有序树 树中任意节点的子节点之间有顺序关系 无序树 树中任意节点的子节点之间没有顺序关系 无序树也成为“自由树” 森林 由 m (m \u003e= 0) 颗互不相交的树组成的集合 ","date":"2020-09-16","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-06-%E4%BA%8C%E5%8F%89%E6%A0%91/:1:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-06-二叉树","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-06-%E4%BA%8C%E5%8F%89%E6%A0%91/"},{"categories":["算法"],"content":"二叉树（Binary Tree） 二叉树的特点 每个节点的度最大为2（最多拥有2颗子树） 左子树和右子树是由顺序的 即使某节点只有一颗子树，也要区分左右子树 二叉树是有序树还是无序树？ 有序树 ","date":"2020-09-16","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-06-%E4%BA%8C%E5%8F%89%E6%A0%91/:2:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-06-二叉树","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-06-%E4%BA%8C%E5%8F%89%E6%A0%91/"},{"categories":["算法"],"content":"二叉树的性质 非空二叉树的第 i 层，最多有 2i-1 个节点（i \u003e= 1） 在高度为 h 的二叉树上最多有 2h - 1 个节点（h \u003e= 1） 对于任何一颗非空二叉树，如果叶子节点个数为 n0 ，度为 2 的节点个数为 n2 ，则有：n0 = n2 + 1 假设度为 1 的节点个数为 n1 ，那么二叉树的节点总数 n = n0 + n1 + n2 二叉树的边数 T = n1 + 2*n2 = n - 1 = n0 + n1 + n2 - 1 ","date":"2020-09-16","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-06-%E4%BA%8C%E5%8F%89%E6%A0%91/:2:1","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-06-二叉树","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-06-%E4%BA%8C%E5%8F%89%E6%A0%91/"},{"categories":["算法"],"content":"真二叉树 真二叉树：所有节点的度都要么为 0 ，要么为 2 ","date":"2020-09-16","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-06-%E4%BA%8C%E5%8F%89%E6%A0%91/:2:2","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-06-二叉树","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-06-%E4%BA%8C%E5%8F%89%E6%A0%91/"},{"categories":["算法"],"content":"满二叉树（Full Binary Tree） 满二叉树：所有节点的度都要么为 0 ， 要么为 2，且所有的叶子节点都在最后一层 在同样高度的二叉树中，满二叉树的叶子节点数量最多，总节点数量最多 满二叉树一定是真二叉树，真二叉树不一定是满二叉树 假设满二叉树的高度为 h （h \u003e= 1），那么 第 i 层的节点数量：2i-1 叶子节点数量：2h-1 总节点数量：n n = 2h - 1 = 20 + 21 + 22 + … + 2h-1 h = log2(n+1) ","date":"2020-09-16","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-06-%E4%BA%8C%E5%8F%89%E6%A0%91/:2:3","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-06-二叉树","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-06-%E4%BA%8C%E5%8F%89%E6%A0%91/"},{"categories":["算法"],"content":"完全二叉树（Complete Binary Tree） 完全二叉树：叶子节点只会出现最后 2 层，且最后 1 层的叶子节点都靠左对齐 完全二叉树从根节点至倒数第 2 层是一颗满二叉树 满二叉树一定是完全二叉树，完全二叉树不一定是满二叉树 完全二叉树的性质 度为 1 的节点只有左子树 度为 1 的节点要么是 1 个，要么是 0 个 同样节点数量的二叉树，完全二叉树的高度最小 假设完全二叉树的高度为 h (h \u003e= 1)，那么 至少有 2h-1 个节点 （20 + 21 + 22 + … + 2h-2 + 1） 最多有 2h -1 个节点 （20 + 21 + 22 + … + 2h-1，满二叉树） 总节点数量为 n 2h-1 \u003c= n \u003c 2h h - 1 \u003c= log2n \u003c h h = floor(log2n) + 1 floor（向下取整）：只取前面的整数，比如 floor(4.6) 为 4 ceiling（向上取整）：如果小数不为 0，取前面的整数加 1，否则只取前面的整数。比如 ceiling(4.6) 为 5，ceiling(4.0) 为 4 foor 和 ceiling 都是不管四舍五入规则的。 一颗有 n 个节点的完全二叉树（n \u003e 0），从上到下、从左到右对节点从 0 开始编号，对任意第 i 个节点 如果 i = 0，它是根节点 如果 i \u003e 0，它的父节点编号为 floor((i-1)/2) 如果 2i + 1 \u003c= n - 1，它的左子节点编号为 2i + 1 如果 2i + 1 \u003e n - 1，它无左子节点 如果 2i + 2 \u003c= n - 1，它的右子节点编号为 2i + 2 如果 2i + 2 \u003e n - 1，它无右子节点 面试题 如果一颗完全二叉树有 768 个节点，求叶子节点的个数 假设叶子节点个数为 n0，度为 1 的节点个数为 n1，度为 2 的节点个数为 n2 总节点个数 n = n0 + n1 + n2，而且 n0 = n2 + 1 n = 2n0 + n1 - 1 完全二叉树的 n1 要么为 0，要么为 1 n1 为 1 时，n = 2n0，n 必然是偶数 叶子节点个数 n0 = n / 2，非叶子节点个数 n1 + n2 = n/2 n1 为 0 时，n = 2n0 - 1，n必然是奇数 叶子节点个数 n0 = (n+1)/2，非叶子节点个数 n1 + n2 = (n-1)/2 叶子节点个数 n0 = floor((n+1)/2) = ceiling(n/2) 非叶子节点个数 n1 + n2 = floor(n/2) = ceiling((n-1)/2) 因此叶子节点个数为384 推导过程 总节点数量 n n 如果是偶数，叶子节点数量 n0 = n / 2 n 如果是奇数，叶子节点数量 n0 = (n+1) / 2 n0 = floor((n+1)/2) 或 n0 = ceiling(n/2) java语言默认是向下取整的，所以 n0 = (n+1)/2 n0 = (n+1) » 1 ","date":"2020-09-16","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-06-%E4%BA%8C%E5%8F%89%E6%A0%91/:2:4","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-06-二叉树","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-06-%E4%BA%8C%E5%8F%89%E6%A0%91/"},{"categories":["算法"],"content":"二叉树的遍历 遍历时数据结构中的常见操作 把所有元素都访问一遍 线性数据结构的遍历比较简单 正序遍历 逆序遍历 根据节点访问顺序的不同，二叉树的常见遍历方式有4种 前序遍历（Preorder Traversal） 中序遍历（Inorder Traversal） 后序遍历（Postorder Traversal） 层序遍历（Level Order Traversal） 前序遍历（Preorder Traversal） 访问顺序 根节点、前序遍历左子树、前序遍历右子树 7、4、2、1、3、5、9、8、11、10、12 在二叉搜索树中使用递归实现前序遍历 public void preorder() { preorder(root); } private void preorder(Node\u003cE\u003e node) { if(node == null) return; System.out.println(node.element); preorder(node.left); preorder(node.right); } 中序遍历（Inorder Traversal） 访问顺序 中序遍历左子树、根节点、中序遍历右子树 1、2、3、4、5、6、7、8、9、10、11、12 如果访问顺序是下面这样呢？ 中序遍历右子树、根节点、中序遍历左子树 12、11、10、9、8、7、6、5、4、3、2、1 二叉搜索树的中序遍历结果是升序或降序的 在二叉搜索树中使用递归实现中序遍历 中序遍历升序 public void inorder() { inorder(root); } private void inorder(Node\u003cE\u003e node) { if(node == null) return; inorder(node.left); System.out.println(node.element); inorder(node.right); } 中序遍历降序 public void inorder() { inorder(root); } private void inorder(Node\u003cE\u003e node) { if(node == null) return; inorder(node.right); System.out.println(node.element); inorder(node.left); } 后序遍历（Postorder Traversal） 访问顺序 后续遍历左子树、后序遍历右子树、根节点 1、3、2、5、4、8、10、12、11、9、7 在二叉搜索树中使用递归实现后序遍历 public void postorder() { postorder(root); } private void postorder(Node\u003cE\u003e node) { if(node == null) return; postorder(node.left); postorder(node.right); System.out.println(node.element); } 层序遍历（Level Order Traversal） 访问顺序 从上到下，从左到右依次访问每一个节点 7、4、9、2、5、8、11、1、3、10、12 实现思路：使用队列 将根节点入队 循环执行以下操作，直到队列为空 将队头节点 A 出队，进行访问 将 A 的左子节点入队 将 A 的右子节点入队 使用队列实现中序遍历 public void levelOrder() { if (root == null) return; Queue\u003cNode\u003cE\u003e\u003e queue = new LinkedList\u003c\u003e(); queue.offer(root); while (!queue.isEmpty()) { Node\u003cE\u003e node = queue.poll(); System.out.println(node.element); if (node.left != null) { queue.offer(node.left); } if (node.right != null) { queue.offer(node.right); } } } 设计遍历接口 思考：如果允许外界遍历二叉树元素（自定义遍历操作）？该如何设计接口？ 通过visitor接口，用户可以自定义遍历二叉树元素时的操作 public static interface Visitor\u003cE\u003e { void visit(E element); } public void inorder(Visitor\u003cE\u003e visitor) { if (visitor == null) return; inorder(root, visitor); } private void inorder(Node\u003cE\u003e node, Visitor\u003cE\u003e visitor) { if (node == null) return; inorder(node.left, visitor); visitor.visit(node.element); inorder(node.right, visitor); } 用户遍历时传入visitor接口实现 bst.inorder(new Visitor\u003cInteger\u003e() { public void visit(Integer element) { System.out.print(\"_\" + element + \"_ \"); } }); 增强遍历接口 遍历时，支持只遍历前几个元素，然后停止遍历，这种该怎么实现呢？ public static abstract class Visitor\u003cE\u003e { boolean stop; /** * 如果返回true，表示停止遍历 * * @param element * @return */ public abstract boolean visit(E element); } public void inorder(Visitor\u003cE\u003e visitor) { if (visitor == null) { return; } inorder(root, visitor); } private void inorder(Node\u003cE\u003e node, Visitor\u003cE\u003e visitor) { if (node == null || visitor.stop) { return; } inorder(node.left, visitor); if (visitor.stop) { return; } visitor.stop = visitor.visit(node.element); inorder(node.right, visitor); } 测试代码 bst.inorder(new Visitor\u003cInteger\u003e() { @Override public boolean visit(Integer element) { System.out.print(element + \" \"); return element == 4 ? true : false; } }); 遍历的应用 前序遍历 树状结构展示（注意左右子树的顺序） 中序遍历 二叉搜索树的中序遍历按升序或降序处理节点 后序遍历 适用于一些先子后父的操作 层序遍历 计算二叉树的高度 判断一棵树是否为完全二叉树 计算二叉树的高度 递归方式实现获取二叉树的高度 public int height2() { return height(root); } private int height(Node\u003cE\u003e node) { if (node == null) { return 0; } return 1 + Math.max(height(node.left), height(node.right)); } 非递归（迭代）方式实现获取二叉树的高度 public int height() { if (root == null) { return 0; } //树的高度 int height = 0; //存储每一层的元素数量 int levelSize = 1; Queue\u003cNode\u003cE\u003e\u003e queue = new LinkedList\u003c\u003e(); queue.offer(root); while (!queue.isEmpty()) { Node\u003cE\u003e node = queue.poll(); levelSize--; if (node.left != null) { queue.offer(node.left); } if (node.right != null) { queue.offer(node.right); } if (levelSize == 0) { //意味着即将要访问下一层 levelSize = queue.size(); height++; } } return height; } 判断一棵树是否为完全二叉树 如果树为空，返回 false 如果树不为空，开始层序遍历二叉树（用队列） 如果 node.left != null \u0026\u0026 node.right != null，将 node.left、node.right按顺序入队 如果 node.left == null \u0026\u0026 node.right != null，返回 false 如果 node.left != null \u0026\u0026 node.right == null 或者 node.left ","date":"2020-09-16","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-06-%E4%BA%8C%E5%8F%89%E6%A0%91/:2:5","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-06-二叉树","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-06-%E4%BA%8C%E5%8F%89%E6%A0%91/"},{"categories":["算法"],"content":"队列（Queue） 队列是一种特殊的线性表，只能在头尾两端进行操作 队尾（rear）：只能从队尾添加元素，一般叫做 enQueue ，入队 队头（front）：只能从队头移除元素，一般叫做 deQueue ，出队 先进先出的原则，First In First Out，FIFO ","date":"2020-09-15","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-05-%E9%98%9F%E5%88%97/:1:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-05-队列","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-05-%E9%98%9F%E5%88%97/"},{"categories":["算法"],"content":"队列的接口设计 int size(); //元素的数量 boolean isEmpty(); //是否为空 void enQueue(E element); //入队 E deQueue(); //出队 E front(); //获取队列的头元素 void clear(); //清空 队列的内部实现是否可以直接利用以前学过的数据结构？ 动态数组、链表 优先使用双向链表，因为队列主要是往头尾操作元素 ","date":"2020-09-15","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-05-%E9%98%9F%E5%88%97/:1:1","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-05-队列","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-05-%E9%98%9F%E5%88%97/"},{"categories":["算法"],"content":"自定义队列的实现 使用双向链表实现队列，首先将以前的自定义双向链表拷贝过来 package org.msdemt.demo.list; public interface List\u003cE\u003e { static final int ELEMENT_NOT_FOUND = -1; /** * 清除所有元素 */ void clear(); /** * 元素的数量 * * @return */ int size(); /** * 是否为空 * * @return */ boolean isEmpty(); /** * 是否包含某个元素 * * @param element * @return */ boolean contains(E element); /** * 添加元素到尾部 * * @param element */ void add(E element); /** * 获取index位置的元素 * * @param index * @return */ E get(int index); /** * 设置index位置的元素 * * @param index * @param element * @return 原来的元素ֵ */ E set(int index, E element); /** * 在index位置插入一个元素 * * @param index * @param element */ void add(int index, E element); /** * 删除index位置的元素 * * @param index * @return */ E remove(int index); /** * 查看元素的索引 * * @param element * @return */ int indexOf(E element); } package org.msdemt.demo.list; public abstract class AbstractList\u003cE\u003e implements List\u003cE\u003e { /** * 元素的数量 */ protected int size; /** * 元素的数量 * * @return */ public int size() { return size; } /** * 是否为空 * * @return */ public boolean isEmpty() { return size == 0; } /** * 是否包含某个元素 * * @param element * @return */ public boolean contains(E element) { return indexOf(element) != ELEMENT_NOT_FOUND; } /** * 添加元素到尾部 * * @param element */ public void add(E element) { add(size, element); } protected void outOfBounds(int index) { throw new IndexOutOfBoundsException(\"Index:\" + index + \", Size:\" + size); } protected void rangeCheck(int index) { if (index \u003c 0 || index \u003e= size) { outOfBounds(index); } } protected void rangeCheckForAdd(int index) { if (index \u003c 0 || index \u003e size) { outOfBounds(index); } } } package org.msdemt.demo.list; public class LinkedList\u003cE\u003e extends AbstractList\u003cE\u003e { private Node\u003cE\u003e first; private Node\u003cE\u003e last; private static class Node\u003cE\u003e { E element; Node\u003cE\u003e prev; Node\u003cE\u003e next; public Node(Node\u003cE\u003e prev, E element, Node\u003cE\u003e next) { this.prev = prev; this.element = element; this.next = next; } @Override public String toString() { StringBuilder sb = new StringBuilder(); if (prev != null) { sb.append(prev.element); } else { sb.append(\"null\"); } sb.append(\"_\").append(element).append(\"_\"); if (next != null) { sb.append(next.element); } else { sb.append(\"null\"); } return sb.toString(); } } @Override public void clear() { size = 0; first = null; last = null; } @Override public E get(int index) { return node(index).element; } @Override public E set(int index, E element) { Node\u003cE\u003e node = node(index); E old = node.element; node.element = element; return old; } @Override public void add(int index, E element) { rangeCheckForAdd(index); // size == 0 // index == 0 if (index == size) { // 往最后面添加元素 Node\u003cE\u003e oldLast = last; last = new Node\u003c\u003e(oldLast, element, null); if (oldLast == null) { // 这是链表添加的第一个元素 first = last; } else { oldLast.next = last; } } else { Node\u003cE\u003e next = node(index); Node\u003cE\u003e prev = next.prev; Node\u003cE\u003e node = new Node\u003c\u003e(prev, element, next); next.prev = node; if (prev == null) { // index == 0 first = node; } else { prev.next = node; } } size++; } @Override public E remove(int index) { rangeCheck(index); Node\u003cE\u003e node = node(index); Node\u003cE\u003e prev = node.prev; Node\u003cE\u003e next = node.next; if (prev == null) { // index == 0 first = next; } else { prev.next = next; } if (next == null) { // index == size - 1 last = prev; } else { next.prev = prev; } size--; return node.element; } @Override public int indexOf(E element) { if (element == null) { Node\u003cE\u003e node = first; for (int i = 0; i \u003c size; i++) { if (node.element == null) return i; node = node.next; } } else { Node\u003cE\u003e node = first; for (int i = 0; i \u003c size; i++) { if (element.equals(node.element)) return i; node = node.next; } } return ELEMENT_NOT_FOUND; } /** * 获取index位置对应的节点对象 * * @param index * @return */ private Node\u003cE\u003e node(int index) { rangeCheck(index); if (index \u003c (size \u003e\u003e 1)) { Node\u003cE\u003e node = first; for (int i = 0; i \u003c index; i++) { node = node.next; } return node; } else { Node\u003cE\u003e node = last; for (int i = size - 1; i \u003e index; i--) { node = node.prev; } return node; } } @Override public String toString() { Stri","date":"2020-09-15","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-05-%E9%98%9F%E5%88%97/:1:2","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-05-队列","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-05-%E9%98%9F%E5%88%97/"},{"categories":["算法"],"content":"java官方的队列 java.util.Queue ","date":"2020-09-15","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-05-%E9%98%9F%E5%88%97/:1:3","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-05-队列","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-05-%E9%98%9F%E5%88%97/"},{"categories":["算法"],"content":"双端队列（Deque） 双端队列是能在头尾两端添加、删除的队列 英文 deque 是 double ended queue 的简称 ","date":"2020-09-15","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-05-%E9%98%9F%E5%88%97/:2:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-05-队列","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-05-%E9%98%9F%E5%88%97/"},{"categories":["算法"],"content":"双端队列的接口设计 int size(); //元素的数量 boolean isEmpty(); //是否为空 void enQueueRear(E element); //从队尾入队 E deQueueFront(); //从队头出队 void enQueueFront(E element); //从队头入队 E deQueueRear(); //从队尾出队 E front(); //获取队列的头元素 E rear(); //获取队列的尾元素 ","date":"2020-09-15","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-05-%E9%98%9F%E5%88%97/:2:1","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-05-队列","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-05-%E9%98%9F%E5%88%97/"},{"categories":["算法"],"content":"自定义双端队列的实现 使用LinkedList实现双端队列 package org.msdemt.demo; import org.msdemt.demo.list.LinkedList; import org.msdemt.demo.list.List; /** * 使用双向链表实现双端队列 */ public class Deque\u003cE\u003e { private List\u003cE\u003e list = new LinkedList\u003c\u003e(); public int size() { return list.size(); } public boolean isEmpty() { return list.isEmpty(); } public void clear() { list.clear(); } /** * 队尾入队列 * * @param element */ public void enQueueRear(E element) { list.add(element); } /** * 队头出队列 * * @return */ public E deQueueFront() { return list.remove(0); } /** * 队头入队列 * * @param element */ public void enQueueFront(E element) { list.add(0, element); } /** * 队尾出队列 * * @return */ public E deQueueRear() { return list.remove(list.size() - 1); } public E front() { return list.get(0); } public E rear() { return list.get(list.size() - 1); } } ","date":"2020-09-15","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-05-%E9%98%9F%E5%88%97/:2:2","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-05-队列","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-05-%E9%98%9F%E5%88%97/"},{"categories":["算法"],"content":"循环队列（Circle Queue） 其实队列底层也可以使用动态数组实现，并且各项接口也可以优化到 O(1) 的时间复杂度，这个用数组实现并且优化之后的队列也叫做：循环队列 循环队列底层用数组实现 动态数组实现循环队列 package org.msdemt.demo.circle; public class CircleQueue\u003cE\u003e { private int front; private int size; private E[] elements; private static final int DEFAULT_CAPACITY = 10; public CircleQueue() { elements = (E[]) new Object[DEFAULT_CAPACITY]; } public int size() { return size; } public boolean isEmpty() { return size == 0; } public void clear() { for (int i = 0; i \u003c size; i++) { elements[index(i)] = null; } front = 0; size = 0; } public void enQueue(E element) { ensureCapacity(size + 1); elements[index(size)] = element; size++; } public E deQueue() { E frontElement = elements[front]; elements[front] = null; front = index(1); size--; return frontElement; } public E front() { return elements[front]; } /** * 获取index索引位置在数组中真实的坐标 * * @param index * @return */ private int index(int index) { index += front; return index - (index \u003e= elements.length ? elements.length : 0); //return (front + index) % elements.length; } /** * 保证数组中有capacity的容量 * * @param capacity */ private void ensureCapacity(int capacity) { int oldCapacity = elements.length; if (oldCapacity \u003e= capacity) return; // 新容量为旧容量的1.5倍 int newCapacity = oldCapacity + (oldCapacity \u003e\u003e 1); E[] newElements = (E[]) new Object[newCapacity]; for (int i = 0; i \u003c size; i++) { newElements[i] = elements[index(i)]; } elements = newElements; // 重置front front = 0; } @Override public String toString() { StringBuilder string = new StringBuilder(); string.append(\"capcacity=\").append(elements.length) .append(\" size=\").append(size) .append(\" front=\").append(front) .append(\", [\"); for (int i = 0; i \u003c elements.length; i++) { if (i != 0) { string.append(\", \"); } string.append(elements[i]); } string.append(\"]\"); return string.toString(); } } 循环双端队列：可以进行两端添加、删除操作的循环队列 动态数组实现循环双端队列 package org.msdemt.demo.circle; public class CircleDeque\u003cE\u003e { private int front; private int size; private E[] elements; private static final int DEFAULT_CAPACITY = 10; public CircleDeque() { elements = (E[]) new Object[DEFAULT_CAPACITY]; } public int size() { return size; } public boolean isEmpty() { return size == 0; } public void clear() { for (int i = 0; i \u003c size; i++) { elements[index(i)] = null; } front = 0; size = 0; } /** * 从尾部入队 * * @param element */ public void enQueueRear(E element) { ensureCapacity(size + 1); elements[index(size)] = element; size++; } /** * 从头部出队 * * @param */ public E deQueueFront() { E frontElement = elements[front]; elements[front] = null; front = index(1); size--; return frontElement; } /** * 从头部入队 * * @param element */ public void enQueueFront(E element) { ensureCapacity(size + 1); front = index(-1); elements[front] = element; size++; } /** * 从尾部出队 * * @param */ public E deQueueRear() { int rearIndex = index(size - 1); E rear = elements[rearIndex]; elements[rearIndex] = null; size--; return rear; } public E front() { return elements[front]; } public E rear() { return elements[index(size - 1)]; } @Override public String toString() { StringBuilder string = new StringBuilder(); string.append(\"capcacity=\").append(elements.length) .append(\" size=\").append(size) .append(\" front=\").append(front) .append(\", [\"); for (int i = 0; i \u003c elements.length; i++) { if (i != 0) { string.append(\", \"); } string.append(elements[i]); } string.append(\"]\"); return string.toString(); } private int index(int index) { index += front; if (index \u003c 0) { return index + elements.length; } return index - (index \u003e= elements.length ? elements.length : 0); } /** * 保证要有capacity的容量 * * @param capacity */ private void ensureCapacity(int capacity) { int oldCapacity = elements.length; if (oldCapacity \u003e= capacity) return; // 新容量为旧容量的1.5倍 int newCapacity = oldCapacity + (oldCapacity \u003e\u003e 1); E[] newElements = (E[]) new Object[newCapacity]; for (int i = 0; i \u003c size; i++) { newElements[i] = elements[index(i)]; } elements = newElements; // 重置front front = 0; } } ","date":"2020-09-15","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-05-%E9%98%9F%E5%88%97/:3:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-05-队列","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-05-%E9%98%9F%E5%88%97/"},{"categories":["算法"],"content":"% 运算符优化 尽量避免使用乘（*）、除（/）、模（%）、浮点数运算，效率低下 将 (front + index) % elements.length 优化为 index - (index \u003e= elements.length ? elements.length : 0) 已知 n\u003e=0, m\u003e0 n%m 等价于 n-(m\u003en?0:m) 的前提条件：n\u003c2m public static void main(String[] args) { int n = 13; int m = 7; if (n \u003e= m) { System.out.println(n - m); } else { System.out.println(n); } // m \u003e 0, n \u003e= 0, n \u003c 2m System.out.println(n - (n \u003e= m ? m : 0)); System.out.println(n % m); } ","date":"2020-09-15","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-05-%E9%98%9F%E5%88%97/:4:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-05-队列","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-05-%E9%98%9F%E5%88%97/"},{"categories":["算法"],"content":"栈（Stack） 栈是一种特殊的线性表，只能在一端进行操作 往栈中添加元素的操作，一般叫做 push ，入栈 从栈中移除元素的操作，一般叫做 pop ，出栈（只能移除栈顶元素，也叫：弹出栈顶元素） 后进先出的原则，Last In First Out，LIFO 注意：本文说的栈和内存中的栈空间是两个不同的概念。 ","date":"2020-09-15","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-04-%E6%A0%88/:1:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-04-栈","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-04-%E6%A0%88/"},{"categories":["算法"],"content":"栈的接口设计 int size(); //元素的数量 boolean isEmpty(); //是否为空 void push(E element); //入栈 E pop(); //出栈 E top(); //获取栈顶元素 思考：栈的内部实现是否可以利用之前学习的数据结构？ 动态数组、链表 ","date":"2020-09-15","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-04-%E6%A0%88/:2:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-04-栈","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-04-%E6%A0%88/"},{"categories":["算法"],"content":"自定义栈的实现 ","date":"2020-09-15","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-04-%E6%A0%88/:3:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-04-栈","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-04-%E6%A0%88/"},{"categories":["算法"],"content":"使用继承的方式实现栈 基于以前自定义实现的动态数组或链表，使用继承的方式实现栈 首先将以前自定义的动态数组和链表拷贝过来，如下 package org.msdemt.demo.list; public interface List\u003cE\u003e { static final int ELEMENT_NOT_FOUND = -1; /** * 清除所有元素 */ void clear(); /** * 元素的数量 * @return */ int size(); /** * 是否为空 * @return */ boolean isEmpty(); /** * 是否包含某个元素 * @param element * @return */ boolean contains(E element); /** * 添加元素到尾部 * @param element */ void add(E element); /** * 获取index位置的元素 * @param index * @return */ E get(int index); /** * 设置index位置的元素 * @param index * @param element * @return */ E set(int index, E element); /** * 在index位置插入一个元素 * @param index * @param element */ void add(int index, E element); /** * 删除index位置的元素 * @param index * @return */ E remove(int index); /** * 查看元素的索引 * @param element * @return */ int indexOf(E element); } package org.msdemt.demo.list; public abstract class AbstractList\u003cE\u003e implements List\u003cE\u003e { /** * 元素的数量 */ protected int size; /** * 获取元素的数量 * @return */ public int size(){ return size; } /** * 是否为空 * @return */ public boolean isEmpty(){ return size == 0; } /** * 是否包含某个元素 * @param element * @return */ public boolean contains(E element){ return indexOf(element) != ELEMENT_NOT_FOUND; } /** * 添加元素到尾部 * @param element */ public void add(E element){ add(size, element); } protected void outOfBounds(int index){ throw new IndexOutOfBoundsException(\"Index: \" + index + \", Size:\" + size); } protected void rangeCheck(int index){ if(index \u003c 0 || index \u003e= size){ outOfBounds(index); } } protected void rangeCheckForAdd(int index){ if(index \u003c 0 || index \u003e size){ outOfBounds(index); } } } package org.msdemt.demo.list; public class ArrayList\u003cE\u003e extends AbstractList\u003cE\u003e { private E[] elements; private static final int DEFAULT_CAPACITY = 10; public ArrayList(int capacity) { capacity = (capacity \u003c DEFAULT_CAPACITY) ? DEFAULT_CAPACITY : capacity; elements = (E[]) new Object[capacity]; } public ArrayList() { this(DEFAULT_CAPACITY); } /** * 清除所有元素 */ @Override public void clear() { for (int i = 0; i \u003c size; i++) { elements[i] = null; } size = 0; } /** * 获取index位置的元素 * 该方法的时间复杂度为O(1) * * @param index * @return */ @Override public E get(int index) { rangeCheck(index); return elements[index]; } /** * 设置index位置的元素 * 该方法的时间复杂度为O(1) * * @param index * @param element * @return */ @Override public E set(int index, E element) { rangeCheck(index); E old = elements[index]; elements[index] = element; return old; } /** * 在index位置插入一个元素 * \u003cp\u003e * 时间复杂度： * 最好： O(1) * 最坏： O(n) * 平均： O(n) * size是数据规模 * * @param index * @param element */ @Override public void add(int index, E element) { rangeCheckForAdd(index); ensureCapacity(size + 1); for (int i = size; i \u003e index; i--) { elements[i] = elements[i - 1]; } elements[index] = element; size++; } /** * 删除index位置的元素 * \u003cp\u003e * 时间复杂度： * 最好： O(1) * 最坏： O(n) * 平均： O(n) * * @param index * @return */ @Override public E remove(int index) { rangeCheck(index); E old = elements[index]; for (int i = index + 1; i \u003c size; i++) { elements[i - 1] = elements[i]; } elements[--size] = null; return old; } /** * 查看元素的索引 * * @param element * @return */ @Override public int indexOf(E element) { if (element == null) { for (int i = 0; i \u003c size; i++) { if (elements[i] == null) return i; } } else { for (int i = 0; i \u003c size; i++) { if (element.equals(elements[i])) return i; } } return ELEMENT_NOT_FOUND; } /** * 保证要有capacity的容量 * * @param capacity */ private void ensureCapacity(int capacity) { int oldCapacity = elements.length; if (oldCapacity \u003e= capacity) return; //新容量为旧容量的1.5倍 int newCapacity = oldCapacity + (oldCapacity \u003e\u003e 1); E[] newElements = (E[]) new Object[newCapacity]; for (int i = 0; i \u003c size; i++) { newElements[i] = elements[i]; } elements = newElements; System.out.println(oldCapacity + \"扩容为：\" + newCapacity); } @Override public String toString() { // size=3, [99, 88, 77] StringBuilder string = new StringBuilder(); string.append(\"size=\").append(size).append(\", [\"); for (int i = 0; i \u003c size; i++) { if (i != 0) { string.append(\", \"); } string.append(elements[i]); //if (i != size - 1","date":"2020-09-15","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-04-%E6%A0%88/:3:1","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-04-栈","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-04-%E6%A0%88/"},{"categories":["算法"],"content":"使用组合的方式实现栈 package org.msdemt.demo; import org.msdemt.demo.list.ArrayList; import org.msdemt.demo.list.List; public class Stack\u003cE\u003e { private List\u003cE\u003e list = new ArrayList\u003c\u003e(); public void clear() { list.clear(); } public int size() { return list.size(); } public boolean isEmpty() { return list.isEmpty(); } public void push(E element) { list.add(element); } public E pop() { return list.remove(list.size() - 1); } public E top() { return list.get(list.size() - 1); } } ","date":"2020-09-15","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-04-%E6%A0%88/:3:2","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-04-栈","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-04-%E6%A0%88/"},{"categories":["算法"],"content":"java官方栈结构 java 官方的栈结构：java.util.Stack ","date":"2020-09-15","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-04-%E6%A0%88/:4:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-04-栈","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-04-%E6%A0%88/"},{"categories":["算法"],"content":"栈的应用 使用两个栈实现浏览器的前进和后退 第一次输入jd.com，进入第一个栈 第二次输入qq.com，进入第一个栈 第三次输入baidu.com，进入第一个栈 第四次点击后退，此时baidu.com出第一个栈，进入第二个栈 第五次点击后退，此时qq.com出第一个栈，进入第二个栈 第六次点击前进，此时qq.com出第二个站，进入第一个栈 第七次输入taobao.com，此时taobao.com入第一个栈，清空第二个栈 类似的应用场景 软件的撤销（Undo）、恢复（Redo）功能 ","date":"2020-09-15","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-04-%E6%A0%88/:5:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-04-栈","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-04-%E6%A0%88/"},{"categories":["算法"],"content":"链表（Linked List） 动态数组有个明显的缺点：可能会造成内存空间的大量浪费（每次扩容 1.5 倍） 能否用到多少就申请多少内存？链表可以做到这一点。 链表是一种链式存储的线性表，所有元素的内存地址不一定是连续的。 ","date":"2020-09-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-03-%E9%93%BE%E8%A1%A8/:1:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-03-链表","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-03-%E9%93%BE%E8%A1%A8/"},{"categories":["算法"],"content":"自定义链表 ","date":"2020-09-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-03-%E9%93%BE%E8%A1%A8/:2:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-03-链表","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-03-%E9%93%BE%E8%A1%A8/"},{"categories":["算法"],"content":"自定义链表的接口设计 自定义动态数组与链表的对外接口是一样的，所以可以使用接口（List\u003cE\u003e）进行统一声明。 自定义动态数组和链表中存在可重用的公共代码，所以可以使用抽象类（AbstractList\u003cE\u003e）存放公共代码，同时该抽象类实现接口（List\u003cE\u003e） 自定义动态数组和链表继承抽象类（AbstractList\u003cE\u003e） 抽象类（AbstractList\u003cE\u003e）对外是不可见的，所以对外界可见的元素放在接口（List\u003cE\u003e）中，如 ELEMENT_NOT_FOUND 的声明。 ","date":"2020-09-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-03-%E9%93%BE%E8%A1%A8/:2:1","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-03-链表","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-03-%E9%93%BE%E8%A1%A8/"},{"categories":["算法"],"content":"单向链表 自定义数组和链表的公共接口 public interface List\u003cE\u003e { static final int ELEMENT_NOT_FOUND = -1; /** * 清除所有元素 */ void clear(); /** * 元素的数量 * @return */ int size(); /** * 是否为空 * @return */ boolean isEmpty(); /** * 是否包含某个元素 * @param element * @return */ boolean contains(E element); /** * 添加元素到尾部 * @param element */ void add(E element); /** * 获取index位置的元素 * @param index * @return */ E get(int index); /** * 设置index位置的元素 * @param index * @param element * @return */ E set(int index, E element); /** * 在index位置插入一个元素 * @param index * @param element */ void add(int index, E element); /** * 删除index位置的元素 * @param index * @return */ E remove(int index); /** * 查看元素的索引 * @param element * @return */ int indexOf(E element); } 自定义动态数组和链表的抽象父类 public abstract class AbstractList\u003cE\u003e implements List\u003cE\u003e { /** * 元素的数量 */ protected int size; /** * 获取元素的数量 * @return */ @Override public int size(){ return size; } /** * 是否为空 * @return */ @Override public boolean isEmpty(){ return size == 0; } /** * 是否包含某个元素 * @param element * @return */ @Override public boolean contains(E element){ return indexOf(element) != ELEMENT_NOT_FOUND; } /** * 添加元素到尾部 * @param element */ @Override public void add(E element){ add(size, element); } protected void outOfBounds(int index){ throw new IndexOutOfBoundsException(\"Index: \" + index + \", Size:\" + size); } protected void rangeCheck(int index){ if(index \u003c 0 || index \u003e= size){ outOfBounds(index); } } protected void rangeCheckForAdd(int index){ if(index \u003c 0 || index \u003e size){ outOfBounds(index); } } } 自定义动态数组 public class ArrayList\u003cE\u003e extends AbstractList\u003cE\u003e { private E[] elements; private static final int DEFAULT_CAPACITY = 10; public ArrayList(int capacity) { capacity = (capacity \u003c DEFAULT_CAPACITY) ? DEFAULT_CAPACITY : capacity; elements = (E[]) new Object[capacity]; } public ArrayList() { this(DEFAULT_CAPACITY); } /** * 清除所有元素 */ @Override public void clear() { for (int i = 0; i \u003c size; i++) { elements[i] = null; } size = 0; } /** * 获取index位置的元素 * 该方法的时间复杂度为O(1) * * @param index * @return */ @Override public E get(int index) { rangeCheck(index); return elements[index]; // O(1) 数组元素访问复杂度为O(1) } /** * 设置index位置的元素 * 该方法的时间复杂度为O(1) * * @param index * @param element * @return */ @Override public E set(int index, E element) { rangeCheck(index); E old = elements[index]; elements[index] = element; return old; } /** * 在index位置插入一个元素 * \u003cp\u003e * 时间复杂度，不包括ensureCapacity() * 最好： O(1) 往最后面添加元素 * 最坏： O(n) 往0位置添加元素 * 平均： O(n) 1+2+...+n=2/n，省略常数，得到评价复杂度为O(n) * size是数据规模 * * @param index * @param element */ @Override public void add(int index, E element) { rangeCheckForAdd(index); ensureCapacity(size + 1); for (int i = size; i \u003e index; i--) { elements[i] = elements[i - 1]; } elements[index] = element; size++; } /** * 删除index位置的元素 * \u003cp\u003e * 时间复杂度： * 最好： O(1) * 最坏： O(n) * 平均： O(n) * * @param index * @return */ @Override public E remove(int index) { rangeCheck(index); E old = elements[index]; for (int i = index + 1; i \u003c size; i++) { elements[i - 1] = elements[i]; } elements[--size] = null; return old; } /** * 查看元素的索引 * * @param element * @return */ @Override public int indexOf(E element) { if (element == null) { for (int i = 0; i \u003c size; i++) { if (elements[i] == null) return i; } } else { for (int i = 0; i \u003c size; i++) { if (element.equals(elements[i])) return i; } } return ELEMENT_NOT_FOUND; } /** * 保证要有capacity的容量 * * @param capacity */ private void ensureCapacity(int capacity) { int oldCapacity = elements.length; if (oldCapacity \u003e= capacity) return; //新容量为旧容量的1.5倍 int newCapacity = oldCapacity + (oldCapacity \u003e\u003e 1); E[] newElements = (E[]) new Object[newCapacity]; for (int i = 0; i \u003c size; i++) { newElements[i] = elements[i]; } elements = newElements; System.out.println(oldCapacity + \"扩容为：\" + newCapacity); } @Override public String toString() { // size=3, [99, 88, 77] StringBuilder string = new StringBuilder(); string.append(\"size=\").append(size).append(\", [\"); for (int i = 0; i \u003c size; i++) { if (i != 0) { string.append(\", \"); } string.append(elements[i])","date":"2020-09-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-03-%E9%93%BE%E8%A1%A8/:2:2","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-03-链表","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-03-%E9%93%BE%E8%A1%A8/"},{"categories":["算法"],"content":"双向链表 使用双向链表可以提升链表的综合性能 自定义双向链表实现 /** * 双向链表 */ public class LinkedList\u003cE\u003e extends AbstractList\u003cE\u003e { private Node\u003cE\u003e first; private Node\u003cE\u003e last; private static class Node\u003cE\u003e { E element; Node\u003cE\u003e prev; Node\u003cE\u003e next; public Node(Node\u003cE\u003e prev, E element, Node\u003cE\u003e next) { this.prev = prev; this.element = element; this.next = next; } @Override public String toString() { StringBuilder sb = new StringBuilder(); if (prev != null) { sb.append(prev.element); } else { sb.append(\"null\"); } sb.append(\"_\").append(element).append(\"_\"); if (next != null) { sb.append(next.element); } else { sb.append(\"null\"); } return sb.toString(); } } @Override public void clear() { size = 0; first = null; last = null; } @Override public E get(int index) { return node(index).element; } @Override public E set(int index, E element) { Node\u003cE\u003e node = node(index); E old = node.element; node.element = element; return old; } @Override public void add(int index, E element) { rangeCheckForAdd(index); // size == 0 // index == 0 if (index == size) { // 往最后面添加元素 Node\u003cE\u003e oldLast = last; last = new Node\u003c\u003e(oldLast, element, null); if (oldLast == null) { // 这是链表添加的第一个元素 first = last; } else { oldLast.next = last; } } else { Node\u003cE\u003e next = node(index); Node\u003cE\u003e prev = next.prev; Node\u003cE\u003e node = new Node\u003c\u003e(prev, element, next); next.prev = node; if (prev == null) { // index == 0 first = node; } else { prev.next = node; } } size++; } @Override public E remove(int index) { rangeCheck(index); Node\u003cE\u003e node = node(index); Node\u003cE\u003e prev = node.prev; Node\u003cE\u003e next = node.next; if (prev == null) { // index == 0 first = next; } else { prev.next = next; } if (next == null) { // index == size - 1 last = prev; } else { next.prev = prev; } size--; return node.element; } @Override public int indexOf(E element) { if (element == null) { Node\u003cE\u003e node = first; for (int i = 0; i \u003c size; i++) { if (node.element == null) return i; node = node.next; } } else { Node\u003cE\u003e node = first; for (int i = 0; i \u003c size; i++) { if (element.equals(node.element)) return i; node = node.next; } } return ELEMENT_NOT_FOUND; } /** * 获取index位置对应的节点对象 * * @param index * @return */ private Node\u003cE\u003e node(int index) { rangeCheck(index); if (index \u003c (size \u003e\u003e 1)) { Node\u003cE\u003e node = first; for (int i = 0; i \u003c index; i++) { node = node.next; } return node; } else { Node\u003cE\u003e node = last; for (int i = size - 1; i \u003e index; i--) { node = node.prev; } return node; } } @Override public String toString() { StringBuilder string = new StringBuilder(); string.append(\"size=\").append(size).append(\", [\"); Node\u003cE\u003e node = first; for (int i = 0; i \u003c size; i++) { if (i != 0) { string.append(\", \"); } string.append(node); node = node.next; } string.append(\"]\"); return string.toString(); } } 双向链表 vs 单向链表 双向链表 vs 动态数组 动态数组：开辟、销毁内存空间的次数相对较少，但可能造成内存空间浪费（可以通过缩容解决） 双向链表：开辟、销毁内存空间的次数相对较多，但不会造成内存空间的浪费 如果频繁在尾部进行添加、删除操作，动态数组、双向链表均可选择 如果频繁在头部进行添加、删除操作，建议选择使用双向链表 如果有频繁的（在任意位置）添加、删除操作，建议选择使用双向链表 如果有频繁的查询操作（随机访问操作），建议选择使用动态数组 有了双向链表，单向链表是否就没有任何用处了？ 并非如此，在哈希表的设计中就用到了单链表 ","date":"2020-09-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-03-%E9%93%BE%E8%A1%A8/:2:3","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-03-链表","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-03-%E9%93%BE%E8%A1%A8/"},{"categories":["算法"],"content":"单向循环链表 单向循环链表的实现 import org.msdemt.demo.AbstractList; /** * 单向循环链表 * * @param \u003cE\u003e */ public class SingleCircleLinkedList\u003cE\u003e extends AbstractList\u003cE\u003e { private Node\u003cE\u003e first; private static class Node\u003cE\u003e { E element; Node\u003cE\u003e next; public Node(E element, Node\u003cE\u003e next) { this.element = element; this.next = next; } @Override public String toString() { StringBuilder sb = new StringBuilder(); sb.append(element).append(\"_\").append(next.element); return sb.toString(); } } @Override public void clear() { size = 0; first = null; } @Override public E get(int index) { return node(index).element; } @Override public E set(int index, E element) { Node\u003cE\u003e node = node(index); E old = node.element; node.element = element; return old; } @Override public void add(int index, E element) { rangeCheck(index); if (index == 0) { Node\u003cE\u003e newFirst = new Node\u003c\u003e(element, first); Node\u003cE\u003e last = (size == 0) ? newFirst : node(size - 1); last.next = newFirst; first = newFirst; } else { Node\u003cE\u003e prev = node(index - 1); prev.next = new Node\u003c\u003e(element, prev.next); } size++; } @Override public E remove(int index) { rangeCheck(index); Node\u003cE\u003e node = first; if (index == 0) { if (size == 1) { first = null; } else { Node\u003cE\u003e last = node(size - 1); first = first.next; last.next = first; } } else { Node\u003cE\u003e prev = node(index - 1); node = prev.next; prev.next = node.next; } size--; return node.element; } @Override public int indexOf(E element) { if (element == null) { Node\u003cE\u003e node = first; for (int i = 0; i \u003c size; i++) { if (node.element == null) return i; node = node.next; } } else { Node\u003cE\u003e node = first; for (int i = 0; i \u003c size; i++) { if (element.equals(node.element)) return i; node = node.next; } } return ELEMENT_NOT_FOUND; } private Node\u003cE\u003e node(int index) { rangeCheck(index); Node\u003cE\u003e node = first; for (int i = 0; i \u003c index; i++) { node = node.next; } return node; } @Override public String toString() { StringBuilder string = new StringBuilder(); string.append(\"size=\").append(size).append(\", [\"); Node\u003cE\u003e node = first; for (int i = 0; i \u003c size; i++) { if (i != 0) { string.append(\", \"); } string.append(node); node = node.next; } string.append(\"]\"); return string.toString(); } } ","date":"2020-09-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-03-%E9%93%BE%E8%A1%A8/:2:4","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-03-链表","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-03-%E9%93%BE%E8%A1%A8/"},{"categories":["算法"],"content":"双向循环链表 双向循环链表的实现 import org.msdemt.demo.AbstractList; /** * 双向循环链表 * * @param \u003cE\u003e */ public class CircleLinkedList\u003cE\u003e extends AbstractList\u003cE\u003e { private Node\u003cE\u003e first; private Node\u003cE\u003e last; private Node\u003cE\u003e current; private static class Node\u003cE\u003e { E element; Node\u003cE\u003e prev; Node\u003cE\u003e next; public Node(Node\u003cE\u003e prev, E element, Node\u003cE\u003e next) { this.prev = prev; this.element = element; this.next = next; } @Override public String toString() { StringBuilder sb = new StringBuilder(); if (prev != null) { sb.append(prev.element); } else { sb.append(\"null\"); } sb.append(\"_\").append(element).append(\"_\"); if (next != null) { sb.append(next.element); } else { sb.append(\"null\"); } return sb.toString(); } } public void reset() { current = first; } public E next() { if (current == null) return null; current = current.next; return current.element; } public E remove() { if (current == null) return null; Node\u003cE\u003e next = current.next; E element = remove(current); if (size == 0) { current = null; } else { current = next; } return element; } @Override public void clear() { size = 0; first = null; last = null; } @Override public E get(int index) { return node(index).element; } @Override public E set(int index, E element) { Node\u003cE\u003e node = node(index); E old = node.element; node.element = element; return old; } @Override public void add(int index, E element) { rangeCheckForAdd(index); if (index == size) { //往最后添加元素 Node\u003cE\u003e oldLast = last; last = new Node\u003c\u003e(oldLast, element, first); if (oldLast == null) { //链表添加的第一个元素 first = last; first.next = first; first.prev = first; } else { oldLast.next = last; first.prev = last; } } else { Node\u003cE\u003e next = node(index); Node\u003cE\u003e prev = next.prev; Node\u003cE\u003e node = new Node\u003c\u003e(prev, element, next); next.prev = node; prev.next = node; if (next == first) { //index == 0 first = node; } } size++; } @Override public E remove(int index) { rangeCheck(index); return remove(node(index)); } @Override public int indexOf(E element) { if (element == null) { Node\u003cE\u003e node = first; for (int i = 0; i \u003c size; i++) { if (node.element == null) return i; node = node.next; } } else { Node\u003cE\u003e node = first; for (int i = 0; i \u003c size; i++) { if (element.equals(node.element)) return i; node = node.next; } } return ELEMENT_NOT_FOUND; } private E remove(Node\u003cE\u003e node) { if (size == 1) { first = null; last = null; } else { Node\u003cE\u003e prev = node.prev; Node\u003cE\u003e next = node.next; prev.next = next; next.prev = prev; if (node == first) { //index == 0 first = next; } if (node == last) { //index =- size -1 last = prev; } } size--; return node.element; } private Node\u003cE\u003e node(int index) { rangeCheck(index); if (index \u003c (size \u003e\u003e 1)) { Node\u003cE\u003e node = first; for (int i = 0; i \u003c index; i++) { node = node.next; } return node; } else { Node\u003cE\u003e node = last; for (int i = size - 1; i \u003e index; i--) { node = node.prev; } return node; } } @Override public String toString() { StringBuilder string = new StringBuilder(); string.append(\"size=\").append(size).append(\", [\"); Node\u003cE\u003e node = first; for (int i = 0; i \u003c size; i++) { if (i != 0) { string.append(\", \"); } string.append(node); node = node.next; } string.append(\"]\"); return string.toString(); } } ","date":"2020-09-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-03-%E9%93%BE%E8%A1%A8/:2:5","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-03-链表","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-03-%E9%93%BE%E8%A1%A8/"},{"categories":["算法"],"content":"使用循环链表解决约瑟夫问题 如何发挥循环链表的最大威力？ 可以考虑增设1个成员变量、3个方法 current：用于指向某个节点 void reset()：让 current 指向头节点 first E next()：让 current 往后走一步，也就是 current = current.next E remove()：删除 current 指向的节点，删除成功后让 current 指向下一个节点 上文双向循环链表已增设了1个成员变量、3个方法 也可以在单向循环链表增设如上1个成员变量、3个方法来解决约瑟夫问题。 static void josephus() { CircleLinkedList\u003cInteger\u003e list = new CircleLinkedList\u003c\u003e(); for (int i = 1; i \u003c= 8; i++) { list.add(i); } // 指向头结点（指向1） list.reset(); while (!list.isEmpty()) { list.next(); list.next(); System.out.println(list.remove()); } } public static void main(String[] args) { josephus(); } ","date":"2020-09-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-03-%E9%93%BE%E8%A1%A8/:2:6","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-03-链表","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-03-%E9%93%BE%E8%A1%A8/"},{"categories":["算法"],"content":"静态链表 前面所学习的链表，是依赖于指针（引用）实现的 有些编程语言是没有指针的，如早期的 BASIC、FORTRAN 语言 没有指针的情况下，如何实现链表？ 可以通过数组来模拟链表，称为静态链表 数组的每个元素存放两个数据：值、下个元素的索引 数组 0 位置存放的是头结点信息 思考：如果数组的每个元素只能存放1个数据呢？ 那就使用 2 个数组，1 个数组存放索引关系，1 个数组存放值。 ","date":"2020-09-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-03-%E9%93%BE%E8%A1%A8/:2:7","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-03-链表","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-03-%E9%93%BE%E8%A1%A8/"},{"categories":["算法"],"content":"ArrayList 能否进一步优化？ 增加或删除首元素时，更改first指针即可。 增加或删除中间元素最多挪动一半的元素 ","date":"2020-09-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-03-%E9%93%BE%E8%A1%A8/:2:8","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-03-链表","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-03-%E9%93%BE%E8%A1%A8/"},{"categories":["算法"],"content":"数据结构 数据结构是计算机存储、组织数据的方式 数据结构可视化网站 https://visualgo.net/zh ","date":"2020-09-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-02-%E5%8A%A8%E6%80%81%E6%95%B0%E7%BB%84/:1:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-02-动态数组","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-02-%E5%8A%A8%E6%80%81%E6%95%B0%E7%BB%84/"},{"categories":["算法"],"content":"线性表 线性表是具有 n 个相同类型元素的有限序列（n\u003e=0） 线性表的特点：具有索引 a1 是首节点（首元素），an 是尾节点（尾元素） a1 是 a2 的前驱，a2 是 a1 的后继。 常见的线性表 数组 链表 栈 队列 哈希表（散列表） ","date":"2020-09-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-02-%E5%8A%A8%E6%80%81%E6%95%B0%E7%BB%84/:2:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-02-动态数组","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-02-%E5%8A%A8%E6%80%81%E6%95%B0%E7%BB%84/"},{"categories":["算法"],"content":"数组（Array） 数组是一种顺序存储的线性表，所有元素的内存地址是连续的。 int[] array = new int[]{11, 22, 33}; 在很多编程语言中，数组都有个致命的缺点：无法动态修改容量。 在实际开发中，我们更希望数组的容量是可以动态改变的。 ","date":"2020-09-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-02-%E5%8A%A8%E6%80%81%E6%95%B0%E7%BB%84/:3:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-02-动态数组","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-02-%E5%8A%A8%E6%80%81%E6%95%B0%E7%BB%84/"},{"categories":["算法"],"content":"自定义动态数组 自己设计一个动态数组，模仿 java 官方的动态数组 ArrayList 动态数组（Dynamic Array）接口设计 我们期望设计的动态数组具有如下接口 int size(); //元素的数量 boolean isEmpty(); //是否为空 boolean contains(E element); //是否包含某个元素 void add(E element); //添加元素到最后 E get(int index); //返回index位置对应的元素 E set(int index, E element); //设置index位置的元素 void add(int index, E element); //往index位置添加元素 E remove(int index); //删除index位置对应的元素 int indexOf(E element); //查看元素的位置 void clear(); //清除所有元素 自定义动态数组 ArrayList 实现 /** * 自定义动态数组ArrayList */ public class ArrayList\u003cE\u003e { /** * 元素的数量 */ private int size; /** * 所有的元素 */ private E[] elements; private static final int DEFAULT_CAPACITY = 10; private static final int ELEMENT_NOT_FOUND = -1; public ArrayList(int capacity) { capacity = (capacity \u003c DEFAULT_CAPACITY) ? DEFAULT_CAPACITY : capacity; elements = (E[]) new Object[capacity]; } public ArrayList() { this(DEFAULT_CAPACITY); } /** * 清除所有元素 */ public void clear() { for (int i = 0; i \u003c size; i++) { elements[i] = null; } size = 0; } /** * 元素的数量 * * @return */ public int size() { return size; } /** * 是否为空 * * @return */ public boolean isEmpty() { return size == 0; } /** * 是否包含某个元素 * * @param element * @return */ public boolean contains(E element) { return indexOf(element) != ELEMENT_NOT_FOUND; } /** * 添加元素到尾部 * * @param element */ public void add(E element) { add(size, element); } /** * 获取index位置的元素 * * @param index * @return */ public E get(int index) { rangeCheck(index); return elements[index]; } /** * 设置index位置的元素 * * @param index * @param element * @return */ public E set(int index, E element) { rangeCheck(index); E old = elements[index]; elements[index] = element; return old; } /** * 在index位置插入一个元素 * * @param index * @param element */ public void add(int index, E element) { rangeCheckForAdd(index); ensureCapacity(size + 1); for (int i = size; i \u003e index; i--) { elements[i] = elements[i - 1]; } elements[index] = element; size++; } /** * 删除index位置的元素 * * @param index * @return */ public E remove(int index) { rangeCheck(index); E old = elements[index]; for (int i = index + 1; i \u003c size; i++) { elements[i - 1] = elements[i]; } elements[--size] = null; return old; } /** * 查看元素的索引 * * @param element * @return */ public int indexOf(E element) { if (element == null) { for (int i = 0; i \u003c size; i++) { if (elements[i] == null) return i; } } else { for (int i = 0; i \u003c size; i++) { if (element.equals(elements[i])) return i; } } return ELEMENT_NOT_FOUND; } public int indexOf2(E element) { for (int i = 0; i \u003c size; i++) { if (valEquals(element, elements[i])) return i; } return ELEMENT_NOT_FOUND; } private boolean valEquals(Object v1, Object v2) { return v1 == null ? v2 == null : v1.equals(v2); } /** * 保证有capacity的容量 * * @param capacity */ private void ensureCapacity(int capacity) { int oldCapacity = elements.length; if (oldCapacity \u003e= capacity) return; //新容量为旧容量的1.5倍 int newCapacity = oldCapacity + (oldCapacity \u003e\u003e 1); E[] newElements = (E[]) new Object[newCapacity]; for (int i = 0; i \u003c size; i++) { newElements[i] = elements[i]; } elements = newElements; System.out.println(oldCapacity + \"扩容为：\" + newCapacity); } private void rangeCheck(int index) { if (index \u003c 0 || index \u003e= size) { outOfBounds(index); } } private void rangeCheckForAdd(int index) { if (index \u003c 0 || index \u003e size) { outOfBounds(index); } } private void outOfBounds(int index) { throw new IndexOutOfBoundsException(\"Index: \" + index + \", Size: \" + size); } @Override public String toString() { StringBuilder sb = new StringBuilder(); sb.append(\"size=\").append(size).append(\", [\"); for (int i = 0; i \u003c size; i++) { if (i != 0) { sb.append(\", \"); } sb.append(elements[i]); //不建议采用此方式拼接，因为此方式多一个减法运算 /* if(i != size -1){ sb.append(\", \"); } */ } sb.append(\"]\"); return sb.toString(); } } ","date":"2020-09-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-02-%E5%8A%A8%E6%80%81%E6%95%B0%E7%BB%84/:3:1","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-02-动态数组","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-02-%E5%8A%A8%E6%80%81%E6%95%B0%E7%BB%84/"},{"categories":["算法"],"content":"什么是算法 算法是用于解决特定问题的一系列的执行步骤 /** * 计算 a 与 b 的和 * * @param a * @param b * @return */ public static int plus(int a, int b) { return a + b; } /** * 计算 1+2+3+...+n 的和 * * @param n * @return */ public static int sum(int n) { int result = 0; for (int i = 1; i \u003c= n; i++) { result += i; } return result; } 使用不同的算法，解决同一个问题，效率可能相差非常大。 比如：求第 n 个斐波那契数（fibonacci number） ","date":"2020-09-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-01-%E5%A4%8D%E6%9D%82%E5%BA%A6/:1:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-01-复杂度","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-01-%E5%A4%8D%E6%9D%82%E5%BA%A6/"},{"categories":["算法"],"content":"如何评判一个算法的优劣？ /** * 计算 1+2+3+...+n 的和 * * @param n * @return */ public static int sum(int n) { int result = 0; for (int i = 1; i \u003c= n; i++) { result += i; } return result; } /** * 计算 1+2+3+...+n 的和 * @param n * @return */ public static int sum2(int n) { return (1 + n) * n / 2; } 如果单从执行效率上进行评估，可能会想到一种方案，即比较不同算法对同一组输入的执行处理时间，这种方案也叫事后统计法。 事后统计法有比较明显的缺点，缺点如下 执行时间严重依赖硬件以及运行时各种不确定的环境因素 必须编写相应的测算代码 测试数据的选择比较难保证公正性 一般我们从以下维度来评估算法的优劣 正确性、可读性、健壮性（对不合理输入的反应能力和处理能力） 时间复杂度（time complexity）：估算程序指令的执行次数（执行时间） 空间复杂度（space complexity）：估算所需占用的存储空间 ","date":"2020-09-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-01-%E5%A4%8D%E6%9D%82%E5%BA%A6/:2:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-01-复杂度","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-01-%E5%A4%8D%E6%9D%82%E5%BA%A6/"},{"categories":["算法"],"content":"大 O 表示法（Big O） 一般用大 O 表示法来描述复杂度，它表示的是数据规模 n 对应的复杂度。 使用大 O 表示法时，忽略常数、系数、低阶，如下 9 » O(1) 2n+3 » O(n) n2+2n+6 » O(n2) 4n3+3n2+22n+100 » O(n3) 注意：大 O 表示法仅仅是一种粗略的分析模型，是一种估算，能帮助我们短时间内了解一个算法的执行效率。 ","date":"2020-09-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-01-%E5%A4%8D%E6%9D%82%E5%BA%A6/:3:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-01-复杂度","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-01-%E5%A4%8D%E6%9D%82%E5%BA%A6/"},{"categories":["算法"],"content":"对数阶的细节 对数阶一般省略底数 log2n = log29 * log9n 所以 log2n 、 log9n 统称为log n ","date":"2020-09-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-01-%E5%A4%8D%E6%9D%82%E5%BA%A6/:3:1","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-01-复杂度","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-01-%E5%A4%8D%E6%9D%82%E5%BA%A6/"},{"categories":["算法"],"content":"常见的复杂度 执行次数 复杂度 非正式术语 12 O(1) 常数阶 2n+3 O(n) 线性阶 4n2+2n+6 O(n2) 平方阶 4log2n+25 O(log n) 对数阶 3n+2nlog3n+15 O(nlog n) nlog n 阶 4n3+3n2+22n+100 O(n3) 立方阶 2n O(2n) 指数阶 复杂度大小比较 O(1) \u003c O(log n) \u003c O(n) \u003c O(nlog n) \u003c O(n2) \u003c O(n3) \u003c O(2n) \u003c O(n!) \u003c O(nn) 可以借助函数生成工具对比复杂度的大小 https://zh.numberempire.com/graphingcalculator.php 数据规模较小时，复杂度对比如下 数据规模较大时，复杂度对比如下 ","date":"2020-09-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-01-%E5%A4%8D%E6%9D%82%E5%BA%A6/:3:2","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-01-复杂度","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-01-%E5%A4%8D%E6%9D%82%E5%BA%A6/"},{"categories":["算法"],"content":"多个数据规模的情况 public static void test(int n, int k) { for (int i = 0; i \u003c n; i++) { System.out.println(\"test\"); } for (int i = 0; i \u003c k; i++) { System.out.println(\"test\"); } } test()函数的时间复杂度为 O(n+k) ","date":"2020-09-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-01-%E5%A4%8D%E6%9D%82%E5%BA%A6/:3:3","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-01-复杂度","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-01-%E5%A4%8D%E6%9D%82%E5%BA%A6/"},{"categories":["算法"],"content":"斐波那契函数 斐波那契数：第一项为 0，第二项为 1，从第三项开始，每一项都等于前两项之和。如下所示 0, 1, 1, 2, 3, 5, 8, 13, … … ","date":"2020-09-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-01-%E5%A4%8D%E6%9D%82%E5%BA%A6/:4:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-01-复杂度","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-01-%E5%A4%8D%E6%9D%82%E5%BA%A6/"},{"categories":["算法"],"content":"使用递归方式实现求第 n 个斐波那契数 /** * 递归方式实现求第N个斐波那契数 * \u003cp\u003e * 若n为64，则结果迟迟出不来，说明该算法效率很低 * 时间复杂度为O(2^n) * @param n * @return */ public static int fib1(int n) { if (n \u003c= 1) return n; return fib1(n - 1) + fib1(n - 2); } 该方式的时间复杂度为 O(2n) 图解： 上图表示了 fib(5) 计算过程的递归树 算法： 检查整数 n ，如果 n 小于等于 1 ， 则返回 n 。 否则，通过递归关系： f(n) = f(n-1) + f(n-2) 调用自身。 直到所有计算返回结果得到答案。 复杂度分析 时间复杂度：O(2n)。这是计算斐波那契数最慢的方法。因为它需要指数的时间。 空间复杂度：O(n)，在堆栈中我们需要与 n 成正比的空间大小。该堆栈跟踪 fib(n) 的函数调用，随着堆栈的不断增长如果没有足够的内存则会导致 StackOverflowError。 ","date":"2020-09-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-01-%E5%A4%8D%E6%9D%82%E5%BA%A6/:4:1","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-01-复杂度","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-01-%E5%A4%8D%E6%9D%82%E5%BA%A6/"},{"categories":["算法"],"content":"使用循环方式实现求第 n 个斐波那契数 该方式的时间复杂度为 O(n) /** * for循环实现求第N个斐波那契数 * 时间复杂度为O(n) * @param n * @return */ public static int fib2(int n) { if (n \u003c= 1) return n; int first = 0; int second = 1; for (int i = 0; i \u003c n - 1; i++) { int sum = first + second; first = second; second = sum; } return second; } /** * 使用while循环实现求第N个斐波那契数 * 时间复杂度为O(n) * 优化：循环体中节省一个局部变量（sum），使用while循环条件中节省一个局部变量 * @param n * @return */ public static int fib3(int n) { if (n \u003c= 1) return n; int first = 0; int second = 1; while (n-- \u003e 1) { second += first; first = second - first; } return second; } ","date":"2020-09-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-01-%E5%A4%8D%E6%9D%82%E5%BA%A6/:4:2","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-01-复杂度","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-01-%E5%A4%8D%E6%9D%82%E5%BA%A6/"},{"categories":["算法"],"content":"使用线性代数方式实现求第 n 个斐波那契数 /** * 使用线性代数实现求第N个斐波那契数 * 时间复杂度为O(1) * @param n * @return */ public static int fib4(int n){ double c = Math.sqrt(5); return (int) ((Math.pow((1+c)/2, n) - Math.pow((1-c)/2, n))/c); } 复杂度视为 O(1) 。 ","date":"2020-09-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-01-%E5%A4%8D%E6%9D%82%E5%BA%A6/:4:3","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-01-复杂度","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-01-%E5%A4%8D%E6%9D%82%E5%BA%A6/"},{"categories":["算法"],"content":"leetcode leetcode 中斐波那契数练习题： https://leetcode-cn.com/problems/fibonacci-number/ 更多斐波那契数列解法参考：https://leetcode-cn.com/problems/fibonacci-number/solution/fei-bo-na-qi-shu-by-leetcode/ ","date":"2020-09-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-01-%E5%A4%8D%E6%9D%82%E5%BA%A6/:4:4","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-01-复杂度","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-01-%E5%A4%8D%E6%9D%82%E5%BA%A6/"},{"categories":["算法"],"content":"算法的优化方向 用尽量少的存储空间 用尽量少的执行步骤（执行时间） 根据情况，可以 空间换时间 时间换空间 ","date":"2020-09-09","objectID":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-01-%E5%A4%8D%E6%9D%82%E5%BA%A6/:5:0","tags":["算法","数据结构","java"],"title":"恋上数据结构与算法-1-01-复杂度","uri":"/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%81%8B%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-1-01-%E5%A4%8D%E6%9D%82%E5%BA%A6/"},{"categories":["documentation"],"content":"20190620 问题： 下午出现了线上环境k8s上的docker容器服务无法接收请求的问题。具体表现如下 [root@k8s-m1 ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-m1 NotReady master,worker 256d v1.6.2 k8s-m2 Ready master,worker 256d v1.6.2 k8s-m3 Ready master,worker 256d v1.6.2 [root@k8s-m1 ~]# node的状态为NotReady，但是可以执行k8s的命令。 若执行命令kubectl delete -f 8081.yaml 删除Deployment的yaml文件，则该部署的pod一直处于Terminating状态。 若执行命令kubectl create -f 8081.yaml新增加部署，则该pod一直处于pending状态， 查看该部署的状态 kubectl describe pods ，日志显示无可用节点。 查看系统日志 tail -1000f /var/log/message 日志显示： Jun 19 16:18:06 clfw1 kube-apiserver: E0619 16:18:06.190083 10596 authentication.go:58] Unable to authenticate the request due to an error: x509: certificate has expired or is not yet valid kube-apiserver组件提示无法通过证书认证请求，因为证书已过期或目前不可用。 分析：证书过期，导致服务不可用。 模拟证书过期的情况，将测试环境的k8s集群的时间调整到证书到期的时间之后。 查看测试环境证书的到期时间，使用如下命令查看admin证书的到期时间，pem证书同样可以使用此命令查看。 [root@c48-1 pki]# openssl x509 -noout -in admin.crt -enddate notAfter=Sep 30 02:43:06 2046 GMT 该证书到期时间是2046年9月30日。 将系统时间调整到证书到期时间之后，使用如下命令调整系统时间 [root@c48-1 pki]# date -s '2047-09-09 12:00' 将系统时间调整到2047年，观察证书到期后k8s的具体情况， [root@c48-1 pki]# date -s '2047-09-09 12:00' 2047年 09月 09日 星期一 12:00:00 CST [root@c48-1 pki]# kubectl get nodes Unable to connect to the server: x509: certificate has expired or is not yet valid [root@c48-1 pki]# kubectl get pods Unable to connect to the server: x509: certificate has expired or is not yet valid [root@c48-1 pki]# kubectl get cs Unable to connect to the server: x509: certificate has expired or is not yet valid [root@c48-1 pki]# 发现，若证书到期，k8s的命令无法执行，且k8s上的容器服务无法正常提供服务。 线上问题解决办法： 重新制作k8s各个组件证书 制作方法请参考https://github.com/opsnull/follow-me-install-kubernetes-cluster分支1.6.2 目前线上环境证书配置为10年。 证书截至日期2029年6月16日 [root@slfw2 ssl]# openssl x509 -noout -in admin.pem -enddate notAfter=Jun 16 14:13:00 2029 GMT 线上问题原因分析： k8s集群内的etcd组件证书过期，其他组件证书有效，导致了kube-apiserver访问etcd时提示证书失效。 上次更新k8s证书有效期为100年时，遗漏etcd证书导致的此次问题。 ","date":"2020-06-23","objectID":"/posts/k8s/k8s%E8%AF%81%E4%B9%A6%E8%BF%87%E6%9C%9F%E9%97%AE%E9%A2%98/:1:0","tags":["k8s"],"title":"K8s证书过期问题","uri":"/posts/k8s/k8s%E8%AF%81%E4%B9%A6%E8%BF%87%E6%9C%9F%E9%97%AE%E9%A2%98/"},{"categories":["documentation"],"content":"20200622 k8s线上环境出现问题，服务无法部署成功，且原有的pod无法删除（通过 kubectl delete -f abc.yaml 提示删除成功，但是实际docker容器还是存在） 查看系统日志，发现是证书问题 kube-apiserver: E0622 19:41:05.146795 1021 authentication.go:58] Unable to authenticate the request due to an error: x509: certificate has expired or is not yet valid kube-apiserver: I0622 19:41:05.146860 1021 wrap.go:75] GET /api/v1/nodes/172.17.60.112?resourceVersion=0: (81.124µs) 401 [[kubelet/v1.6.2 (linux/amd64) kubernetes/477efc3] 172.17.60.112:32843] kubelet: E0622 19:41:05.147265 2153 kubelet_node_status.go:326] Error updating node status, will retry: error getting node \"172.17.60.112\": the server has asked for the client to provide credentials (get nodes 172.17.60.112) kube-apiserver: E0622 19:41:05.147554 1021 authentication.go:58] Unable to authenticate the request due to an error: x509: certificate has expired or is not yet valid kube-apiserver: I0622 19:41:05.147586 1021 wrap.go:75] GET /api/v1/nodes/172.17.60.112: (44.506µs) 401 [[kubelet/v1.6.2 (linux/amd64) kubernetes/477efc3] 172.17.60.112:32843] kube-apiserver: E0622 19:41:05.148081 1021 authentication.go:58] Unable to authenticate the request due to an error: x509: certificate has expired or is not yet valid kubelet: E0622 19:41:05.147843 2153 kubelet_node_status.go:326] Error updating node status, will retry: error getting node \"172.17.60.112\": the server has asked for the client to provide credentials (get nodes 172.17.60.112) kubelet: E0622 19:41:05.148392 2153 kubelet_node_status.go:326] Error updating node status, will retry: error getting node \"172.17.60.112\": the server has asked for the client to provide credentials (get nodes 172.17.60.112) kubelet: E0622 19:41:05.148906 2153 kubelet_node_status.go:326] Error updating node status, will retry: error getting node \"172.17.60.112\": the server has asked for the client to provide credentials (get nodes 172.17.60.112) kube-apiserver: I0622 19:41:05.148110 1021 wrap.go:75] GET /api/v1/nodes/172.17.60.112: (52.508µs) 401 [[kubelet/v1.6.2 (linux/amd64) kubernetes/477efc3] 172.17.60.112:32843] kube-apiserver: E0622 19:41:05.148627 1021 authentication.go:58] Unable to authenticate the request due to an error: x509: certificate has expired or is not yet valid kube-apiserver: I0622 19:41:05.148655 1021 wrap.go:75] GET /api/v1/nodes/172.17.60.112: (35.999µs) 401 [[kubelet/v1.6.2 (linux/amd64) kubernetes/477efc3] 172.17.60.112:32843] kube-apiserver: E0622 19:41:05.149120 1021 authentication.go:58] Unable to authenticate the request due to an error: x509: certificate has expired or is not yet valid kube-apiserver: I0622 19:41:05.149149 1021 wrap.go:75] GET /api/v1/nodes/172.17.60.112: (36.26µs) 401 [[kubelet/v1.6.2 (linux/amd64) kubernetes/477efc3] 172.17.60.112:32843] kubelet: E0622 19:41:05.149475 2153 kubelet_node_status.go:326] Error updating node status, will retry: error getting node \"172.17.60.112\": the server has asked for the client to provide credentials (get nodes 172.17.60.112) kubelet: E0622 19:41:05.149487 2153 kubelet_node_status.go:318] Unable to update node status: update node status exceeds retry count kubelet证书过期，查看 k8s 的证书有效期 # openssl x509 -noout -in ca.pem -enddate notAfter=Jun 17 15:11:00 2024 GMT # openssl x509 -noout -in /etc/etcd/ssl/etcd.pem -enddate notAfter=Jun 16 15:12:00 2029 GMT # openssl x509 -noout -in admin.pem -enddate notAfter=Jun 16 15:14:00 2029 GMT # openssl x509 -noout -in /etc/flanneld/ssl/flanneld.pem -enddate notAfter=Jun 16 15:15:00 2029 GMT # openssl x509 -noout -in kubernetes.pem -enddate notAfter=Jun 16 15:16:00 2029 GMT # openssl x509 -noout -in kube-proxy.pem -enddate notAfter=Jun 16 15:19:00 2029 GMT # openssl x509 -noout -in kubelet.crt -enddate notAfter=Jun 19 09:22:59 2019 GMT # openssl x509 -noout -in kubelet-client.crt -enddate notAfter=Jun 18 14:05:00 2020 GMT kubelet.crt 证书是创建k8s集群时，kubelet首次启动时生成的证书，kubelet-client.crt是kubelet与kube-apiserver交互使用的证书，kubelet-client. kubelet证书续期: 删除 kubelet.kubeconfig 文件（文件位置：/etc/kubernetes ，此处安全起见，使用备份的方式） mv kubelet","date":"2020-06-23","objectID":"/posts/k8s/k8s%E8%AF%81%E4%B9%A6%E8%BF%87%E6%9C%9F%E9%97%AE%E9%A2%98/:2:0","tags":["k8s"],"title":"K8s证书过期问题","uri":"/posts/k8s/k8s%E8%AF%81%E4%B9%A6%E8%BF%87%E6%9C%9F%E9%97%AE%E9%A2%98/"},{"categories":null,"content":"一个 流程图 mermaid 示例: {{\u003c mermaid \u003e}} graph LR; A[Hard edge] --\u003e|Link text| B(Round edge) B --\u003e C{Decision} C --\u003e|One| D[Result one] C --\u003e|Two| E[Result two] {{\u003c /mermaid \u003e}} 呈现的输出效果如下: ","date":"2020-06-07","objectID":"/posts/hugo/mermaid/:0:0","tags":null,"title":"Mermaid","uri":"/posts/hugo/mermaid/"},{"categories":null,"content":" 参考： https://blog.csdn.net/ouyangxs/article/details/84193077 https://blog.csdn.net/weixin_30897233/article/details/96292006 https://jingyan.baidu.com/article/ab0b563061fa84c15afa7d8c.html 使用 ultraiso 制作 centos7 u 盘启动盘后，使用 u 盘启动盘安装 centos7 系统，出现如下的错误： 原因： 使用 ultraiso 制作的 centos7 u 盘启动盘的卷标是 CentOS 7 x8，然而 centos7 系统镜像中的 EFI/BOOT/grub.cfg 文件中卷标写的是 CentOS\\x207\\x20x86_64，isolinux/isolinux.cfg 文件中卷标写的是 CentOS\\x207\\x20x86_64 ，isolinux/syslinux.cfg 文件中卷标写的是 CentOS\\x207\\x20x8 Win 系统下 fat32 分区卷的信息只能写入 11 字符而且不可以有 \\ 字符，而且只能为大写字母，如果是小写字母会自动转为大写。 解决办法一： 修改 centos7 u 盘启动盘 grub.cfg 文件中的 CentOS\\x207\\x20x86_64 、isolinux.cfg 文件中的 CentOS\\x207\\x20x86_64 、syslinux.cfg 文件中 CentOS\\x207\\x20x8 统一修改为 CENTOS7 或者其他大写字母。 将 u 盘启动盘的盘符修改为 CENTOS7 或者其他大写字母，与上面的修改保持一致。 然后就可以使用该 u 盘启动盘安装 centos7 系统了。 解决方法二： 参考： https://jingyan.baidu.com/article/ab0b563061fa84c15afa7d8c.html ","date":"2020-06-04","objectID":"/posts/linux/ultraiso%E5%88%B6%E4%BD%9Ccentos7%E5%90%AF%E5%8A%A8%E7%9B%98%E5%AE%89%E8%A3%85%E7%B3%BB%E7%BB%9F%E6%97%B6%E6%89%BE%E4%B8%8D%E5%88%B0%E7%A3%81%E7%9B%98/:0:0","tags":["java"],"title":"Ultraiso制作centos7启动盘安装系统时找不到磁盘","uri":"/posts/linux/ultraiso%E5%88%B6%E4%BD%9Ccentos7%E5%90%AF%E5%8A%A8%E7%9B%98%E5%AE%89%E8%A3%85%E7%B3%BB%E7%BB%9F%E6%97%B6%E6%89%BE%E4%B8%8D%E5%88%B0%E7%A3%81%E7%9B%98/"},{"categories":null,"content":"第一章 File类 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:0:0","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"1.1 概述 java.io.File 类是文件和目录路径名的抽象表示，主要用于文件和目录的创建、查找和删除等操作。 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:1:0","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"1.2 构造方法 public File(String pathname) ：通过将给定的路径名字符串转换为抽象路径名来创建新的 File实例。 public File(String parent, String child) ：从父路径名字符串和子路径名字符串创建新的 File实例。 public File(File parent, String child) ：从父抽象路径名和子路径名字符串创建新的 File实例。 构造举例，代码如下： // 文件路径名 String pathname = \"D:\\\\aaa.txt\"; File file1 = new File(pathname); // 文件路径名 String pathname2 = \"D:\\\\aaa\\\\bbb.txt\"; File file2 = new File(pathname2); // 通过父路径和子路径字符串 String parent = \"d:\\\\aaa\"; String child = \"bbb.txt\"; File file3 = new File(parent, child); // 通过父级File对象和子路径字符串 File parentDir = new File(\"d:\\\\aaa\"); String child = \"bbb.txt\"; File file4 = new File(parentDir, child); 小贴士： 一个File对象代表硬盘中实际存在的一个文件或者目录。 无论该路径下是否存在文件或者目录，都不影响File对象的创建。 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:2:0","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"1.3 常用方法 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:3:0","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"获取功能的方法 public String getAbsolutePath() ：返回此File的绝对路径名字符串。 public String getPath() ：将此File转换为路径名字符串。 public String getName() ：返回由此File表示的文件或目录的名称。 public long length() ：返回由此File表示的文件的长度。 方法演示，代码如下： public class FileGet { public static void main(String[] args) { File f = new File(\"d:/aaa/bbb.java\"); System.out.println(\"文件绝对路径:\"+f.getAbsolutePath()); System.out.println(\"文件构造路径:\"+f.getPath()); System.out.println(\"文件名称:\"+f.getName()); System.out.println(\"文件长度:\"+f.length()+\"字节\"); File f2 = new File(\"d:/aaa\"); System.out.println(\"目录绝对路径:\"+f2.getAbsolutePath()); System.out.println(\"目录构造路径:\"+f2.getPath()); System.out.println(\"目录名称:\"+f2.getName()); System.out.println(\"目录长度:\"+f2.length()); } } 输出结果： 文件绝对路径:d:\\aaa\\bbb.java 文件构造路径:d:\\aaa\\bbb.java 文件名称:bbb.java 文件长度:636字节 目录绝对路径:d:\\aaa 目录构造路径:d:\\aaa 目录名称:aaa 目录长度:4096 API中说明：length()，表示文件的长度。但是File对象表示目录，则返回值未指定。 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:3:1","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"绝对路径和相对路径 绝对路径：从盘符开始的路径，这是一个完整的路径。 相对路径：相对于项目目录的路径，这是一个便捷的路径，开发中经常使用。 public class FilePath { public static void main(String[] args) { // D盘下的bbb.java文件 File f = new File(\"D:\\\\bbb.java\"); System.out.println(f.getAbsolutePath()); // 项目下的bbb.java文件 File f2 = new File(\"bbb.java\"); System.out.println(f2.getAbsolutePath()); } } 输出结果： D:\\bbb.java D:\\idea_project_test4\\bbb.java ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:3:2","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"判断功能的方法 public boolean exists() ：此File表示的文件或目录是否实际存在。 public boolean isDirectory() ：此File表示的是否为目录。 public boolean isFile() ：此File表示的是否为文件。 方法演示，代码如下： public class FileIs { public static void main(String[] args) { File f = new File(\"d:\\\\aaa\\\\bbb.java\"); File f2 = new File(\"d:\\\\aaa\"); // 判断是否存在 System.out.println(\"d:\\\\aaa\\\\bbb.java 是否存在:\"+f.exists()); System.out.println(\"d:\\\\aaa 是否存在:\"+f2.exists()); // 判断是文件还是目录 System.out.println(\"d:\\\\aaa 文件?:\"+f2.isFile()); System.out.println(\"d:\\\\aaa 目录?:\"+f2.isDirectory()); } } 输出结果： d:\\aaa\\bbb.java 是否存在:true d:\\aaa 是否存在:true d:\\aaa 文件?:false d:\\aaa 目录?:true ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:3:3","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"创建删除功能的方法 public boolean createNewFile() ：当且仅当具有该名称的文件尚不存在时，创建一个新的空文件。 public boolean delete() ：删除由此File表示的文件或目录。 public boolean mkdir() ：创建由此File表示的目录。 public boolean mkdirs() ：创建由此File表示的目录，包括任何必需但不存在的父目录。 方法演示，代码如下： public class FileCreateDelete { public static void main(String[] args) throws IOException { // 文件的创建 File f = new File(\"aaa.txt\"); System.out.println(\"是否存在:\"+f.exists()); // false System.out.println(\"是否创建:\"+f.createNewFile()); // true System.out.println(\"是否存在:\"+f.exists()); // true // 目录的创建 File f2= new File(\"newDir\"); System.out.println(\"是否存在:\"+f2.exists());// false System.out.println(\"是否创建:\"+f2.mkdir()); // true System.out.println(\"是否存在:\"+f2.exists());// true // 创建多级目录 File f3= new File(\"newDira\\\\newDirb\"); System.out.println(f3.mkdir());// false File f4= new File(\"newDira\\\\newDirb\"); System.out.println(f4.mkdirs());// true // 文件的删除 System.out.println(f.delete());// true // 目录的删除 System.out.println(f2.delete());// true System.out.println(f4.delete());// false } } API中说明：delete方法，如果此File表示目录，则目录必须为空才能删除。 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:3:4","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"1.4 目录的遍历 public String[] list() ：返回一个String数组，表示该File目录中的所有子文件或目录。 public File[] listFiles() ：返回一个File数组，表示该File目录中的所有的子文件或目录。 public class FileFor { public static void main(String[] args) { File dir = new File(\"d:\\\\java_code\"); //获取当前目录下的文件以及文件夹的名称。 String[] names = dir.list(); for(String name : names){ System.out.println(name); } //获取当前目录下的文件以及文件夹对象，只要拿到了文件对象，那么就可以获取更多信息 File[] files = dir.listFiles(); for (File file : files) { System.out.println(file); } } } 小贴士： 调用listFiles方法的File对象，表示的必须是实际存在的目录，否则返回null，无法进行遍历。 第二章 递归 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:4:0","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"2.1 概述 递归：指在当前方法内调用自己的这种现象。 递归的分类: 递归分为两种，直接递归和间接递归。 直接递归称为方法自身调用自己。 间接递归可以A方法调用B方法，B方法调用C方法，C方法调用A方法。 注意事项： 递归一定要有条件限定，保证递归能够停止下来，否则会发生栈内存溢出。 在递归中虽然有限定条件，但是递归次数不能太多。否则也会发生栈内存溢出。 构造方法,禁止递归 public class Demo01DiGui { public static void main(String[] args) { // a(); b(1); } /* * 3.构造方法,禁止递归 * 编译报错:构造方法是创建对象使用的,不能让对象一直创建下去 */ public Demo01DiGui() { //Demo01DiGui(); } /* * 2.在递归中虽然有限定条件，但是递归次数不能太多。否则也会发生栈内存溢出。 * 4993 * Exception in thread \"main\" java.lang.StackOverflowError */ private static void b(int i) { System.out.println(i); //添加一个递归结束的条件,i==5000的时候结束 if(i==5000){ return;//结束方法 } b(++i); } /* * 1.递归一定要有条件限定，保证递归能够停止下来，否则会发生栈内存溢出。 Exception in thread \"main\" * java.lang.StackOverflowError */ private static void a() { System.out.println(\"a方法\"); a(); } } ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:5:0","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"2.2 递归累加求和 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:6:0","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"计算1 ~ n的和 分析：num的累和 = num + (num-1)的累和，所以可以把累和的操作定义成一个方法，递归调用。 实现代码： public class DiGuiDemo { public static void main(String[] args) { //计算1~num的和，使用递归完成 int num = 5; // 调用求和的方法 int sum = getSum(num); // 输出结果 System.out.println(sum); } /* 通过递归算法实现. 参数列表:int 返回值类型: int */ public static int getSum(int num) { /* num为1时,方法返回1, 相当于是方法的出口,num总有是1的情况 */ if(num == 1){ return 1; } /* num不为1时,方法返回 num +(num-1)的累和 递归调用getSum方法 */ return num + getSum(num-1); } } ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:6:1","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"代码执行图解 小贴士：递归一定要有条件限定，保证递归能够停止下来，次数不要太多，否则会发生栈内存溢出。 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:6:2","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"2.3 递归求阶乘 阶乘：所有小于及等于该数的正整数的积。 n的阶乘：n! = n * (n-1) *...* 3 * 2 * 1 分析：这与累和类似,只不过换成了乘法运算，学员可以自己练习，需要注意阶乘值符合int类型的范围。 推理得出：n! = n * (n-1)! 代码实现： public class DiGuiDemo { //计算n的阶乘，使用递归完成 public static void main(String[] args) { int n = 3; // 调用求阶乘的方法 int value = getValue(n); // 输出结果 System.out.println(\"阶乘为:\"+ value); } /* 通过递归算法实现. 参数列表:int 返回值类型: int */ public static int getValue(int n) { // 1的阶乘为1 if (n == 1) { return 1; } /* n不为1时,方法返回 n! = n*(n-1)! 递归调用getValue方法 */ return n * getValue(n - 1); } } ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:7:0","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"2.4 递归打印多级目录 分析：多级目录的打印，就是当目录的嵌套。遍历之前，无从知道到底有多少级目录，所以我们还是要使用递归实现。 代码实现： public class DiGuiDemo2 { public static void main(String[] args) { // 创建File对象 File dir = new File(\"D:\\\\aaa\"); // 调用打印目录方法 printDir(dir); } public static void printDir(File dir) { // 获取子文件和目录 File[] files = dir.listFiles(); // 循环打印 /* 判断: 当是文件时,打印绝对路径. 当是目录时,继续调用打印目录的方法,形成递归调用. */ for (File file : files) { // 判断 if (file.isFile()) { // 是文件,输出文件绝对路径 System.out.println(\"文件名:\"+ file.getAbsolutePath()); } else { // 是目录,输出目录绝对路径 System.out.println(\"目录:\"+file.getAbsolutePath()); // 继续遍历,调用printDir,形成递归 printDir(file); } } } } 第三章 综合案例 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:8:0","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"3.1 文件搜索 搜索D:\\aaa 目录中的.java 文件。 分析： 目录搜索，无法判断多少级目录，所以使用递归，遍历所有目录。 遍历目录时，获取的子文件，通过文件名称，判断是否符合条件。 代码实现： public class DiGuiDemo3 { public static void main(String[] args) { // 创建File对象 File dir = new File(\"D:\\\\aaa\"); // 调用打印目录方法 printDir(dir); } public static void printDir(File dir) { // 获取子文件和目录 File[] files = dir.listFiles(); // 循环打印 for (File file : files) { if (file.isFile()) { // 是文件，判断文件名并输出文件绝对路径 if (file.getName().endsWith(\".java\")) { System.out.println(\"文件名:\" + file.getAbsolutePath()); } } else { // 是目录，继续遍历,形成递归 printDir(file); } } } } ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:9:0","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"3.2 文件过滤器优化 java.io.FileFilter是一个接口，是File的过滤器。 该接口的对象可以传递给File类的listFiles(FileFilter) 作为参数， 接口中只有一个方法。 boolean accept(File pathname) ：测试pathname是否应该包含在当前File目录中，符合则返回true。 分析： 接口作为参数，需要传递子类对象，重写其中方法。我们选择匿名内部类方式，比较简单。 accept方法，参数为File，表示当前File下所有的子文件和子目录。保留住则返回true，过滤掉则返回false。保留规则： 要么是.java文件。 要么是目录，用于继续遍历。 通过过滤器的作用，listFiles(FileFilter)返回的数组元素中，子文件对象都是符合条件的，可以直接打印。 代码实现： public class DiGuiDemo4 { public static void main(String[] args) { File dir = new File(\"D:\\\\aaa\"); printDir2(dir); } public static void printDir2(File dir) { // 匿名内部类方式,创建过滤器子类对象 File[] files = dir.listFiles(new FileFilter() { @Override public boolean accept(File pathname) { return pathname.getName().endsWith(\".java\")||pathname.isDirectory(); } }); // 循环打印 for (File file : files) { if (file.isFile()) { System.out.println(\"文件名:\" + file.getAbsolutePath()); } else { printDir2(file); } } } } ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:10:0","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"3.3 Lambda优化 分析：FileFilter是只有一个方法的接口，因此可以用lambda表达式简写。 lambda格式： ()-\u003e{ } 代码实现： public static void printDir3(File dir) { // lambda的改写 File[] files = dir.listFiles(f -\u003e{ return f.getName().endsWith(\".java\") || f.isDirectory(); }); // 循环打印 for (File file : files) { if (file.isFile()) { System.out.println(\"文件名:\" + file.getAbsolutePath()); } else { printDir3(file); } } } 第一章 IO概述 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:11:0","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"1.1 什么是IO 生活中，你肯定经历过这样的场景。当你编辑一个文本文件，忘记了ctrl+s ，可能文件就白白编辑了。当你电脑上插入一个U盘，可以把一个视频，拷贝到你的电脑硬盘里。那么数据都是在哪些设备上的呢？键盘、内存、硬盘、外接设备等等。 我们把这种数据的传输，可以看做是一种数据的流动，按照流动的方向，以内存为基准，分为输入input 和输出output ，即流向内存是输入流，流出内存的输出流。 Java中I/O操作主要是指使用java.io包下的内容，进行输入、输出操作。输入也叫做读取数据，输出也叫做作写出数据。 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:12:0","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"1.2 IO的分类 根据数据的流向分为：输入流和输出流。 输入流 ：把数据从其他设备上读取到内存中的流。 输出流 ：把数据从内存 中写出到其他设备上的流。 格局数据的类型分为：字节流和字符流。 字节流 ：以字节为单位，读写数据的流。 字符流 ：以字符为单位，读写数据的流。 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:13:0","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"1.4 顶级父类们 输入流 输出流 字节流 字节输入流 InputStream 字节输出流 OutputStream 字符流 字符输入流 Reader 字符输出流 Writer 第二章 字节流 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:14:0","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"2.1 一切皆为字节 一切文件数据(文本、图片、视频等)在存储时，都是以二进制数字的形式保存，都一个一个的字节，那么传输时一样如此。所以，字节流可以传输任意文件数据。在操作流的时候，我们要时刻明确，无论使用什么样的流对象，底层传输的始终为二进制数据。 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:15:0","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"2.2 字节输出流【OutputStream】 java.io.OutputStream 抽象类是表示字节输出流的所有类的超类，将指定的字节信息写出到目的地。它定义了字节输出流的基本共性功能方法。 public void close() ：关闭此输出流并释放与此流相关联的任何系统资源。 public void flush() ：刷新此输出流并强制任何缓冲的输出字节被写出。 public void write(byte[] b)：将 b.length字节从指定的字节数组写入此输出流。 public void write(byte[] b, int off, int len) ：从指定的字节数组写入 len字节，从偏移量 off开始输出到此输出流。 public abstract void write(int b) ：将指定的字节输出流。 小贴士： close方法，当完成流的操作时，必须调用此方法，释放系统资源。 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:16:0","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"2.3 FileOutputStream类 OutputStream有很多子类，我们从最简单的一个子类开始。 java.io.FileOutputStream 类是文件输出流，用于将数据写出到文件。 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:17:0","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"构造方法 public FileOutputStream(File file)：创建文件输出流以写入由指定的 File对象表示的文件。 public FileOutputStream(String name)： 创建文件输出流以指定的名称写入文件。 当你创建一个流对象时，必须传入一个文件路径。该路径下，如果没有这个文件，会创建该文件。如果有这个文件，会清空这个文件的数据。 构造举例，代码如下： public class FileOutputStreamConstructor throws IOException { public static void main(String[] args) { // 使用File对象创建流对象 File file = new File(\"a.txt\"); FileOutputStream fos = new FileOutputStream(file); // 使用文件名称创建流对象 FileOutputStream fos = new FileOutputStream(\"b.txt\"); } } ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:17:1","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"写出字节数据 写出字节：write(int b) 方法，每次可以写出一个字节数据，代码使用演示： public class FOSWrite { public static void main(String[] args) throws IOException { // 使用文件名称创建流对象 FileOutputStream fos = new FileOutputStream(\"fos.txt\"); // 写出数据 fos.write(97); // 写出第1个字节 fos.write(98); // 写出第2个字节 fos.write(99); // 写出第3个字节 // 关闭资源 fos.close(); } } 输出结果： abc 小贴士： 虽然参数为int类型四个字节，但是只会保留一个字节的信息写出。 流操作完毕后，必须释放系统资源，调用close方法，千万记得。 写出字节数组：write(byte[] b)，每次可以写出数组中的数据，代码使用演示： public class FOSWrite { public static void main(String[] args) throws IOException { // 使用文件名称创建流对象 FileOutputStream fos = new FileOutputStream(\"fos.txt\"); // 字符串转换为字节数组 byte[] b = \"黑马程序员\".getBytes(); // 写出字节数组数据 fos.write(b); // 关闭资源 fos.close(); } } 输出结果： 黑马程序员 写出指定长度字节数组：write(byte[] b, int off, int len) ,每次写出从off索引开始，len个字节，代码使用演示： public class FOSWrite { public static void main(String[] args) throws IOException { // 使用文件名称创建流对象 FileOutputStream fos = new FileOutputStream(\"fos.txt\"); // 字符串转换为字节数组 byte[] b = \"abcde\".getBytes(); // 写出从索引2开始，2个字节。索引2是c，两个字节，也就是cd。 fos.write(b,2,2); // 关闭资源 fos.close(); } } 输出结果： cd ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:17:2","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"数据追加续写 经过以上的演示，每次程序运行，创建输出流对象，都会清空目标文件中的数据。如何保留目标文件中数据，还能继续添加新数据呢？ public FileOutputStream(File file, boolean append)： 创建文件输出流以写入由指定的 File对象表示的文件。 public FileOutputStream(String name, boolean append)： 创建文件输出流以指定的名称写入文件。 这两个构造方法，参数中都需要传入一个boolean类型的值，true 表示追加数据，false 表示清空原有数据。这样创建的输出流对象，就可以指定是否追加续写了，代码使用演示： public class FOSWrite { public static void main(String[] args) throws IOException { // 使用文件名称创建流对象 FileOutputStream fos = new FileOutputStream(\"fos.txt\"，true); // 字符串转换为字节数组 byte[] b = \"abcde\".getBytes(); // 写出从索引2开始，2个字节。索引2是c，两个字节，也就是cd。 fos.write(b); // 关闭资源 fos.close(); } } 文件操作前：cd 文件操作后：cdabcde ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:17:3","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"写出换行 Windows系统里，换行符号是\\r\\n 。把 以指定是否追加续写了，代码使用演示： public class FOSWrite { public static void main(String[] args) throws IOException { // 使用文件名称创建流对象 FileOutputStream fos = new FileOutputStream(\"fos.txt\"); // 定义字节数组 byte[] words = {97,98,99,100,101}; // 遍历数组 for (int i = 0; i \u003c words.length; i++) { // 写出一个字节 fos.write(words[i]); // 写出一个换行, 换行符号转成数组写出 fos.write(\"\\r\\n\".getBytes()); } // 关闭资源 fos.close(); } } 输出结果： a b c d e 回车符\\r和换行符\\n ： 回车符：回到一行的开头（return）。 换行符：下一行（newline）。 系统中的换行： Windows系统里，每行结尾是 回车+换行 ，即\\r\\n； Unix系统里，每行结尾只有 换行 ，即\\n； Mac系统里，每行结尾是 回车 ，即\\r。从 Mac OS X开始与Linux统一。 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:17:4","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"2.4 字节输入流【InputStream】 java.io.InputStream 抽象类是表示字节输入流的所有类的超类，可以读取字节信息到内存中。它定义了字节输入流的基本共性功能方法。 public void close() ：关闭此输入流并释放与此流相关联的任何系统资源。 public abstract int read()： 从输入流读取数据的下一个字节。 public int read(byte[] b)： 从输入流中读取一些字节数，并将它们存储到字节数组 b中 。 小贴士： close方法，当完成流的操作时，必须调用此方法，释放系统资源。 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:18:0","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"2.5 FileInputStream类 java.io.FileInputStream 类是文件输入流，从文件中读取字节。 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:19:0","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"构造方法 FileInputStream(File file)： 通过打开与实际文件的连接来创建一个 FileInputStream ，该文件由文件系统中的 File对象 file命名。 FileInputStream(String name)： 通过打开与实际文件的连接来创建一个 FileInputStream ，该文件由文件系统中的路径名 name命名。 当你创建一个流对象时，必须传入一个文件路径。该路径下，如果没有该文件,会抛出FileNotFoundException 。 构造举例，代码如下： public class FileInputStreamConstructor throws IOException{ public static void main(String[] args) { // 使用File对象创建流对象 File file = new File(\"a.txt\"); FileInputStream fos = new FileInputStream(file); // 使用文件名称创建流对象 FileInputStream fos = new FileInputStream(\"b.txt\"); } } ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:19:1","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"读取字节数据 读取字节：read方法，每次可以读取一个字节的数据，提升为int类型，读取到文件末尾，返回-1，代码使用演示： public class FISRead { public static void main(String[] args) throws IOException{ // 使用文件名称创建流对象 FileInputStream fis = new FileInputStream(\"read.txt\"); // 读取数据，返回一个字节 int read = fis.read(); System.out.println((char) read); read = fis.read(); System.out.println((char) read); read = fis.read(); System.out.println((char) read); read = fis.read(); System.out.println((char) read); read = fis.read(); System.out.println((char) read); // 读取到末尾,返回-1 read = fis.read(); System.out.println( read); // 关闭资源 fis.close(); } } 输出结果： a b c d e -1 循环改进读取方式，代码使用演示： public class FISRead { public static void main(String[] args) throws IOException{ // 使用文件名称创建流对象 FileInputStream fis = new FileInputStream(\"read.txt\"); // 定义变量，保存数据 int b ； // 循环读取 while ((b = fis.read())!=-1) { System.out.println((char)b); } // 关闭资源 fis.close(); } } 输出结果： a b c d e 小贴士： 虽然读取了一个字节，但是会自动提升为int类型。 流操作完毕后，必须释放系统资源，调用close方法，千万记得。 使用字节数组读取：read(byte[] b)，每次读取b的长度个字节到数组中，返回读取到的有效字节个数，读取到末尾时，返回-1 ，代码使用演示： public class FISRead { public static void main(String[] args) throws IOException{ // 使用文件名称创建流对象. FileInputStream fis = new FileInputStream(\"read.txt\"); // 文件中为abcde // 定义变量，作为有效个数 int len; // 定义字节数组，作为装字节数据的容器 byte[] b = new byte[2]; // 循环读取 while (( len= fis.read(b))!=-1) { // 每次读取后,把数组变成字符串打印 System.out.println(new String(b)); } // 关闭资源 fis.close(); } } 输出结果： ab cd ed read.txt中的内容为abcde，错误输出数据d，是由于最后一次读取时，只读取一个字节e，数组中，上次读取的数据没有被完全替换，所以要通过len ，获取有效的字节，代码使用演示： public class FISRead { public static void main(String[] args) throws IOException{ // 使用文件名称创建流对象. FileInputStream fis = new FileInputStream(\"read.txt\"); // 文件中为abcde // 定义变量，作为有效个数 int len ； // 定义字节数组，作为装字节数据的容器 byte[] b = new byte[2]; // 循环读取 while (( len= fis.read(b))!=-1) { // 每次读取后,把数组的有效字节部分，变成字符串打印 System.out.println(new String(b，0，len));// len 每次读取的有效字节个数 } // 关闭资源 fis.close(); } } 输出结果： ab cd e 小贴士： 使用数组读取，每次读取多个字节，减少了系统间的IO操作次数，从而提高了读写的效率，建议开发中使用。 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:19:2","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"2.6 字节流练习：图片复制 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:20:0","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"复制原理图解 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:20:1","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"案例实现 复制图片文件，代码使用演示： public class Copy { public static void main(String[] args) throws IOException { // 1.创建流对象 // 1.1 指定数据源 FileInputStream fis = new FileInputStream(\"D:\\\\test.jpg\"); // 1.2 指定目的地 FileOutputStream fos = new FileOutputStream(\"test_copy.jpg\"); // 2.读写数据 // 2.1 定义数组 byte[] b = new byte[1024]; // 2.2 定义长度 int len; // 2.3 循环读取 while ((len = fis.read(b))!=-1) { // 2.4 写出数据 fos.write(b, 0 , len); } // 3.关闭资源 fos.close(); fis.close(); } } 小贴士： 流的关闭原则：先开后关，后开先关。 第三章 字符流 当使用字节流读取文本文件时，可能会有一个小问题。就是遇到中文字符时，可能不会显示完整的字符，那是因为一个中文字符可能占用多个字节存储。所以Java提供一些字符流类，以字符为单位读写数据，专门用于处理文本文件。 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:20:2","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"3.1 字符输入流【Reader】 java.io.Reader抽象类是表示用于读取字符流的所有类的超类，可以读取字符信息到内存中。它定义了字符输入流的基本共性功能方法。 public void close() ：关闭此流并释放与此流相关联的任何系统资源。 public int read()： 从输入流读取一个字符。 public int read(char[] cbuf)： 从输入流中读取一些字符，并将它们存储到字符数组 cbuf中 。 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:21:0","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"3.2 FileReader类 java.io.FileReader 类是读取字符文件的便利类。构造时使用系统默认的字符编码和默认字节缓冲区。 小贴士： 字符编码：字节与字符的对应规则。Windows系统的中文编码默认是GBK编码表。 idea中UTF-8 字节缓冲区：一个字节数组，用来临时存储字节数据。 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:22:0","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"构造方法 FileReader(File file)： 创建一个新的 FileReader ，给定要读取的File对象。 FileReader(String fileName)： 创建一个新的 FileReader ，给定要读取的文件的名称。 当你创建一个流对象时，必须传入一个文件路径。类似于FileInputStream 。 构造举例，代码如下： public class FileReaderConstructor throws IOException{ public static void main(String[] args) { // 使用File对象创建流对象 File file = new File(\"a.txt\"); FileReader fr = new FileReader(file); // 使用文件名称创建流对象 FileReader fr = new FileReader(\"b.txt\"); } } ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:22:1","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"读取字符数据 读取字符：read方法，每次可以读取一个字符的数据，提升为int类型，读取到文件末尾，返回-1，循环读取，代码使用演示： public class FRRead { public static void main(String[] args) throws IOException { // 使用文件名称创建流对象 FileReader fr = new FileReader(\"read.txt\"); // 定义变量，保存数据 int b; // 循环读取 while ((b = fr.read())!=-1) { System.out.println((char)b); } // 关闭资源 fr.close(); } } 输出结果： 黑 马 程 序 员 小贴士：虽然读取了一个字符，但是会自动提升为int类型。 为什么会自动提升为Int类型呢？ https://blog.csdn.net/ju_362204801/article/details/104715442 https://blog.csdn.net/qq_42305808/article/details/83215551 使用字符数组读取：read(char[] cbuf)，每次读取b的长度个字符到数组中，返回读取到的有效字符个数，读取到末尾时，返回-1 ，代码使用演示： public class FRRead { public static void main(String[] args) throws IOException { // 使用文件名称创建流对象 FileReader fr = new FileReader(\"read.txt\"); // 定义变量，保存有效字符个数 int len ； // 定义字符数组，作为装字符数据的容器 char[] cbuf = new char[2]; // 循环读取 while ((len = fr.read(cbuf))!=-1) { System.out.println(new String(cbuf)); } // 关闭资源 fr.close(); } } 输出结果： 黑马 程序 员序 获取有效的字符改进，代码使用演示： public class FISRead { public static void main(String[] args) throws IOException { // 使用文件名称创建流对象 FileReader fr = new FileReader(\"read.txt\"); // 定义变量，保存有效字符个数 int len ； // 定义字符数组，作为装字符数据的容器 char[] cbuf = new char[2]; // 循环读取 while ((len = fr.read(cbuf))!=-1) { System.out.println(new String(cbuf,0,len)); } // 关闭资源 fr.close(); } } 输出结果： 黑马 程序 员 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:22:2","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"3.3 字符输出流【Writer】 java.io.Writer 抽象类是表示用于写出字符流的所有类的超类，将指定的字符信息写出到目的地。它定义了字节输出流的基本共性功能方法。 void write(int c) 写入单个字符。 void write(char[] cbuf) 写入字符数组。 abstract void write(char[] cbuf, int off, int len) 写入字符数组的某一部分,off数组的开始索引,len写的字符个数。 void write(String str) 写入字符串。 void write(String str, int off, int len) 写入字符串的某一部分,off字符串的开始索引,len写的字符个数。 void flush() 刷新该流的缓冲。 void close() 关闭此流，但要先刷新它。 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:23:0","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"3.4 FileWriter类 java.io.FileWriter 类是写出字符到文件的便利类。构造时使用系统默认的字符编码和默认字节缓冲区。 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:24:0","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"构造方法 FileWriter(File file)： 创建一个新的 FileWriter，给定要读取的File对象。 FileWriter(String fileName)： 创建一个新的 FileWriter，给定要读取的文件的名称。 当你创建一个流对象时，必须传入一个文件路径，类似于FileOutputStream。 构造举例，代码如下： public class FileWriterConstructor { public static void main(String[] args) throws IOException { // 使用File对象创建流对象 File file = new File(\"a.txt\"); FileWriter fw = new FileWriter(file); // 使用文件名称创建流对象 FileWriter fw = new FileWriter(\"b.txt\"); } } ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:24:1","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"基本写出数据 写出字符：write(int b) 方法，每次可以写出一个字符数据，代码使用演示： public class FWWrite { public static void main(String[] args) throws IOException { // 使用文件名称创建流对象 FileWriter fw = new FileWriter(\"fw.txt\"); // 写出数据 fw.write(97); // 写出第1个字符 fw.write('b'); // 写出第2个字符 fw.write('C'); // 写出第3个字符 fw.write(30000); // 写出第4个字符，中文编码表中30000对应一个汉字。 /* 【注意】关闭资源时,与FileOutputStream不同。 如果不关闭,数据只是保存到缓冲区，并未保存到文件。 */ // fw.close(); } } 输出结果： abC田 小贴士： 虽然参数为int类型四个字节，但是只会保留一个字符的信息写出。 未调用close方法，数据只是保存到了缓冲区，并未写出到文件中。 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:24:2","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"关闭和刷新 因为内置缓冲区的原因，如果不关闭输出流，无法写出字符到文件中。但是关闭的流对象，是无法继续写出数据的。如果我们既想写出数据，又想继续使用流，就需要flush 方法了。 flush ：刷新缓冲区，流对象可以继续使用。 close :先刷新缓冲区，然后通知系统释放资源。流对象不可以再被使用了。 代码使用演示： public class FWWrite { public static void main(String[] args) throws IOException { // 使用文件名称创建流对象 FileWriter fw = new FileWriter(\"fw.txt\"); // 写出数据，通过flush fw.write('刷'); // 写出第1个字符 fw.flush(); fw.write('新'); // 继续写出第2个字符，写出成功 fw.flush(); // 写出数据，通过close fw.write('关'); // 写出第1个字符 fw.close(); fw.write('闭'); // 继续写出第2个字符,【报错】java.io.IOException: Stream closed fw.close(); } } 小贴士：即便是flush方法写出了数据，操作的最后还是要调用close方法，释放系统资源。 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:24:3","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"写出其他数据 写出字符数组 ：write(char[] cbuf) 和 write(char[] cbuf, int off, int len) ，每次可以写出字符数组中的数据，用法类似FileOutputStream，代码使用演示： public class FWWrite { public static void main(String[] args) throws IOException { // 使用文件名称创建流对象 FileWriter fw = new FileWriter(\"fw.txt\"); // 字符串转换为字节数组 char[] chars = \"黑马程序员\".toCharArray(); // 写出字符数组 fw.write(chars); // 黑马程序员 // 写出从索引2开始，2个字节。索引2是'程'，两个字节，也就是'程序'。 fw.write(b,2,2); // 程序 // 关闭资源 fos.close(); } } 写出字符串：write(String str) 和 write(String str, int off, int len) ，每次可以写出字符串中的数据，更为方便，代码使用演示： public class FWWrite { public static void main(String[] args) throws IOException { // 使用文件名称创建流对象 FileWriter fw = new FileWriter(\"fw.txt\"); // 字符串 String msg = \"黑马程序员\"; // 写出字符数组 fw.write(msg); //黑马程序员 // 写出从索引2开始，2个字节。索引2是'程'，两个字节，也就是'程序'。 fw.write(msg,2,2); // 程序 // 关闭资源 fos.close(); } } 续写和换行：操作类似于FileOutputStream。 public class FWWrite { public static void main(String[] args) throws IOException { // 使用文件名称创建流对象，可以续写数据 FileWriter fw = new FileWriter(\"fw.txt\"，true); // 写出字符串 fw.write(\"黑马\"); // 写出换行 fw.write(\"\\r\\n\"); // 写出字符串 fw.write(\"程序员\"); // 关闭资源 fw.close(); } } 输出结果: 黑马 程序员 小贴士：字符流，只能操作文本文件，不能操作图片，视频等非文本文件。 当我们单纯读或者写文本文件时 使用字符流 其他情况使用字节流 第四章 IO异常的处理 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:24:4","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"JDK7前处理 之前的入门练习，我们一直把异常抛出，而实际开发中并不能这样处理，建议使用try...catch...finally 代码块，处理异常部分，代码使用演示： public class HandleException1 { public static void main(String[] args) { // 声明变量 FileWriter fw = null; try { //创建流对象 fw = new FileWriter(\"fw.txt\"); // 写出数据 fw.write(\"黑马程序员\"); //黑马程序员 } catch (IOException e) { e.printStackTrace(); } finally { try { if (fw != null) { fw.close(); } } catch (IOException e) { e.printStackTrace(); } } } } ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:24:5","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"JDK7的处理(扩展知识点了解内容) 还可以使用JDK7优化后的try-with-resource 语句，该语句确保了每个资源在语句结束时关闭。所谓的资源（resource）是指在程序完成后，必须关闭的对象。 格式： try (创建流对象语句，如果多个,使用';'隔开) { // 读写数据 } catch (IOException e) { e.printStackTrace(); } 代码使用演示： public class HandleException2 { public static void main(String[] args) { // 创建流对象 try ( FileWriter fw = new FileWriter(\"fw.txt\"); ) { // 写出数据 fw.write(\"黑马程序员\"); //黑马程序员 } catch (IOException e) { e.printStackTrace(); } } } ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:24:6","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"JDK9的改进(扩展知识点了解内容) JDK9中try-with-resource 的改进，对于引入对象的方式，支持的更加简洁。被引入的对象，同样可以自动关闭，无需手动close，我们来了解一下格式。 改进前格式： // 被final修饰的对象 final Resource resource1 = new Resource(\"resource1\"); // 普通对象 Resource resource2 = new Resource(\"resource2\"); // 引入方式：创建新的变量保存 try (Resource r1 = resource1; Resource r2 = resource2) { // 使用对象 } 改进后格式： // 被final修饰的对象 final Resource resource1 = new Resource(\"resource1\"); // 普通对象 Resource resource2 = new Resource(\"resource2\"); // 引入方式：直接引入 try (resource1; resource2) { // 使用对象 } 改进后，代码使用演示： public class TryDemo { public static void main(String[] args) throws IOException { // 创建流对象 final FileReader fr = new FileReader(\"in.txt\"); FileWriter fw = new FileWriter(\"out.txt\"); // 引入到try中 try (fr; fw) { // 定义变量 int b; // 读取数据 while ((b = fr.read())!=-1) { // 写出数据 fw.write(b); } } catch (IOException e) { e.printStackTrace(); } } } 第五章 属性集 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:24:7","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"5.1 概述 java.util.Properties 继承于 Hashtable，来表示一个持久的属性集。它使用键值结构存储数据，每个键及其对应值都是一个字符串。该类也被许多Java类使用，比如获取系统属性时，System.getProperties 方法就是返回一个Properties对象。 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:25:0","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"5.2 Properties类 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:26:0","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"构造方法 public Properties() :创建一个空的属性列表。 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:26:1","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"基本的存储方法 public Object setProperty(String key, String value) ： 保存一对属性。 public String getProperty(String key) ：使用此属性列表中指定的键搜索属性值。 public Set\u003cString\u003e stringPropertyNames() ：所有键的名称的集合。 public class ProDemo { public static void main(String[] args) throws FileNotFoundException { // 创建属性集对象 Properties properties = new Properties(); // 添加键值对元素 properties.setProperty(\"filename\", \"a.txt\"); properties.setProperty(\"length\", \"209385038\"); properties.setProperty(\"location\", \"D:\\\\a.txt\"); // 打印属性集对象 System.out.println(properties); // 通过键,获取属性值 System.out.println(properties.getProperty(\"filename\")); System.out.println(properties.getProperty(\"length\")); System.out.println(properties.getProperty(\"location\")); // 遍历属性集,获取所有键的集合 Set\u003cString\u003e strings = properties.stringPropertyNames(); // 打印键值对 for (String key : strings ) { System.out.println(key+\" -- \"+properties.getProperty(key)); } } } 输出结果： {filename=a.txt, length=209385038, location=D:\\a.txt} a.txt 209385038 D:\\a.txt filename -- a.txt length -- 209385038 location -- D:\\a.txt ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:26:2","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"与流相关的方法 public void load(InputStream inStream)： 从字节输入流中读取键值对。 参数中使用了字节输入流，通过流对象，可以关联到某文件上，这样就能够加载文本中的数据了。文本数据格式: filename=a.txt length=209385038 location=D:\\a.txt 加载代码演示： public class ProDemo2 { public static void main(String[] args) throws FileNotFoundException { // 创建属性集对象 Properties pro = new Properties(); // 加载文本中信息到属性集 pro.load(new FileInputStream(\"read.txt\")); // 遍历集合并打印 Set\u003cString\u003e strings = pro.stringPropertyNames(); for (String key : strings ) { System.out.println(key+\" -- \"+pro.getProperty(key)); } } } 输出结果： filename -- a.txt length -- 209385038 location -- D:\\a.txt 小贴士：文本中的数据，必须是键值对形式，可以使用空格、等号、冒号等符号分隔。 第一章 缓冲流 昨天学习了基本的一些流，作为IO流的入门，今天我们要见识一些更强大的流。比如能够高效读写的缓冲流，能够转换编码的转换流，能够持久化存储对象的序列化流等等。这些功能更为强大的流，都是在基本的流对象基础之上创建而来的，就像穿上铠甲的武士一样，相当于是对基本流对象的一种增强。 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:26:3","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"1.1 概述 缓冲流,也叫高效流，是对4个基本的FileXxx 流的增强，所以也是4个流，按照数据类型分类： 字节缓冲流：BufferedInputStream，BufferedOutputStream 字符缓冲流：BufferedReader，BufferedWriter 缓冲流的基本原理，是在创建流对象时，会创建一个内置的默认大小的缓冲区数组，通过缓冲区读写，减少系统IO次数，从而提高读写的效率。 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:27:0","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"1.2 字节缓冲流 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:28:0","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"构造方法 public BufferedInputStream(InputStream in) ：创建一个 新的缓冲输入流。 public BufferedOutputStream(OutputStream out)： 创建一个新的缓冲输出流。 构造举例，代码如下： // 创建字节缓冲输入流 BufferedInputStream bis = new BufferedInputStream(new FileInputStream(\"bis.txt\")); // 创建字节缓冲输出流 BufferedOutputStream bos = new BufferedOutputStream(new FileOutputStream(\"bos.txt\")); ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:28:1","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"效率测试 查询API，缓冲流读写方法与基本的流是一致的，我们通过复制大文件（375MB），测试它的效率。 基本流，代码如下： public class BufferedDemo { public static void main(String[] args) throws FileNotFoundException { // 记录开始时间 long start = System.currentTimeMillis(); // 创建流对象 try ( FileInputStream fis = new FileInputStream(\"jdk9.exe\"); FileOutputStream fos = new FileOutputStream(\"copy.exe\") ){ // 读写数据 int b; while ((b = fis.read()) != -1) { fos.write(b); } } catch (IOException e) { e.printStackTrace(); } // 记录结束时间 long end = System.currentTimeMillis(); System.out.println(\"普通流复制时间:\"+(end - start)+\" 毫秒\"); } } 十几分钟过去了... 缓冲流，代码如下： public class BufferedDemo { public static void main(String[] args) throws FileNotFoundException { // 记录开始时间 long start = System.currentTimeMillis(); // 创建流对象 try ( BufferedInputStream bis = new BufferedInputStream(new FileInputStream(\"jdk9.exe\")); BufferedOutputStream bos = new BufferedOutputStream(new FileOutputStream(\"copy.exe\")); ){ // 读写数据 int b; while ((b = bis.read()) != -1) { bos.write(b); } } catch (IOException e) { e.printStackTrace(); } // 记录结束时间 long end = System.currentTimeMillis(); System.out.println(\"缓冲流复制时间:\"+(end - start)+\" 毫秒\"); } } 缓冲流复制时间:8016 毫秒 如何更快呢？ 使用数组的方式，代码如下： public class BufferedDemo { public static void main(String[] args) throws FileNotFoundException { // 记录开始时间 long start = System.currentTimeMillis(); // 创建流对象 try ( BufferedInputStream bis = new BufferedInputStream(new FileInputStream(\"jdk9.exe\")); BufferedOutputStream bos = new BufferedOutputStream(new FileOutputStream(\"copy.exe\")); ){ // 读写数据 int len; byte[] bytes = new byte[8*1024]; while ((len = bis.read(bytes)) != -1) { bos.write(bytes, 0 , len); } } catch (IOException e) { e.printStackTrace(); } // 记录结束时间 long end = System.currentTimeMillis(); System.out.println(\"缓冲流使用数组复制时间:\"+(end - start)+\" 毫秒\"); } } 缓冲流使用数组复制时间:666 毫秒 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:28:2","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"1.3 字符缓冲流 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:29:0","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"构造方法 public BufferedReader(Reader in) ：创建一个 新的缓冲输入流。 public BufferedWriter(Writer out)： 创建一个新的缓冲输出流。 构造举例，代码如下： // 创建字符缓冲输入流 BufferedReader br = new BufferedReader(new FileReader(\"br.txt\")); // 创建字符缓冲输出流 BufferedWriter bw = new BufferedWriter(new FileWriter(\"bw.txt\")); ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:29:1","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"特有方法 字符缓冲流的基本方法与普通字符流调用方式一致，不再阐述，我们来看它们具备的特有方法。 BufferedReader：public String readLine(): 读一行文字。 BufferedWriter：public void newLine(): 写一行行分隔符,由系统属性定义符号。 readLine方法演示，代码如下： public class BufferedReaderDemo { public static void main(String[] args) throws IOException { // 创建流对象 BufferedReader br = new BufferedReader(new FileReader(\"in.txt\")); // 定义字符串,保存读取的一行文字 String line = null; // 循环读取,读取到最后返回null while ((line = br.readLine())!=null) { System.out.print(line); System.out.println(\"------\"); } // 释放资源 br.close(); } } newLine方法演示，代码如下： public class BufferedWriterDemo throws IOException { public static void main(String[] args) throws IOException { // 创建流对象 BufferedWriter bw = new BufferedWriter(new FileWriter(\"out.txt\")); // 写出数据 bw.write(\"黑马\"); // 写出换行 bw.newLine(); bw.write(\"程序\"); bw.newLine(); bw.write(\"员\"); bw.newLine(); // 释放资源 bw.close(); } } 输出效果: 黑马 程序 员 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:29:2","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"1.4 练习:文本排序 请将文本信息恢复顺序。 3.侍中、侍郎郭攸之、费祎、董允等，此皆良实，志虑忠纯，是以先帝简拔以遗陛下。愚以为宫中之事，事无大小，悉以咨之，然后施行，必得裨补阙漏，有所广益。 8.愿陛下托臣以讨贼兴复之效，不效，则治臣之罪，以告先帝之灵。若无兴德之言，则责攸之、祎、允等之慢，以彰其咎；陛下亦宜自谋，以咨诹善道，察纳雅言，深追先帝遗诏，臣不胜受恩感激。 4.将军向宠，性行淑均，晓畅军事，试用之于昔日，先帝称之曰能，是以众议举宠为督。愚以为营中之事，悉以咨之，必能使行阵和睦，优劣得所。 2.宫中府中，俱为一体，陟罚臧否，不宜异同。若有作奸犯科及为忠善者，宜付有司论其刑赏，以昭陛下平明之理，不宜偏私，使内外异法也。 1.先帝创业未半而中道崩殂，今天下三分，益州疲弊，此诚危急存亡之秋也。然侍卫之臣不懈于内，忠志之士忘身于外者，盖追先帝之殊遇，欲报之于陛下也。诚宜开张圣听，以光先帝遗德，恢弘志士之气，不宜妄自菲薄，引喻失义，以塞忠谏之路也。 9.今当远离，临表涕零，不知所言。 6.臣本布衣，躬耕于南阳，苟全性命于乱世，不求闻达于诸侯。先帝不以臣卑鄙，猥自枉屈，三顾臣于草庐之中，咨臣以当世之事，由是感激，遂许先帝以驱驰。后值倾覆，受任于败军之际，奉命于危难之间，尔来二十有一年矣。 7.先帝知臣谨慎，故临崩寄臣以大事也。受命以来，夙夜忧叹，恐付托不效，以伤先帝之明，故五月渡泸，深入不毛。今南方已定，兵甲已足，当奖率三军，北定中原，庶竭驽钝，攘除奸凶，兴复汉室，还于旧都。此臣所以报先帝而忠陛下之职分也。至于斟酌损益，进尽忠言，则攸之、祎、允之任也。 5.亲贤臣，远小人，此先汉所以兴隆也；亲小人，远贤臣，此后汉所以倾颓也。先帝在时，每与臣论此事，未尝不叹息痛恨于桓、灵也。侍中、尚书、长史、参军，此悉贞良死节之臣，愿陛下亲之信之，则汉室之隆，可计日而待也。 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:30:0","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"案例分析 逐行读取文本信息。 解析文本信息到集合中。 遍历集合，按顺序，写出文本信息。 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:30:1","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"案例实现 public class BufferedTest { public static void main(String[] args) throws IOException { // 创建map集合,保存文本数据,键为序号,值为文字 HashMap\u003cString, String\u003e lineMap = new HashMap\u003c\u003e(); // 创建流对象 BufferedReader br = new BufferedReader(new FileReader(\"in.txt\")); BufferedWriter bw = new BufferedWriter(new FileWriter(\"out.txt\")); // 读取数据 String line = null; while ((line = br.readLine())!=null) { // 解析文本 String[] split = line.split(\"\\\\.\"); // 保存到集合 lineMap.put(split[0],split[1]); } // 释放资源 br.close(); // 遍历map集合 for (int i = 1; i \u003c= lineMap.size(); i++) { String key = String.valueOf(i); // 获取map中文本 String value = lineMap.get(key); // 写出拼接文本 bw.write(key+\".\"+value); // 写出换行 bw.newLine(); } // 释放资源 bw.close(); } } 第二章 转换流 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:30:2","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"2.1 字符编码和字符集 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:31:0","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"字符编码 计算机中储存的信息都是用二进制数表示的，而我们在屏幕上看到的数字、英文、标点符号、汉字等字符是二进制数转换之后的结果。按照某种规则，将字符存储到计算机中，称为编码 。反之，将存储在计算机中的二进制数按照某种规则解析显示出来，称为解码 。比如说，按照A规则存储，同样按照A规则解析，那么就能显示正确的文本符号。反之，按照A规则存储，再按照B规则解析，就会导致乱码现象。 编码:字符(能看懂的)–字节(看不懂的) 解码:字节(看不懂的)–\u003e字符(能看懂的) 字符编码Character Encoding : 就是一套自然语言的字符与二进制数之间的对应规则。 编码表:生活中文字和计算机中二进制的对应规则 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:31:1","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"字符集 字符集 Charset：也叫编码表。是一个系统支持的所有字符的集合，包括各国家文字、标点符号、图形符号、数字等。 计算机要准确的存储和识别各种字符集符号，需要进行字符编码，一套字符集必然至少有一套字符编码。常见字符集有ASCII字符集、GBK字符集、Unicode字符集等。 可见，当指定了编码，它所对应的字符集自然就指定了，所以编码才是我们最终要关心的。 ASCII字符集 ： ASCII（American Standard Code for Information Interchange，美国信息交换标准代码）是基于拉丁字母的一套电脑编码系统，用于显示现代英语，主要包括控制字符（回车键、退格、换行键等）和可显示字符（英文大小写字符、阿拉伯数字和西文符号）。 基本的ASCII字符集，使用7位（bits）表示一个字符，共128字符。ASCII的扩展字符集使用8位（bits）表示一个字符，共256字符，方便支持欧洲常用字符。 ISO-8859-1字符集： 拉丁码表，别名Latin-1，用于显示欧洲使用的语言，包括荷兰、丹麦、德语、意大利语、西班牙语等。 ISO-8859-1使用单字节编码，兼容ASCII编码。 GBxxx字符集： GB就是国标的意思，是为了显示中文而设计的一套字符集。 GB2312：简体中文码表。一个小于127的字符的意义与原来相同。但两个大于127的字符连在一起时，就表示一个汉字，这样大约可以组合了包含7000多个简体汉字，此外数学符号、罗马希腊的字母、日文的假名们都编进去了，连在ASCII里本来就有的数字、标点、字母都统统重新编了两个字节长的编码，这就是常说的\"全角\"字符，而原来在127号以下的那些就叫\"半角\"字符了。 GBK：最常用的中文码表。是在GB2312标准基础上的扩展规范，使用了双字节编码方案，共收录了21003个汉字，完全兼容GB2312标准，同时支持繁体汉字以及日韩汉字等。 GB18030：最新的中文码表。收录汉字70244个，采用多字节编码，每个字可以由1个、2个或4个字节组成。支持中国国内少数民族的文字，同时支持繁体汉字以及日韩汉字等。 Unicode字符集 ： Unicode编码系统为表达任意语言的任意字符而设计，是业界的一种标准，也称为统一码、标准万国码。 它最多使用4个字节的数字来表达每个字母、符号，或者文字。有三种编码方案，UTF-8、UTF-16和UTF-32。最为常用的UTF-8编码。 UTF-8编码，可以用来表示Unicode标准中任何字符，它是电子邮件、网页及其他存储或传送文字的应用中，优先采用的编码。互联网工程工作小组（IETF）要求所有互联网协议都必须支持UTF-8编码。所以，我们开发Web应用，也要使用UTF-8编码。它使用一至四个字节为每个字符编码，编码规则： 128个US-ASCII字符，只需一个字节编码。 拉丁文等字符，需要二个字节编码。 大部分常用字（含中文），使用三个字节编码。 其他极少使用的Unicode辅助字符，使用四字节编码。 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:31:2","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"2.2 编码引出的问题 在IDEA中，使用FileReader 读取项目中的文本文件。由于IDEA的设置，都是默认的UTF-8编码，所以没有任何问题。但是，当读取Windows系统中创建的文本文件时，由于Windows系统的默认是GBK编码，就会出现乱码。 public class ReaderDemo { public static void main(String[] args) throws IOException { FileReader fileReader = new FileReader(\"E:\\\\File_GBK.txt\"); int read; while ((read = fileReader.read()) != -1) { System.out.print((char)read); } fileReader.close(); } } 输出结果： ��� 那么如何读取GBK编码的文件呢？ ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:32:0","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"2.3 InputStreamReader类 转换流java.io.InputStreamReader，是Reader的子类，是从字节流到字符流的桥梁。它读取字节，并使用指定的字符集将其解码为字符。它的字符集可以由名称指定，也可以接受平台的默认字符集。 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:33:0","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"构造方法 InputStreamReader(InputStream in): 创建一个使用默认字符集的字符流。 InputStreamReader(InputStream in, String charsetName): 创建一个指定字符集的字符流。 构造举例，代码如下： InputStreamReader isr = new InputStreamReader(new FileInputStream(\"in.txt\")); InputStreamReader isr2 = new InputStreamReader(new FileInputStream(\"in.txt\") , \"GBK\"); ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:33:1","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"指定编码读取 public class ReaderDemo2 { public static void main(String[] args) throws IOException { // 定义文件路径,文件为gbk编码 String FileName = \"E:\\\\file_gbk.txt\"; // 创建流对象,默认UTF8编码 InputStreamReader isr = new InputStreamReader(new FileInputStream(FileName)); // 创建流对象,指定GBK编码 InputStreamReader isr2 = new InputStreamReader(new FileInputStream(FileName) , \"GBK\"); // 定义变量,保存字符 int read; // 使用默认编码字符流读取,乱码 while ((read = isr.read()) != -1) { System.out.print((char)read); // ��Һ� } isr.close(); // 使用指定编码字符流读取,正常解析 while ((read = isr2.read()) != -1) { System.out.print((char)read);// 大家好 } isr2.close(); } } ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:33:2","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"2.4 OutputStreamWriter类 转换流java.io.OutputStreamWriter ，是Writer的子类，是从字符流到字节流的桥梁。使用指定的字符集将字符编码为字节。它的字符集可以由名称指定，也可以接受平台的默认字符集。 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:34:0","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"构造方法 OutputStreamWriter(OutputStream in): 创建一个使用默认字符集的字符流。 OutputStreamWriter(OutputStream in, String charsetName): 创建一个指定字符集的字符流。 构造举例，代码如下： OutputStreamWriter isr = new OutputStreamWriter(new FileOutputStream(\"out.txt\")); OutputStreamWriter isr2 = new OutputStreamWriter(new FileOutputStream(\"out.txt\") , \"GBK\"); ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:34:1","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"指定编码写出 public class OutputDemo { public static void main(String[] args) throws IOException { // 定义文件路径 String FileName = \"E:\\\\out.txt\"; // 创建流对象,默认UTF8编码 OutputStreamWriter osw = new OutputStreamWriter(new FileOutputStream(FileName)); // 写出数据 osw.write(\"你好\"); // 保存为6个字节 osw.close(); // 定义文件路径 String FileName2 = \"E:\\\\out2.txt\"; // 创建流对象,指定GBK编码 OutputStreamWriter osw2 = new OutputStreamWriter(new FileOutputStream(FileName2),\"GBK\"); // 写出数据 osw2.write(\"你好\");// 保存为4个字节 osw2.close(); } } ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:34:2","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"转换流理解图解 转换流是字节与字符间的桥梁！ ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:34:3","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"2.5 练习：转换文件编码 将GBK编码的文本文件，转换为UTF-8编码的文本文件。 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:35:0","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"案例分析 指定GBK编码的转换流，读取文本文件。 使用UTF-8编码的转换流，写出文本文件。 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:35:1","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"案例实现 public class TransDemo { public static void main(String[] args) { // 1.定义文件路径 String srcFile = \"file_gbk.txt\"; String destFile = \"file_utf8.txt\"; // 2.创建流对象 // 2.1 转换输入流,指定GBK编码 InputStreamReader isr = new InputStreamReader(new FileInputStream(srcFile) , \"GBK\"); // 2.2 转换输出流,默认utf8编码 OutputStreamWriter osw = new OutputStreamWriter(new FileOutputStream(destFile)); // 3.读写数据 // 3.1 定义数组 char[] cbuf = new char[1024]; // 3.2 定义长度 int len; // 3.3 循环读取 while ((len = isr.read(cbuf))!=-1) { // 循环写出 osw.write(cbuf,0,len); } // 4.释放资源 osw.close(); isr.close(); } } 第三章 序列化 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:35:2","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"3.1 概述 Java 提供了一种对象序列化的机制。用一个字节序列可以表示一个对象，该字节序列包含该对象的数据、对象的类型和对象中存储的属性等信息。字节序列写出到文件之后，相当于文件中持久保存了一个对象的信息。 反之，该字节序列还可以从文件中读取回来，重构对象，对它进行反序列化。对象的数据、对象的类型和对象中存储的数据信息，都可以用来在内存中创建对象。看图理解序列化： ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:36:0","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"3.2 ObjectOutputStream类 java.io.ObjectOutputStream 类，将Java对象的原始数据类型写出到文件,实现对象的持久存储。 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:37:0","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"构造方法 public ObjectOutputStream(OutputStream out) ： 创建一个指定OutputStream的ObjectOutputStream。 构造举例，代码如下： FileOutputStream fileOut = new FileOutputStream(\"employee.txt\"); ObjectOutputStream out = new ObjectOutputStream(fileOut); ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:37:1","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"序列化操作 一个对象要想序列化，必须满足两个条件: 该类必须实现java.io.Serializable 接口，Serializable 是一个标记接口，不实现此接口的类将不会使任何状态序列化或反序列化，会抛出NotSerializableException 。 该类的所有属性必须是可序列化的。如果有一个属性不需要可序列化的，则该属性必须注明是瞬态的，使用transient 关键字修饰。 public class Employee implements java.io.Serializable { public String name; public String address; public transient int age; // transient瞬态修饰成员,不会被序列化 public void addressCheck() { System.out.println(\"Address check : \" + name + \" -- \" + address); } } 2.写出对象方法 public final void writeObject (Object obj) : 将指定的对象写出。 public class SerializeDemo{ public static void main(String [] args) { Employee e = new Employee(); e.name = \"zhangsan\"; e.address = \"beiqinglu\"; e.age = 20; try { // 创建序列化流对象 ObjectOutputStream out = new ObjectOutputStream(new FileOutputStream(\"employee.txt\")); // 写出对象 out.writeObject(e); // 释放资源 out.close(); fileOut.close(); System.out.println(\"Serialized data is saved\"); // 姓名，地址被序列化，年龄没有被序列化。 } catch(IOException i) { i.printStackTrace(); } } } 输出结果： Serialized data is saved ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:37:2","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"3.3 ObjectInputStream类 ObjectInputStream反序列化流，将之前使用ObjectOutputStream序列化的原始数据恢复为对象。 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:38:0","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"构造方法 public ObjectInputStream(InputStream in) ： 创建一个指定InputStream的ObjectInputStream。 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:38:1","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"反序列化操作1 如果能找到一个对象的class文件，我们可以进行反序列化操作，调用ObjectInputStream读取对象的方法： public final Object readObject () : 读取一个对象。 public class DeserializeDemo { public static void main(String [] args) { Employee e = null; try { // 创建反序列化流 FileInputStream fileIn = new FileInputStream(\"employee.txt\"); ObjectInputStream in = new ObjectInputStream(fileIn); // 读取一个对象 e = (Employee) in.readObject(); // 释放资源 in.close(); fileIn.close(); }catch(IOException i) { // 捕获其他异常 i.printStackTrace(); return; }catch(ClassNotFoundException c) { // 捕获类找不到异常 System.out.println(\"Employee class not found\"); c.printStackTrace(); return; } // 无异常,直接打印输出 System.out.println(\"Name: \" + e.name); // zhangsan System.out.println(\"Address: \" + e.address); // beiqinglu System.out.println(\"age: \" + e.age); // 0 } } 对于JVM可以反序列化对象，它必须是能够找到class文件的类。如果找不到该类的class文件，则抛出一个 ClassNotFoundException 异常。 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:38:2","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"反序列化操作2 **另外，当JVM反序列化对象时，能找到class文件，但是class文件在序列化对象之后发生了修改，那么反序列化操作也会失败，抛出一个InvalidClassException异常。**发生这个异常的原因如下： 该类的序列版本号与从流中读取的类描述符的版本号不匹配 该类包含未知数据类型 该类没有可访问的无参数构造方法 Serializable 接口给需要序列化的类，提供了一个序列版本号。serialVersionUID 该版本号的目的在于验证序列化的对象和对应类是否版本匹配。 public class Employee implements java.io.Serializable { // 加入序列版本号 private static final long serialVersionUID = 1L; public String name; public String address; // 添加新的属性 ,重新编译, 可以反序列化,该属性赋为默认值. public int eid; public void addressCheck() { System.out.println(\"Address check : \" + name + \" -- \" + address); } } ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:38:3","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"3.4 练习：序列化集合 将存有多个自定义对象的集合序列化操作，保存到list.txt文件中。 反序列化list.txt ，并遍历集合，打印对象信息。 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:39:0","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"案例分析 把若干学生对象 ，保存到集合中。 把集合序列化。 反序列化读取时，只需要读取一次，转换为集合类型。 遍历集合，可以打印所有的学生信息 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:39:1","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"案例实现 public class SerTest { public static void main(String[] args) throws Exception { // 创建 学生对象 Student student = new Student(\"老王\", \"laow\"); Student student2 = new Student(\"老张\", \"laoz\"); Student student3 = new Student(\"老李\", \"laol\"); ArrayList\u003cStudent\u003e arrayList = new ArrayList\u003c\u003e(); arrayList.add(student); arrayList.add(student2); arrayList.add(student3); // 序列化操作 // serializ(arrayList); // 反序列化 ObjectInputStream ois = new ObjectInputStream(new FileInputStream(\"list.txt\")); // 读取对象,强转为ArrayList类型 ArrayList\u003cStudent\u003e list = (ArrayList\u003cStudent\u003e)ois.readObject(); for (int i = 0; i \u003c list.size(); i++ ){ Student s = list.get(i); System.out.println(s.getName()+\"--\"+ s.getPwd()); } } private static void serializ(ArrayList\u003cStudent\u003e arrayList) throws Exception { // 创建 序列化流 ObjectOutputStream oos = new ObjectOutputStream(new FileOutputStream(\"list.txt\")); // 写出对象 oos.writeObject(arrayList); // 释放资源 oos.close(); } } 第四章 打印流 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:39:2","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"4.1 概述 平时我们在控制台打印输出，是调用print方法和println方法完成的，这两个方法都来自于java.io.PrintStream类，该类能够方便地打印各种数据类型的值，是一种便捷的输出方式。 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:40:0","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"4.2 PrintStream类 ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:41:0","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"构造方法 public PrintStream(String fileName) ： 使用指定的文件名创建一个新的打印流。 构造举例，代码如下： PrintStream ps = new PrintStream(\"ps.txt\")； ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:41:1","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"改变打印流向 System.out就是PrintStream类型的，只不过它的流向是系统规定的，打印在控制台上。不过，既然是流对象，我们就可以玩一个\"小把戏\"，改变它的流向。 public class PrintDemo { public static void main(String[] args) throws IOException { // 调用系统的打印流,控制台直接输出97 System.out.println(97); // 创建打印流,指定文件的名称 PrintStream ps = new PrintStream(\"ps.txt\"); // 设置系统的打印流流向,输出到ps.txt System.setOut(ps); // 调用系统的打印流,ps.txt中输出97 System.out.println(97); } } ","date":"2020-06-02","objectID":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/:41:2","tags":["java"],"title":"File类与IO流","uri":"/posts/java/java%E5%9F%BA%E7%A1%80/file%E7%B1%BB%E4%B8%8Eio%E6%B5%81/"},{"categories":null,"content":"git 子模块的管理和使用 参考：https://www.jianshu.com/p/9000cd49822c git 中可以使用子模块 submodule 管理不同的项目， submodule 允许你将一个 git 仓库当作另一个 git 仓库的子目录。 本文以 hugo 静态网页作为示例，运行如下命令新建一个名为 mysite 的 hugo 静态网页项目 hugo new site mysite 进入 mysite 静态网页项目中 cd mysite 将 mysite 初始化为一个 git 项目 git init ","date":"2020-05-12","objectID":"/posts/git/git%E5%AD%90%E6%A8%A1%E5%9D%97%E7%9A%84%E4%BD%BF%E7%94%A8/:1:0","tags":["git","github"],"title":"git子模块的使用","uri":"/posts/git/git%E5%AD%90%E6%A8%A1%E5%9D%97%E7%9A%84%E4%BD%BF%E7%94%A8/"},{"categories":null,"content":"添加子模块 将 LoveIt 主题作为子模块添加到 themes 文件夹下 git submodule add https://github.com/dillonzq/LoveIt.git themes/LoveIt 运行 git status ，可以看到目录增加了 .gitmodules 文件，这个文件用来保存子模块的信息。 PS D:\\workspace\\vscode\\mysite\u003e git status On branch main No commits yet Changes to be committed: (use \"git rm --cached \u003cfile\u003e...\" to unstage) new file: .gitmodules new file: themes/LoveIt Untracked files: (use \"git add \u003cfile\u003e...\" to include in what will be committed) archetypes/ config.toml 为了方便管理基于 github 的 hugo 博客，可以将 public 目录作为 hugo 博客的子项目文件夹，比如 git submodule add https://github.com/msdemt/msdemt.github.io.git public ","date":"2020-05-12","objectID":"/posts/git/git%E5%AD%90%E6%A8%A1%E5%9D%97%E7%9A%84%E4%BD%BF%E7%94%A8/:1:1","tags":["git","github"],"title":"git子模块的使用","uri":"/posts/git/git%E5%AD%90%E6%A8%A1%E5%9D%97%E7%9A%84%E4%BD%BF%E7%94%A8/"},{"categories":null,"content":"查看子模块 查看子模块命令 git submodule 示例： PS D:\\workspace\\vscode\\mysite\u003e git submodule 23a52ef1030b2c2057aa0bb1da57b6247da645a5 themes/LoveIt (v0.2.6-4-g23a52ef) 此时，若修改public下的子模块后，可以直接进行提交，比如 PS D:\\workspace\\vscodeProjects\\abc\\demo\\public\u003e rm .\\404.html PS D:\\workspace\\vscodeProjects\\abc\\demo\\public\u003e git add . PS D:\\workspace\\vscodeProjects\\abc\\demo\\public\u003e git commit -m \"test\" [main 51b91ee] test 1 file changed, 110 deletions(-) delete mode 100644 404.html PS D:\\workspace\\vscodeProjects\\abc\\demo\\public\u003e git push origin main Enumerating objects: 3, done. Counting objects: 100% (3/3), done. Delta compression using up to 4 threads Compressing objects: 100% (2/2), done. Writing objects: 100% (2/2), 207 bytes | 103.00 KiB/s, done. Total 2 (delta 1), reused 0 (delta 0), pack-reused 0 remote: Resolving deltas: 100% (1/1), completed with 1 local object. remote: Checking connectivity: 2, done. To https://github.com/msdemt/msdemt.github.io.git ae1ac3d..51b91ee main -\u003e main PS D:\\workspace\\vscodeProjects\\abc\\demo\\public\u003e ","date":"2020-05-12","objectID":"/posts/git/git%E5%AD%90%E6%A8%A1%E5%9D%97%E7%9A%84%E4%BD%BF%E7%94%A8/:1:2","tags":["git","github"],"title":"git子模块的使用","uri":"/posts/git/git%E5%AD%90%E6%A8%A1%E5%9D%97%E7%9A%84%E4%BD%BF%E7%94%A8/"},{"categories":null,"content":"第二次下载项目 下载具有子模块的项目 方式1，只下载项目，不下载子模块 $ git clone https://github.com/msdemt/mysite.git 查看子模块 $ git submodule -ae1ac3d4cdbf539c55e7a5f6fdcb234f0b1da55e public -f787a4e5ad4edf60467658d10c286248dc5027a6 themes/LoveIt 首先需要执行下子模块初始化命令 git submodule init 下载github上的子模块的最新版本 $ git submodule update --remote 更新项目内的子模块到上次同步的版本（建议使用上一个更新命令） git submodule update 此时最好提交下代码，以避免子模块代码更新，自己再次修改后，无法提交 git add . git commit -m \"update\" git push origin main 方式2，项目和子模块一起下载 $ git clone https://github.com/msdemt/mysite.git --recursive 进入 public 子模块目录 修改public目录下子模块，然后提交，出现问题，无法成功提交 hekai@HK-DESKTOP MINGW64 /d/workspace/vscodeProjects/test/mysite/public ((51b91ee...)) $ git add . hekai@HK-DESKTOP MINGW64 /d/workspace/vscodeProjects/test/mysite/public ((51b91ee...)) $ git commit -m \"delete\" [detached HEAD 662f7bc] delete 2 files changed, 399 deletions(-) delete mode 100644 \"vscode\\347\\232\\204\\344\\275\\277\\347\\224\\250/index.html\" delete mode 100644 \"vscode\\347\\232\\204\\344\\275\\277\\347\\224\\250/index.md\" hekai@HK-DESKTOP MINGW64 /d/workspace/vscodeProjects/test/mysite/public ((662f7bc...)) $ git push origin main Everything up-to-date 查看下当前分支 hekai@HK-DESKTOP MINGW64 /d/workspace/vscodeProjects/test/mysite/public ((662f7bc...)) $ git branch -a * (HEAD detached from 51b91ee) main remotes/origin/HEAD -\u003e origin/main remotes/origin/main 现在public子模块在一个本地分支上，切换到main分支 git checkout main 提交后发现 github 仓库中缺少目录，原因是 git 仅跟踪文件的变动，不跟踪目录。 解决办法是再空目录下创建 .gitkeep 文件 .gitkeep 是一个约定俗成的文件名并不会带有特殊规则 ","date":"2020-05-12","objectID":"/posts/git/git%E5%AD%90%E6%A8%A1%E5%9D%97%E7%9A%84%E4%BD%BF%E7%94%A8/:2:0","tags":["git","github"],"title":"git子模块的使用","uri":"/posts/git/git%E5%AD%90%E6%A8%A1%E5%9D%97%E7%9A%84%E4%BD%BF%E7%94%A8/"},{"categories":null,"content":"克隆包含子模块的项目 克隆包含子模块的项目有二种方法：一种是先克隆父项目，再更新子模块；另一种是直接递归克隆整个项目。 克隆父项目，再更新子模块 克隆父项目 git clone https://github.com/msdemt/mysite.git 查看子模块 PS D:\\workspace\u003e cd .\\mysite\\ PS D:\\workspace\\mysite\u003e git submodule -23a52ef1030b2c2057aa0bb1da57b6247da645a5 themes/LoveIt 子模块前面有一个 - ，说明子模块还未检入（空文件夹） 初始化子模块 git submodule init 初始化模块只需在克隆父项目后运行一次。 更新子模块到上次同步的版本 git submodule update 递归克隆整个项目，包括子模块 git clone https://github.com/msdemt/mysite.git --recursive 若想将项目文件内容克隆到指定文件夹，则可以在 git clone url 后加上指定的文件夹名称，比如 git clone https://github.com/msdemt/mysite.git assets --recursive ","date":"2020-05-12","objectID":"/posts/git/git%E5%AD%90%E6%A8%A1%E5%9D%97%E7%9A%84%E4%BD%BF%E7%94%A8/:2:1","tags":["git","github"],"title":"git子模块的使用","uri":"/posts/git/git%E5%AD%90%E6%A8%A1%E5%9D%97%E7%9A%84%E4%BD%BF%E7%94%A8/"},{"categories":null,"content":"修改子模块 子模块修改之前，当前位于一个本地的临时分支上，需要先执行下切换到主分支 git checkout main 在子模块文件夹中修改文件后，在子模块文件中中执行如下命令提交到子模块对应的远程项目分支 git add . git commit -m \"commit\" git push origin main ","date":"2020-05-12","objectID":"/posts/git/git%E5%AD%90%E6%A8%A1%E5%9D%97%E7%9A%84%E4%BD%BF%E7%94%A8/:2:2","tags":["git","github"],"title":"git子模块的使用","uri":"/posts/git/git%E5%AD%90%E6%A8%A1%E5%9D%97%E7%9A%84%E4%BD%BF%E7%94%A8/"},{"categories":null,"content":"删除子模块 删除子模块比较麻烦，需要手动删除相关的文件，否则在添加子模块时有可能出现错误 删除 LoveIt 子模块文件夹 git rm --cache themes/LoveIt rm -rf themes/LoveIt 删除 .gitmodules 文件中相关子模块信息 [submodule \"themes/LoveIt\"] path = themes/LoveIt url = https://github.com/dillonzq/LoveIt.git 删除 .git/config 中相关子模块信息 [submodule \"themes/LoveIt\"] url = https://github.com/dillonzq/LoveIt.git 删除 .git 文件夹中的相关子模块文件 rm -rf .git/modules/themes/LoveIt ","date":"2020-05-12","objectID":"/posts/git/git%E5%AD%90%E6%A8%A1%E5%9D%97%E7%9A%84%E4%BD%BF%E7%94%A8/:2:3","tags":["git","github"],"title":"git子模块的使用","uri":"/posts/git/git%E5%AD%90%E6%A8%A1%E5%9D%97%E7%9A%84%E4%BD%BF%E7%94%A8/"},{"categories":null,"content":"git 的回滚 git 提交失败后回滚到上一版本 git log 查看历史本信息 PS C:\\Users\\He\\OneDrive\\Documents\\vscode\\mysite\u003e git log commit 135e549469efe632297600fbb22e4eb9f59e8ee3 (HEAD -\u003e main, origin/main, origin/HEAD) Author: msdemt \u003c276516848@qq.com\u003e Date: Sun Jun 7 10:20:47 2020 +0800 提交更新 commit b1c4b5812fd787f3b1ad3dbb95f382f325b5b983 Author: msdemt \u003c276516848@qq.com\u003e Date: Wed Jun 3 21:45:17 2020 +0800 java基础 git reset –hard 历史id 回滚到某个id版本 PS C:\\Users\\He\\OneDrive\\Documents\\vscode\\mysite\u003e git reset --hard b1c4b5812fd787f3b1ad3dbb95f382f325b5b983 HEAD is now at b1c4b58 java基础 提交到github git push -f origin main 回滚后如果想再次恢复上一版本 git reflog PS C:\\Users\\He\\OneDrive\\Documents\\vscode\\mysite\u003e git reflog b1c4b58 (HEAD -\u003e main, origin/main, origin/HEAD) HEAD@{0}: reset: moving to b1c4b5812fd787f3b1ad3dbb95f382f325b5b983 135e549 HEAD@{1}: commit: 提交更新 b1c4b58 (HEAD -\u003e main, origin/main, origin/HEAD) HEAD@{2}: clone: from https://github.com/msdemt/mysite.git 使用 git reset --hard 历史id PS C:\\Users\\He\\OneDrive\\Documents\\vscode\\mysite\u003e git reset --hard 135e549 HEAD is now at 135e549 提交更新 ","date":"2020-05-12","objectID":"/posts/git/git%E5%AD%90%E6%A8%A1%E5%9D%97%E7%9A%84%E4%BD%BF%E7%94%A8/:3:0","tags":["git","github"],"title":"git子模块的使用","uri":"/posts/git/git%E5%AD%90%E6%A8%A1%E5%9D%97%E7%9A%84%E4%BD%BF%E7%94%A8/"},{"categories":null,"content":"使用 k8s 安装 gitlab https://docs.gitlab.com/charts/ gitlab 官方的 helm chart 地址：https://gitlab.com/gitlab-org/charts/gitlab 下载 master 分支的 chart mkdir /home/k8s/gitlab \u0026\u0026 cd /home/k8s/gitlab wget https://gitlab.com/gitlab-org/charts/gitlab/-/archive/master/gitlab-master.tar.gz tar -zxf gitlab-master.tar.gz 查看 chart 里的一些说明，如：gitlab-master/doc/advanced/external-nginx/index.md 中指出了自定义 gitlab 的 ingress 选项， 如果 k8s 集群中有多个 nginx ingress 控制器，则 Ingress 使用 annotation 来标志哪一个控制器实现它，如果你不想使用 git lab chart 自带的 nginx ingress 控制器， 而是想使用其他的 nginx ingress 控制器，可以通过 global.ingress.class 进行指定。 多个 nginx ingress controller 参考：https://github.com/kubernetes/ingress-nginx/blob/c3ce6b892e5f1cc1066f81c19482dded2901ad45/docs/user-guide/multiple-ingress.md 如果你的 k8s 集群中已有一个 nginx ingress 控制器了，你不想 gitlab chart 再安装一个 nginx ingress 控制器，可以通过 nginx-ingress.enabled=false 来让 gitlab chart 不安装 nginx ingress 控制器。 该文件里还指出了自定义证书管理器的说明：如果你的 k8s 集群里已经有一个额外的 ingress 控制器了，那么你可能也已有一个额外的 cert-manager 实例或其他管理证书的工具。如果你的 k8s 集群中已经有 cert-manager 了，不想 gitlab chart 安装，可以通过 certmanager.install=false 来让 gitlab chart 不安装 cert-manager。并且通过设置 global.ingress.configureCertmanager=false 来让 gitlab chart 不去创建证书。详细的文档可以参考：https://gitlab.com/gitlab-org/charts/gitlab/blob/master/doc/installation/tls.md。 查看 gitlab-master/doc/installation/tls.md，里面说如果你已经安装了 cert-manager ，可以通过使用 global.ingress.annotations 来为你的cert-manager 部署配置适合的 annotations。本文已经安装 nginx ingress 控制器、 cert-manager 了，并在 cert-manager 配置了 Issuer ，所以需要看 External cert-manager and Issuer (external)，内容如下 To make use of an external `cert-manager` and `Issuer` resource you must provide several items, so that self-signed certificates are not activated. 1. Annotations to activate the external `cert-manager` (see [documentation][cm-annotations] for further details) 1. Names of TLS secrets for each service (this deactivates [self-signed behaviors](#option-4-use-auto-generated-self-signed-wildcard-certificate)) helm install ... --set cert-manager.install=false \\ --set global.ingress.configureCertmanager=false \\ --set global.ingress.annotations.\"kubernetes\\.io/tls-acme\"=true \\ --set gitlab.unicorn.ingress.tls.secretName=RELEASE-gitlab-tls \\ --set registry.ingress.tls.secretName=RELEASE-registry-tls \\ --set minio.ingress.tls.secretName=RELEASE-minio-tls 查看 gitlab-master/doc/charts/globals.md，里面说明了怎么指定 annotations 和 configureCertmanager的作用。 本文使用的 cert-manager 自动生成证书的 annotation 为 cert-manager.io/cluster-issuer: cluster-issuer，需要替换下，然后下面的三个 secretName 是 cluster-issuer 生成的 Ingress 使用的 secret （用于 https）。 综上，我们使用如下命令生成 gitlab chart 的清单文件 [root@k8s-m1 gitlab]# helm template ./gitlab-master --name mygitlab --set global.edition=ce --set global.time_zone=CST+8 --set global.hosts.domain=k8s.abc --set nginx-ingress.enabled=false --set global.ingress.class=nginx --set prometheus.install=false --set certmanager.install=false --set global.ingress.configureCertmanager=false --set global.ingress.annotations.\"cert-manager\\.io\\/cluster-issuer\"=cluster-issuer --set gitlab.unicorn.ingress.tls.secretName=RELEASE-gitlab-tls --set registry.ingress.tls.secretName=RELEASE-registry-tls --set minio.ingress.tls.secretName=RELEASE-minio-tls --namespace gitlab \u003e gitlab.yaml 报错了 Error: found in requirements.yaml, but missing in charts/ directory: cert-manager, prometheus, postgresql, gitlab-runner, grafana 因为 gitlab chart 依赖这些chart，使用如下命令下载依赖 helm dep up ./gitlab-master 因为这些子 chart 的地址是 kubernetes-charts.storage.googleapis.com，国内无法访问，所以可能下载失败。 可以通过配置代理的方式，实现下载这些子 chart，在 /etc/profile 里添加 export http_proxy=\"http://192.168.14.58:8580\"，然后再执行 source /etc/profile，这台 linux 通过代理下载子chart 了。 如果想要去除这个代理，在 /etc/profile 里将 http_proxy 修改为 export http_proxy=，然后执行 source /etc/profile 就可以了。 注意：不要配置 https_proxy，如果配置了可能会报错 proxyconnect tcp: tls: first record does not look like a TLS handshake。 [root@k8s-m1 gitlab]# helm dep up ./gitlab-master Hang tight while we grab the latest from your chart repositories... ...Unable to get an update from the \"local\" chart repository (http://127.0.0.1:8879/charts): G","date":"2019-11-01","objectID":"/posts/linux/gitlab%E5%AE%89%E8%A3%85%E8%AE%B0%E5%BD%95/:0:0","tags":null,"title":"Gitlab安装记录","uri":"/posts/linux/gitlab%E5%AE%89%E8%A3%85%E8%AE%B0%E5%BD%95/"},{"categories":null,"content":"运行 jenkins master 的清单文件，使用上面的镜像部署一个 jenkins 的服务。 kubectl apply -f jenkins.yaml -f jenkins-ing.yml 查看 jenkins 是否运行成功 $ kubectl get pods -n jenkins-ns NAME READY STATUS RESTARTS AGE jenkins-deploy-7dbcdd9848-rkksg 1/1 Running 0 16m jenkins 首次登录需要输入自动生成的密码，使用如下命令获取 kubectl logs -n jenkins-ns -f jenkins-deploy-7dbcdd9848-rkksg 密码如下 ************************************************************* Jenkins initial setup is required. An admin user has been created and a password generated. Please use the following password to proceed to installation: 5ad0222348dc4a24b36a9238b82ae725 This may also be found at: /var/jenkins_home/secrets/initialAdminPassword ************************************************************* 安装推荐的插件，由于网络的原因，可能会下载失败。 跳过， jenkins 默认通过 http://updates.jenkins-ci.org 域名下载插件，如 http://updates.jenkins-ci.org/download/plugins/trilead-api/1.0.5/trilead-api.hpi，某些情况下该域名无法访问，所以跳过推荐插件安装，待进入 jenkins 页面后配置插件源再安装需要的插件。 配置jenkins 插件源 进入jenkins 页面后，依次选择 Manage Jenkins - Manage Plugins - Advanced - Update Site 将 https://updates.jenkins.io/update-center.json 修改为清华大学镜像源 https://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.json，点击Submit。 安装推荐的插件 jenkins 默认推荐的插件如下图，可按需安装 这么多推荐的插件，一个个手动搜索安装特别麻烦 或者 jenkins pod 启动成功后，修改挂载的文件中指定的插件源， 本文是使用 nfs 挂载的，找到 jenkins 挂载出的目录 /home/data/nfs/data/jenkins-ns-jenkins-home-claim-pvc-6db72e8b-85a9-448c-be0f-4499d19983cd 修改 hudson.model.UpdateCenter.xml 将 https://updates.jenkins.io/update-center.json 修改为 https://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.json 修改后重启 jenkin pod，从而让 jenkins 重新加载插件源的地址 kubectl delete po -n jenkins-ns jenkins-deploy-7dbcdd9848-l7wq2 然后登录 jenkins 进行推荐插件的安装 发现，还是有下载失败的插件，查看 jenkins 的日志，发现插件还是从updates.jenkins-ci.org 下载的 因为https://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.json 中指定的插件的下载地址是updates.jenkins-ci.org。 解决：修改 jenkins目录/updates/default.json，将updates.jenkins-ci.org/download 修改为https://mirrors.tuna.tsinghua.edu.cn/jenkins，从而真正使用清华大学的镜像地址下载插件。 方式一：使用 vim 进行批量修改 vim default.json 输入 : 进入命令模式，然后输入如下内容进行替换，然后按Enter 1,$s/http:\\/\\/updates.jenkins-ci.org\\/download/https:\\/\\/mirrors.tuna.tsinghua.edu.cn\\/jenkins/g 替换连接测试url，再次输入 : 进入命令模式，然后输入如下命令后按Enter 1,$s/http:\\/\\/www.google.com/https:\\/\\/www.baidu.com/g 替换玩后输入 :wq 保存退出 方式二： 使用 sed 修改 sed -i 's/http:\\/\\/updates.jenkins-ci.org\\/download/https:\\/\\/mirrors.tuna.tsinghua.edu.cn\\/jenkins/g' default.json \u0026\u0026 sed -i 's/http:\\/\\/www.google.com/https:\\/\\/www.baidu.com/g' default.json 修改后重启 jenkin pod，从而让 jenkins 重新加载插件源的地址 kubectl delete po -n jenkins-ns jenkins-deploy-7dbcdd9848-l7wq2 登录 jenkins 通过上面查看到的初始密码，若忘记了，可以通过 jenkins 挂载目录下的 secret 中的 initialAdminPassword文件查看，如本文的为 [root@c48-2 secrets]# pwd /home/data/nfs/data/jenkins-ns-jenkins-home-claim-pvc-1391b4f1-d614-4321-85f9-6279e0dca936/secrets [root@c48-2 secrets]# cat initialAdminPassword 73c7a3a8a9ed4972a7cdb06d5de72076 安装推荐的插件，速度很快就安装好了。 创建第一个管理员用户 实例配置，默认即可 登录后，页面显示空白！ 搜索了下，说是空白的原因是因为 镜像的升级站点为 https 导致的，需要改为 http。 尝试了下，访问 https://jenkins.k8s.abc/pluginManager/advanced ，当前的升级站点为 https://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.json，改为 http://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.json，再次访问主页就正常了。 所以 修改的时候，只修改 updates/default.json 就可以了，不用修改 hudson.model.UpdateCenter.xml 不行，这样首次登录还是页面空白 需要将 hudson.model.UpdateCenter.xml 中的地址改为 http，且修改update/default.json。 验证后，确实改为 http 后 首次登录正常了。为什么不该为清华镜像的http呢？？ 所以，总结下，将hudson.model.UpdateCenter.xml中的地址改为 http://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.json ，然后修改updates/default.json。 sed -i 's/http:\\/\\/updates.jenkins-ci.org\\/download/https:\\/\\/mirrors.tuna.tsinghua.edu.cn\\/jenkins/g' updates/default.json \u0026\u0026 sed -i 's/http:\\/\\/www.google.com/https:\\/\\/www.baidu.com/g' updates/default.json 再次安装，首次登录后又出现了空白的情况，重启下 pod 就可以了。 有强迫症的话，也可以把 updates/default.json 里清华大学镜像源的地址配置为 http 的。 如果 jenkins 有启动参数可以修改插件的地址就好了。 总结： jenkins 部署后，修改 jenkins 挂载出的目录的 hudson.model.UpdateCenter.xml ，将 url 的值修改为 https://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.js","date":"2019-10-25","objectID":"/posts/k8s/jenkins%E5%AE%89%E8%A3%85%E8%AE%B0%E5%BD%95/:0:0","tags":null,"title":"Jenkins安装记录","uri":"/posts/k8s/jenkins%E5%AE%89%E8%A3%85%E8%AE%B0%E5%BD%95/"},{"categories":null,"content":"官方文档地址：https://pingcap.com/docs-cn/v3.0/ ","date":"2019-10-24","objectID":"/posts/k8s/tidb-v3.0-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:0:0","tags":null,"title":"Tidb V3","uri":"/posts/k8s/tidb-v3.0-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"TiDB in Kubernetes ","date":"2019-10-24","objectID":"/posts/k8s/tidb-v3.0-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:1:0","tags":null,"title":"Tidb V3","uri":"/posts/k8s/tidb-v3.0-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"安装本地持久卷提供者 参考：https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner#version-compatibility 在下载页面下载 发布包 wget https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner/archive/v2.3.3.tar.gz 解压 tar -zxvf v2.3.3.tar.gz 新建一个 StorageClass To delay volume binding until pod scheduling and to handle multiple local PVs in a single pod, a StorageClass must to be created with volumeBindingMode set to WaitForFirstConsumer. cd sig-storage-local-static-provisioner-2.3.3 cp deployment/kubernetes/example/default_example_storageclass.yaml deployment/kubernetes/example/default_example_storageclass.yaml.orig 修改 StorageClass 的 yaml ，将 sc 的名字由 fast-disks 改为 local-disk-storage 。（此步可以跳过，我只是觉得 local-disk-storage 这个名字更好辨认） vi deployment/kubernetes/example/default_example_storageclass.yaml 部署 StorageClass kubectl create -f deployment/kubernetes/example/default_example_storageclass.yaml 查看新建的 StorageClass $ kubectl get sc NAME PROVISIONER AGE local-disk-storage kubernetes.io/no-provisioner 6s managed-nfs-storage (default) nfs-storage 12h 使用 helm 创建本地持久卷提供者 修改 helm 的 values.yaml cp helm/provisioner/values.yaml helm/provisioner/values.yaml.orig 修改后和原文对比如下 # diff helm/provisioner/values.yaml helm/provisioner/values.yaml.orig 12c12 \u003c namespace: local-static-provisioner --- \u003e namespace: default 16c16 \u003c createNamespace: true --- \u003e createNamespace: false 53c53 \u003c - name: local-disk-storage # Defines name of storage classe. --- \u003e - name: fast-disks # Defines name of storage classe. 56c56 \u003c hostDir: /home/data/local-disk-storage --- \u003e hostDir: /mnt/fast-disks 修改了命名空间名，改为了将改 helm 的资源安装到 local-static-provisioner 命名空间，并且自动新建该命名空间，修改了hostDir，改为了 /home/data/local-disk-storage， 注意，因为在上面修改了 StorageClass 的名字，所以需要在 values.yaml 里也修改下。 使用 helm template 命令将该 chart 生成为一个清单文件。 helm template ./helm/provisioner \u003e deployment/kubernetes/provisioner_generated.yaml 部署提供者 kubectl create -f deployment/kubernetes/provisioner_generated.yaml 测试 本地卷提供者启动成功后，它会检查并创建 本地卷 pv。比如，如果 /home/data/local-disk-storage 目录中包含一个子目录 /home/data/local-disk-storage/vol1 ，且该子目录已经被挂载了，那么静态本地卷提供者会自动创建对应的 pv 。 mkdir /home/data/local-disk-storage/vol1 mount -t tmpfs /home/data/local-disk-storage/vol1/ /home/data/local-disk-storage/vol1/ 上文中，使用的是该文件夹自己挂载自己，来模拟文件夹被其他磁盘卷挂载。 可以看到，本地卷提供者自动新建了 pv $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE local-pv-5b8d1757 7997Mi RWO Delete Available local-disk-storage 4m41s $ kubectl describe pv local-pv-5b8d1757 Name: local-pv-5b8d1757 Labels: \u003cnone\u003e Annotations: pv.kubernetes.io/provisioned-by: local-volume-provisioner-k8s-m1-7725028a-d16c-455f-93d5-400dccf19d8a Finalizers: [kubernetes.io/pv-protection] StorageClass: local-disk-storage Status: Available Claim: Reclaim Policy: Delete Access Modes: RWO VolumeMode: Filesystem Capacity: 7997Mi Node Affinity: Required Terms: Term 0: kubernetes.io/hostname in [k8s-m1] Message: Source: Type: LocalVolume (a persistent volume backed by local storage on a node) Path: /home/data/local-disk-storage/vol1 Events: \u003cnone\u003e https://www.codercto.com/a/53701.html https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner/blob/master/docs/faqs.md#why-i-need-to-bind-mount-normal-directories-to-create-pvs-for-them Why I need to bind mount normal directories to create PVs for them This is because there is a race between mounting another filesystem volume on a normal directory and creating a PV for it. If you want to create PVs for normal directories which do not have a mount point, you need to bind mount them onto another directory under discovery directory, or themselves if they are already in. Mount point on directory explicitly express it is ready to have a PV created for it. 本地卷提供者自动创建 pv 需要两个条件：该目录是被绑定的，该目录是发现目录的第一级子目录。 Provisioner only creates local PVs for mounted directories in the first level of discovery directory. This is because there is no way to detect whether the sub-directory is created by system admin for discovering as local PVs or by user ap","date":"2019-10-24","objectID":"/posts/k8s/tidb-v3.0-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:1:1","tags":null,"title":"Tidb V3","uri":"/posts/k8s/tidb-v3.0-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":" 使用 vscode 的过程中，发现对 yaml 格式的文本进行 tab ，之后内容的格式会发生变化。 如对下面的 yaml 文本 apiVersion:v1kind:Servicemetadata:name:nginxspec:ports:- name:httpport:80protocol:TCPtargetPort:80selector:app:nginxtype:LoadBalancer 使用 tab 键后，结果如下 apiVersion:v1kind:Servicemetadata:name:nginxspec:ports:- name:httpport:80protocol:TCPtargetPort:80selector:app:nginxtype:LoadBalancer 解决方法： 进入 File - Preferences - Settings ，搜索 tab ，将 Editor: Use Tab Stops 关掉 或者 直接在 settings.json 中添加 \"editor.useTabStops\": false 缩进转为空格 将文本复制到 vscode 中时，原文本中的 tab 不会自动转为空格，可以点击 shift+control+p ，然后输入 convert ， 查找 convert Indention to spaces，可以批量地将当前文本中的 tab 转为空格。 markdown 中输入上标和下标 上标输出 n 的平方有两种方法： n^2^ n^2^ n2 n\u003csup\u003e2\u003c/sup\u003e 下标输出 n 下标2有两种方法： n~2~ n~2~ n2 n\u003csub\u003e2\u003c/sub\u003e vscode 自带的 preview 无法显示，需要安装扩展 Markdown Preview Enhanced ，然后使用其 preview 进行预览。 vscode 添加代码分割线（垂直标尺） 在 settings.json 中添加 \"editor.rulers\": [100] powershell core 在使用 vscode 默认的 terminal 时，会弹出一个提示，如下： Try the new cross-platform PowerShell https://aka.ms/pscore6 尝试新的跨平台的 powershell。 最初，Windows PowerShell 是在 .NET Framework 基础之上构建而成，仅适用于 Windows 系统。 在最新版本中， PowerShell Core 使用 .NET Core 2.x 作为运行时。 PowerShell Core 支持 Windows、macOS 和 Linux 平台。 那就安装下这个跨平台版本的 powershell，项目地址位于 github https://github.com/powershell/powershell 可以从该 github 的 release 页面下载最新的发布版本进行安装。 安装后，点击 vscode 的终端界面（使用 crtl + ~ 打开），选择终端界面右侧终端选择的下拉框中的 选择默认Shell ， 选择新安装的 Power Shell Core 即可，选择了之后，会在 settings.json 中自动添加如下配置 \"terminal.integrated.shell.windows\": \"C:\\\\Program Files\\\\PowerShell\\\\6\\\\pwsh.exe\", vscode 的缩进空格数 发现 md 文档中，某些 md 文档的缩进空格数是 4 ，某些是 2 ，为什么呢？通过测试，发现如果 md 文档中含有 yaml 格式的代码块， 会将缩进空格数置为 2 。 因为在 vscode 的设置中，启用了 Detect Indentation ，它会检测文件内容从而自动配置缩进空格数和tab转为空格，将其关闭即可。 关闭后，会在 settings.json 中自动生成如下配置 \"editor.detectIndentation\": false, ","date":"2019-10-23","objectID":"/posts/vscode/vscode%E7%9A%84%E4%BD%BF%E7%94%A8/:0:0","tags":null,"title":"Vscode的使用","uri":"/posts/vscode/vscode%E7%9A%84%E4%BD%BF%E7%94%A8/"},{"categories":null,"content":" 参考： https://github.com/kubernetes/dashboard Kubernetes Dashboard is a general purpose, web-based UI for Kubernetes clusters. It allows users to manage applications running in the cluster and troubleshoot them, as well as manage the cluster itself. ","date":"2019-10-22","objectID":"/posts/k8s/kubernetes-dashboard-v1.10.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:0:0","tags":null,"title":"Kubernetes-Dashboard-V1.10.1-官方文档阅读笔记","uri":"/posts/k8s/kubernetes-dashboard-v1.10.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Total View ","date":"2019-10-22","objectID":"/posts/k8s/kubernetes-dashboard-v1.10.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:1:0","tags":null,"title":"Kubernetes-Dashboard-V1.10.1-官方文档阅读笔记","uri":"/posts/k8s/kubernetes-dashboard-v1.10.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Getting Started IMPORTANT: Read the Access Control guide before performing any further steps. The default Dashboard deployment contains a minimal set of RBAC privileges needed to run. To deploy Dashboard, execute following command: $ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml To access Dashboard from your local workstation you must create a secure channel to your Kubernetes cluster. Run the following command: $ kubectl proxy Now access Dashboard at: http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/. ","date":"2019-10-22","objectID":"/posts/k8s/kubernetes-dashboard-v1.10.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:1:1","tags":null,"title":"Kubernetes-Dashboard-V1.10.1-官方文档阅读笔记","uri":"/posts/k8s/kubernetes-dashboard-v1.10.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Create An Authentication Token (RBAC) To find out how to create sample user and log in follow Creating sample user guide. NOTE: Kubeconfig Authentication method does not support external identity providers or certificate-based authentication. Dashboard can only be accessed over HTTPS Heapster has to be running in the cluster for the metrics and graphs to be available. Read more about it in Integrations guide. ","date":"2019-10-22","objectID":"/posts/k8s/kubernetes-dashboard-v1.10.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:1:2","tags":null,"title":"Kubernetes-Dashboard-V1.10.1-官方文档阅读笔记","uri":"/posts/k8s/kubernetes-dashboard-v1.10.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Documentation Dashboard documentation can be found on docs directory which contains: Common: Entry-level overview User Guide: Installation, Accessing Dashboard and more for users Developer Guide: Getting Started, Dependency Management and more for anyone interested in contributing ","date":"2019-10-22","objectID":"/posts/k8s/kubernetes-dashboard-v1.10.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:1:3","tags":null,"title":"Kubernetes-Dashboard-V1.10.1-官方文档阅读笔记","uri":"/posts/k8s/kubernetes-dashboard-v1.10.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Kubernetes Dashboard Documentation ","date":"2019-10-22","objectID":"/posts/k8s/kubernetes-dashboard-v1.10.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:2:0","tags":null,"title":"Kubernetes-Dashboard-V1.10.1-官方文档阅读笔记","uri":"/posts/k8s/kubernetes-dashboard-v1.10.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Common FAQ Roadmap Dashboard arguments ","date":"2019-10-22","objectID":"/posts/k8s/kubernetes-dashboard-v1.10.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:2:1","tags":null,"title":"Kubernetes-Dashboard-V1.10.1-官方文档阅读笔记","uri":"/posts/k8s/kubernetes-dashboard-v1.10.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"User Guide Installation Certificate management Accessing Dashboard 1.7.x and above 1.6.x and below Access control Creating sample user Integrations Labels ","date":"2019-10-22","objectID":"/posts/k8s/kubernetes-dashboard-v1.10.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:2:2","tags":null,"title":"Kubernetes-Dashboard-V1.10.1-官方文档阅读笔记","uri":"/posts/k8s/kubernetes-dashboard-v1.10.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Developer Guide Getting started Release procedures Dependency management Architecture Code conventions Text conventions Internationalization Plugins Common Dashboard arguments Dashboard container accepts multiple arguments that can be used to customize it a bit. In example we are using --auto-generate-certificates flag in our recommended setup YAML files to pass certificates to Dashboard. Arguments Argument name Default value Description insecure-port 9090 The port to listen to for incoming HTTP requests. port 8443 The secure port to listen to for incoming HTTPS requests. insecure-bind-address 127.0.0.1 The IP address on which to serve the --insecure-port (set to 127.0.0.1 for all interfaces). bind-address 0.0.0.0 The IP address on which to serve the --port (set to 0.0.0.0 for all interfaces). default-cert-dir /certs Directory path containing --tls-cert-file and --tls-key-file files. Used also when auto-generating certificates flag is set. Relative to the container, not the host. tls-cert-file - File containing the default x509 Certificate for HTTPS. tls-key-file - File containing the default x509 private key matching –tls-cert-file. auto-generate-certificates false When set to true, Dashboard will automatically generate certificates used to serve HTTPS. apiserver-host - The address of the Kubernetes Apiserver to connect to in the format of protocol://address:port, e.g., http://localhost:8080. If not specified, the assumption is that the binary runs inside a Kubernetes cluster and local discovery is attempted. api-log-level INFO Level of API request logging. Should be one of ‘INFO heapster-host - The address of the Heapster Apiserver to connect to in the format of protocol://address:port, e.g., http://localhost:8082. If not specified, the assumption is that the binary runs inside a Kubernetes cluster and service proxy will be used. sidecar-host - The address of the Sidecar Apiserver to connect to in the format of protocol://address:port, e.g., http://localhost:8000. If not specified, the assumption is that the binary runs inside a Kubernetes cluster and service proxy will be used. metrics-provider sidecar Select provider type for metrics. ‘none’ will not check metrics. metric-client-check-period 30 Time in seconds that defines how often configured metric client health check should be run. kubeconfig - Path to kubeconfig file with authorization and master location information. namespace kube-system When non-default namespace is used, create encryption key in the specified namespace. token-ttl 900 Expiration time (in seconds) of JWE tokens generated by dashboard. ‘0’ never expires. authentication-mode token Enables authentication options that will be reflected on login screen. Supported values: token, basic. Note that basic option should only be used if apiserver has ‘–authorization-mode=ABAC’ and ‘–basic-auth-file’ flags set. enable-insecure-login false When enabled, Dashboard login view will also be shown when Dashboard is not served over HTTPS. enable-skip-login false When enabled, the skip button on the login page will be shown. disable-settings-authorizer false When enabled, Dashboard settings page will not require user to be logged in and authorized to access settings page. locale-config ./locale_conf.json File containing the configuration of locales. system-banner - When non-empty displays message to Dashboard users. Accepts simple HTML tags. system-banner-severity INFO Severity of system banner. Should be one of ‘INFO User Guide Installation Official release IMPORTANT: Before upgrading from older version of Dashboard to 1.7+ make sure to delete Cluster Role Binding for kubernetes-dashboard Service Account, otherwise Dashboard will have full admin access to the cluster. Quick setup The fastest way of deploying Dashboard has been described in our README. It is destined for people that are new to Kubernetes and want to quickly start using Dashboard. Other possible setups for more experienced users, that want to know more ab","date":"2019-10-22","objectID":"/posts/k8s/kubernetes-dashboard-v1.10.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:2:3","tags":null,"title":"Kubernetes-Dashboard-V1.10.1-官方文档阅读笔记","uri":"/posts/k8s/kubernetes-dashboard-v1.10.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Create Service Account We are creating Service Account with name admin-user in namespace kubernetes-dashboard first. apiVersion:v1kind:ServiceAccountmetadata:name:admin-usernamespace:kubernetes-dashboard ","date":"2019-10-22","objectID":"/posts/k8s/kubernetes-dashboard-v1.10.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:3:0","tags":null,"title":"Kubernetes-Dashboard-V1.10.1-官方文档阅读笔记","uri":"/posts/k8s/kubernetes-dashboard-v1.10.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Create ClusterRoleBinding In most cases after provisioning our cluster using kops or kubeadm or any other popular tool, the ClusterRole admin-Role already exists in the cluster. We can use it and create only ClusterRoleBinding for our ServiceAccount. NOTE: apiVersion of ClusterRoleBinding resource may differ between Kubernetes versions. Prior to Kubernetes v1.8 the apiVersion was rbac.authorization.k8s.io/v1beta1. apiVersion:rbac.authorization.k8s.io/v1kind:ClusterRoleBindingmetadata:name:admin-userroleRef:apiGroup:rbac.authorization.k8s.iokind:ClusterRolename:cluster-adminsubjects:- kind:ServiceAccountname:admin-usernamespace:kubernetes-dashboard ","date":"2019-10-22","objectID":"/posts/k8s/kubernetes-dashboard-v1.10.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:4:0","tags":null,"title":"Kubernetes-Dashboard-V1.10.1-官方文档阅读笔记","uri":"/posts/k8s/kubernetes-dashboard-v1.10.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Bearer Token Now we need to find token we can use to log in. Execute following command: kubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret | grep admin-user | awk '{print $1}') It should print something like: Name: admin-user-token-v57nw Namespace: kubernetes-dashboard Labels: \u003cnone\u003e Annotations: kubernetes.io/service-account.name: admin-user kubernetes.io/service-account.uid: 0303243c-4040-4a58-8a47-849ee9ba79c1 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1066 bytes namespace: 20 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLXY1N253Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIwMzAzMjQzYy00MDQwLTRhNTgtOGE0Ny04NDllZTliYTc5YzEiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZXJuZXRlcy1kYXNoYm9hcmQ6YWRtaW4tdXNlciJ9.Z2JrQlitASVwWbc-s6deLRFVk5DWD3P_vjUFXsqVSY10pbjFLG4njoZwh8p3tLxnX_VBsr7_6bwxhWSYChp9hwxznemD5x5HLtjb16kI9Z7yFWLtohzkTwuFbqmQaMoget_nYcQBUC5fDmBHRfFvNKePh_vSSb2h_aYXa8GV5AcfPQpY7r461itme1EXHQJqv-SN-zUnguDguCTjD80pFZ_CmnSE1z9QdMHPB8hoB4V68gtswR1VLa6mSYdgPwCHauuOobojALSaMc3RH7MmFUumAgguhqAkX3Omqd3rJbYOMRuMjhANqd08piDC3aIabINX6gP5-Tuuw2svnV6NYQ Now copy the token and paste it into Enter token field on login screen. Click Sign in button and that’s it. You are now logged in as an admin. In order to find out more about how to grant/deny permissions in Kubernetes read official authentication \u0026 authorization documentation. ","date":"2019-10-22","objectID":"/posts/k8s/kubernetes-dashboard-v1.10.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:5:0","tags":null,"title":"Kubernetes-Dashboard-V1.10.1-官方文档阅读笔记","uri":"/posts/k8s/kubernetes-dashboard-v1.10.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":" 参考：https://rook.io/docs/rook/v1.1/ ","date":"2019-10-20","objectID":"/posts/k8s/rook-ceph-v1.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:0:0","tags":null,"title":"Rook-Ceph-v1.1-官方文档阅读笔记","uri":"/posts/k8s/rook-ceph-v1.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Rook Rook is an open source cloud-native storage orchestrator, providing the platform, framework, and support for a diverse set of storage solutions to natively integrate with cloud-native environments. Rook turns storage software into self-managing, self-scaling, and self-healing storage services. It does this by automating deployment, bootstrapping, configuration, provisioning, scaling, upgrading, migration, disaster recovery, monitoring, and resource management. Rook uses the facilities provided by the underlying cloud-native container management, scheduling and orchestration platform to perform its duties. Rook integrates deeply into cloud native environments leveraging extension points and providing a seamless experience for scheduling, lifecycle management, resource management, security, monitoring, and user experience. For more details about the status of storage solutions currently supported by Rook, please refer to the project status section of the Rook repository. We plan to continue adding support for other storage systems and environments based on community demand and engagement in future releases. ","date":"2019-10-20","objectID":"/posts/k8s/rook-ceph-v1.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:1:0","tags":null,"title":"Rook-Ceph-v1.1-官方文档阅读笔记","uri":"/posts/k8s/rook-ceph-v1.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Quick Start Guides Starting Rook in your cluster is as simple as two kubectl commands. See our Quickstart guide for the details on what you need to get going. Storage Provider Status Description Ceph V1 Ceph is a highly scalable distributed storage solution for block storage, object storage, and shared file systems with years of production deployments. EdgeFS V1 EdgeFS is high-performance and low-latency object storage system with Geo-Transparent data access via standard protocols (S3, NFS, iSCSI) from on-prem, private/public clouds or small footprint edge (IoT) devices. Cassandra Alpha Cassandra is a highly available NoSQL database featuring lightning fast performance, tunable consistency and massive scalability. CockroachDB Alpha CockroachDB is a cloud-native SQL database for building global, scalable cloud services that survive disasters. Minio Alpha Minio is a high performance distributed object storage server, designed for large-scale private cloud infrastructure. NFS Alpha NFS allows remote hosts to mount file systems over a network and interact with those file systems as though they are mounted locally. YugabyteDB Alpha YugaByteDB is a high-performance, cloud-native distributed SQL database which can tolerate disk, node, zone and region failures automatically. ","date":"2019-10-20","objectID":"/posts/k8s/rook-ceph-v1.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:2:0","tags":null,"title":"Rook-Ceph-v1.1-官方文档阅读笔记","uri":"/posts/k8s/rook-ceph-v1.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Ceph This guide will walk you through the basic setup of a Ceph cluster and enable you to consume block, object, and file storage from other pods running in your cluster. Prerequisites Rook can be installed on any existing Kubernetes clusters as long as it meets the minimum version and have the required privilege to run in the cluster (see below for more information). If you dont have a Kubernetes cluster, you can quickly set one up using Minikube, Kubeadm or CoreOS/Vagrant. If you are using dataDirHostPath to persist rook data on kubernetes hosts, make sure your host has at least 5GB of space available on the specified path. Minimum Version Kubernetes v1.10 or higher is supported by Rook. Privileges and RBAC Rook requires privileges to manage the storage in your cluster. See the details here or below for setting up Rook in a Kubernetes cluster with Pod Security Policies enabled. Using Rook with Pod Security Policies Cluster Role NOTE Cluster role configuration is only needed when you are not already cluster-admin in your Kubernetes cluster! Creating the Rook operator requires privileges for setting up RBAC. To launch the operator you need to have created your user certificate that is bound to ClusterRole cluster-admin. One simple way to achieve it is to assign your certificate with the system:masters group: -subj \"/CN=admin/O=system:masters\" system:masters is a special group that is bound to cluster-admin ClusterRole, but it can’t be easily revoked so be careful with taking that route in a production setting. Binding individual certificate to ClusterRole cluster-admin is revocable by deleting the ClusterRoleBinding. RBAC for PodSecurityPolicies If you have activated the PodSecurityPolicy Admission Controller and thus are using PodSecurityPolicies, you will require additional (Cluster)RoleBindings for the different ServiceAccounts Rook uses to start the Rook Storage Pods. Security policies will differ for different backends. See Ceph’s Pod Security Policies set up in its common.yaml for an example of how this is done in practice. Note: You do not have to perform these steps if you do not have the PodSecurityPolicy Admission Controller activated! PodSecurityPolicy You need at least one PodSecurityPolicy that allows privileged Pod execution. Here is an example which should be more permissive than is needed for any backend: 这个 PodSecurityPolicy 在 common.yaml 也有。 apiVersion:policy/v1beta1kind:PodSecurityPolicymetadata:name:privilegedspec:fsGroup:rule:RunAsAnyprivileged:truerunAsUser:rule:RunAsAnyseLinux:rule:RunAsAnysupplementalGroups:rule:RunAsAnyvolumes:- '*'allowedCapabilities:- '*'hostPID:true# hostNetwork is required for using host networkinghostNetwork:false Hint: Allowing hostNetwork usage is required when using hostNetwork: true in a Cluster CustomResourceDefinition! You are then also required to allow the usage of hostPorts in the PodSecurityPolicy. The given port range will allow all ports: hostPorts:# Ceph msgr2 port- min:1max:65535 Flexvolume Configuration Rook uses FlexVolume to integrate with Kubernetes for performing storage operations. In some operating systems where Kubernetes is deployed, the default Flexvolume plugin directory (the directory where FlexVolume drivers are installed) is read-only. This is the case for Kubernetes deployments on: Atomic ContainerLinux (previously named CoreOS) OpenShift Rancher Google Kubernetes Engine (GKE) Azure AKS Especially in these environments, the kubelet needs to be told to use a different FlexVolume plugin directory that is accessible and read/write (rw). These steps need to be carried out on all nodes in your cluster. Please refer to the section that is applicable to your environment/platform, it contains more information on FlexVolume on your platform. Not a listed platform If you are not using a platform that is listed above and the path /usr/libexec/kubernetes/kubelet-plugins/volume/exec/ is read/write, you don’t need to configure anything. That is because /usr/libexec/","date":"2019-10-20","objectID":"/posts/k8s/rook-ceph-v1.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:2:1","tags":null,"title":"Rook-Ceph-v1.1-官方文档阅读笔记","uri":"/posts/k8s/rook-ceph-v1.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":" 在k8s-1安装nfs server nfs客户端和服务端都安装nfs-utils包，安装时会同时安装rpcbind。安装后会创建nfsnobody用户和组，uid和gid都是65534。 yum install -y nfs-utils 配置端口 nfs 除了主程序端口 2049 和 rpcbind 的端口 111 是固定的，还会使用一些随机端口，以下配置将定义这些端口，以便配置防火墙。 vim /etc/sysconfig/nfs 搜索PORT，取消注释 结果如下： [root@k8s-1 nfs-client]# cat /etc/sysconfig/nfs # # Note: For new values to take effect the nfs-config service # has to be restarted with the following command: # systemctl restart nfs-config # # Optional arguments passed to in-kernel lockd #LOCKDARG= # TCP port rpc.lockd should listen on. LOCKD_TCPPORT=32803 # UDP port rpc.lockd should listen on. LOCKD_UDPPORT=32769 # # Optional arguments passed to rpc.nfsd. See rpc.nfsd(8) RPCNFSDARGS=\"\" # Number of nfs server processes to be started. # The default is 8. #RPCNFSDCOUNT=16 # # Set V4 grace period in seconds #NFSD_V4_GRACE=90 # # Set V4 lease period in seconds #NFSD_V4_LEASE=90 # # Optional arguments passed to rpc.mountd. See rpc.mountd(8) RPCMOUNTDOPTS=\"\" # Port rpc.mountd should listen on. MOUNTD_PORT=892 # # Optional arguments passed to rpc.statd. See rpc.statd(8) STATDARG=\"\" # Port rpc.statd should listen on. STATD_PORT=662 # Outgoing port statd should used. The default is port # is random STATD_OUTGOING_PORT=2020 # Specify callout program #STATD_HA_CALLOUT=\"/usr/local/bin/foo\" # # # Optional arguments passed to sm-notify. See sm-notify(8) SMNOTIFYARGS=\"\" # # Optional arguments passed to rpc.idmapd. See rpc.idmapd(8) RPCIDMAPDARGS=\"\" # # Optional arguments passed to rpc.gssd. See rpc.gssd(8) # Note: The rpc-gssd service will not start unless the # file /etc/krb5.keytab exists. If an alternate # keytab is needed, that separate keytab file # location may be defined in the rpc-gssd.service's # systemd unit file under the ConditionPathExists # parameter RPCGSSDARGS=\"\" # # Enable usage of gssproxy. See gssproxy-mech(8). GSS_USE_PROXY=\"yes\" # # Optional arguments passed to blkmapd. See blkmapd(8) BLKMAPDARGS=\"\" 添加防火墙端口（k8s集群防火墙已关闭，故不需执行） firewall-cmd –zone=public –add-port=111/tcp –add-port=111/udp –add-port=2049/tcp –add-port=2049/udp –add-port=662/tcp –add-port=662/udp –add-port=892/tcp –add-port=892/udp –add-port=32803/tcp –add-port=32769/udp –add-port=2020/tcp –add-port=2020/udp –permanent 重载防火墙规则 firewall-cmd –reload 移除防火墙端口 firewall-cmd –zone=public –remove-port=111/tcp –remove-port=111/udp –remove-port=2049/tcp –remove-port=2049/udp –remove-port=662/tcp –remove-port=662/udp –remove-port=892/tcp –remove-port=892/udp –remove-port=32803/tcp –remove-port=32769/udp –remove-port=2020/tcp –remove-port=2020/udp –permanent 配置共享文件夹 # cat /etc/exports /home/nfs/data 192.168.16.0/24(rw,sync,no_root_squash) exports文件配置格式: NFS共享的目录 NFS客户端地址1(参数1,参数2,…) 客户端地址2(参数1,参数2,…) NFS客户端地址： 指定IP: 192.168.0.1 指定子网所有主机: 192.168.0.0/24 指定域名的主机: test.com 指定域名所有主机: *.test.com 所有主机: * 参数说明: ro：共享目录只读 rw：共享目录可读可写 all_squash：所有访问用户都映射为匿名用户或用户组 no_all_squash（默认）：访问用户先与本机用户匹配，匹配失败后再映射为匿名用户或用户组 root_squash（默认）：将来访的root用户映射为匿名用户或用户组 no_root_squash：来访的root用户保持root帐号权限 anonuid=\u003cUID\u003e：指定匿名访问用户的本地用户UID，默认为nfsnobody（65534） anongid=\u003cGID\u003e：指定匿名访问用户的本地用户组GID，默认为nfsnobody（65534） secure（默认）：限制客户端只能从小于1024的tcp/ip端口连接服务器 insecure：允许客户端从大于1024的tcp/ip端口连接服务器 sync：将数据同步写入内存缓冲区与磁盘中，效率低，但可以保证数据的一致性 async：将数据先保存在内存缓冲区中，必要时才写入磁盘 wdelay（默认）：检查是否有相关的写操作，如果有则将这些写操作一起执行，这样可以提高效率 no_wdelay：若有写操作则立即执行，应与sync配合使用 subtree_check（默认） ：若输出目录是一个子目录，则nfs服务器将检查其父目录的权限 no_subtree_check ：即使输出目录是一个子目录，nfs服务器也不检查其父目录的权限，这样可以提高效率 注意：使用nfs作为k8s storageclass provision是，参数必须指定为no_root_squash 启动nfs server并配置开机启动 systemctl enable --now rpcbind systemctl enable --now nfs 客户端挂载测试 在k8s-2上执行挂载命令 [root@k8s-2 home]# mkdir test [root@k8s-2 home]# mount -t nfs 192.168.16.131:/home/nfs/data /home/test 进入/home/test目录，新建文件，然后查看k8s-1的/home/nfs/data目录下是否存在该文件，若存在，则nfs server搭建成功。 客户端配置开机自动挂载 编辑 /etc/fstab，添加 192.168.16.131:/home/nfs/data /home/test nfs default 0 0 其他命令： 查看nfs服务器共享目录showmount -e：显示指定的NFS服务器上所有输出的共享目录。 -a：显示指定的NFS服务器的所有客户端主机及其所连接的目录。 -d：显示指定的NFS服务器中已被客户端连接的所有输出目录 管理当前NFS共享的文件系统列表 exportfs -a：打开或取消所有目录共享。 -o：options,…","date":"2019-10-19","objectID":"/posts/linux/centos7%E6%90%AD%E5%BB%BAnfs-server-20191019/:0:0","tags":["linux","centos7"],"title":"Centos7搭建NFS-Server-20191019","uri":"/posts/linux/centos7%E6%90%AD%E5%BB%BAnfs-server-20191019/"},{"categories":null,"content":"StorageClass 参考： https://kubernetes.io/docs/concepts/storage/storage-classes/ https://github.com/kubernetes-incubator/external-storage ","date":"2019-10-19","objectID":"/posts/k8s/nfs-storageclsss-20191019-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:1:0","tags":null,"title":"Nfs-Storageclsss-20191019-官方文档阅读笔记","uri":"/posts/k8s/nfs-storageclsss-20191019-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Introduction A StorageClass provides a way for administrators to describe the “classes” of storage they offer. Different classes might map to quality-of-service(服务质量) levels, or to backup policies(备份策略), or to arbitrary(任意的) policies determined by the cluster administrators. Kubernetes itself is unopinionated(不关注，没有意见) about what classes represent. This concept is sometimes called “profiles” in other storage systems. ","date":"2019-10-19","objectID":"/posts/k8s/nfs-storageclsss-20191019-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:1:1","tags":null,"title":"Nfs-Storageclsss-20191019-官方文档阅读笔记","uri":"/posts/k8s/nfs-storageclsss-20191019-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"The StorageClass Resource Each StorageClass contains the fields provisioner(供应者), parameters, and reclaimPolicy, which are used when a PersistentVolume belonging to the class needs to be dynamically(动态地) provisioned. The name of a StorageClass object is significant(重要), and is how users can request a particular class. Administrators set the name and other parameters of a class when first creating StorageClass objects, and the objects cannot be updated once they are created. Administrators can specify a default StorageClass just for PVCs that don’t request any particular class to bind to: see the PersistentVolumeClaim section for details. apiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:standardprovisioner:kubernetes.io/aws-ebsparameters:type:gp2reclaimPolicy:RetainallowVolumeExpansion:truemountOptions:- debugvolumeBindingMode:Immediate Provisioner Storage classes have a provisioner that determines what volume plugin is used for provisioning PVs. This field must be specified. Volume Plugin Internal Provisioner Config Example AWSElasticBlockStore ✓ AWS EBS AzureFile ✓ Azure File AzureDisk ✓ Azure Disk CephFS - - Cinder ✓ OpenStack Cinder FC - - FlexVolume - - Flocker ✓ - GCEPersistentDisk ✓ GCE PD Glusterfs ✓ Glusterfs iSCSI - - Quobyte ✓ Quobyte NFS - - RBD ✓ Ceph RBD VsphereVolume ✓ vSphere PortworxVolume ✓ Portworx Volume ScaleIO ✓ ScaleIO StorageOS ✓ StorageOS Local - Local You are not restricted to specifying the “internal” provisioners listed here (whose names are prefixed with “kubernetes.io” and shipped alongside Kubernetes). You can also run and specify external provisioners, which are independent programs that follow a specification defined by Kubernetes. Authors of external provisioners have full discretion over where their code lives, how the provisioner is shipped, how it needs to be run, what volume plugin it uses (including Flex), etc. The repository kubernetes-sigs/sig-storage-lib-external-provisioner houses a library for writing external provisioners that implements the bulk of the specification. Some external provisioners are listed under the repository kubernetes-incubator/external-storage. For example, NFS doesn’t provide an internal provisioner, but an external provisioner can be used. There are also cases when 3rd party storage vendors provide their own external provisioner. Reclaim Policy(回收策略) Persistent Volumes that are dynamically created by a storage class will have the reclaim policy specified in the reclaimPolicy field of the class, which can be either Delete or Retain. If no reclaimPolicy is specified when a StorageClass object is created, it will default to Delete. Persistent Volumes that are created manually and managed via a storage class will have whatever reclaim policy they were assigned at creation. Allow Volume Expansion(允许卷扩容) FEATURE STATE: Kubernetes v1.11 beta Persistent Volumes can be configured to be expandable. This feature when set to true, allows the users to resize the volume by editing the corresponding PVC object. The following types of volumes support volume expansion, when the underlying Storage Class has the field allowVolumeExpansion set to true. Volume type Required Kubernetes version gcePersistentDisk 1.11 awsElasticBlockStore 1.11 Cinder 1.11 glusterfs 1.11 rbd 1.11 Azure File 1.11 Azure Disk 1.11 Portworx 1.11 FlexVolume 1.13 CSI 1.14 (alpha), 1.16 (beta) Note: You can only use the volume expansion feature to grow a Volume, not to shrink(收缩) it. Mount Options Persistent Volumes that are dynamically created by a storage class will have the mount options specified in the mountOptions field of the class. If the volume plugin does not support mount options but mount options are specified, provisioning will fail. Mount options are not validated on either the class or PV, so mount of the PV will simply fail if one is invalid. Volume Binding Mode The volumeBindingMode field controls when volume binding and dynamic provisioning should occur. By default, the Immediate mo","date":"2019-10-19","objectID":"/posts/k8s/nfs-storageclsss-20191019-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:1:2","tags":null,"title":"Nfs-Storageclsss-20191019-官方文档阅读笔记","uri":"/posts/k8s/nfs-storageclsss-20191019-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Parameters Storage classes have parameters that describe volumes belonging to the storage class. Different parameters may be accepted depending on the provisioner. For example, the value io1, for the parameter type, and the parameter iopsPerGB are specific to EBS. When a parameter is omitted, some default is used. AWS EBS apiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:slowprovisioner:kubernetes.io/aws-ebsparameters:type:io1iopsPerGB:\"10\"fsType:ext4 type: io1, gp2, sc1, st1. See AWS docs for details. Default: gp2. zone (Deprecated): AWS zone. If neither zone nor zones is specified, volumes are generally round-robin-ed across all active zones where Kubernetes cluster has a node. zone and zones parameters must not be used at the same time. zones (Deprecated): A comma separated list of AWS zone(s). If neither zone nor zones is specified, volumes are generally round-robin-ed across all active zones where Kubernetes cluster has a node. zone and zones parameters must not be used at the same time. iopsPerGB: only for io1 volumes. I/O operations per second per GiB. AWS volume plugin multiplies this with size of requested volume to compute IOPS of the volume and caps it at 20 000 IOPS (maximum supported by AWS, see AWS docs. A string is expected here, i.e. \"10\", not 10. fsType: fsType that is supported by kubernetes. Default: “ext4”. encrypted: denotes whether the EBS volume should be encrypted or not. Valid values are \"true\" or \"false\". A string is expected here, i.e. \"true\", not true. kmsKeyId: optional. The full Amazon Resource Name of the key to use when encrypting the volume. If none is supplied but encrypted is true, a key is generated by AWS. See AWS docs for valid ARN value. Note: zone and zones parameters are deprecated and replaced with allowedTopologies GCE PD apiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:slowprovisioner:kubernetes.io/gce-pdparameters:type:pd-standardreplication-type:none type: pd-standard or pd-ssd. Default: pd-standard zone (Deprecated): GCE zone. If neither zone nor zones is specified, volumes are generally round-robin-ed across all active zones where Kubernetes cluster has a node. zone and zones parameters must not be used at the same time. zones (Deprecated): A comma separated list of GCE zone(s). If neither zone nor zones is specified, volumes are generally round-robin-ed across all active zones where Kubernetes cluster has a node. zone and zones parameters must not be used at the same time. replication-type: none or regional-pd. Default: none. If replication-type is set to none, a regular (zonal) PD will be provisioned. If replication-type is set to regional-pd, a Regional Persistent Disk will be provisioned. In this case, users must use zones instead of zone to specify the desired replication zones. If exactly two zones are specified, the Regional PD will be provisioned in those zones. If more than two zones are specified, Kubernetes will arbitrarily choose among the specified zones. If the zones parameter is omitted, Kubernetes will arbitrarily choose among zones managed by the cluster. Note: zone and zones parameters are deprecated and replaced with allowedTopologies Glusterfs apiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:slowprovisioner:kubernetes.io/glusterfsparameters:resturl:\"http://127.0.0.1:8081\"clusterid:\"630372ccdc720a92c681fb928f27b53f\"restauthenabled:\"true\"restuser:\"admin\"secretNamespace:\"default\"secretName:\"heketi-secret\"gidMin:\"40000\"gidMax:\"50000\"volumetype:\"replicate:3\" resturl: Gluster REST service/Heketi service url which provision gluster volumes on demand. The general format should be IPaddress:Port and this is a mandatory(强制性的) parameter for GlusterFS dynamic provisioner. If Heketi service is exposed as a routable service in openshift/kubernetes setup, this can have a format similar to http://heketi-storage-project.cloudapps.mystorage.com where the fqdn is a resolvable Heketi service url. restauthenabled : Gluster REST service authentication b","date":"2019-10-19","objectID":"/posts/k8s/nfs-storageclsss-20191019-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:1:3","tags":null,"title":"Nfs-Storageclsss-20191019-官方文档阅读笔记","uri":"/posts/k8s/nfs-storageclsss-20191019-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"nfs-storageclass ","date":"2019-10-19","objectID":"/posts/k8s/nfs-storageclsss-20191019-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:2:0","tags":null,"title":"Nfs-Storageclsss-20191019-官方文档阅读笔记","uri":"/posts/k8s/nfs-storageclsss-20191019-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"部署 NFS server 略 ","date":"2019-10-19","objectID":"/posts/k8s/nfs-storageclsss-20191019-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:2:1","tags":null,"title":"Nfs-Storageclsss-20191019-官方文档阅读笔记","uri":"/posts/k8s/nfs-storageclsss-20191019-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"部署 NFS client 参考：https://github.com/kubernetes-incubator/external-storage/tree/master/nfs-client nfs-client is an automatic provisioner that use your existing and already configured NFS server to support dynamic provisioning of Kubernetes Persistent Volumes via Persistent Volume Claims. Persistent volumes are provisioned as ${namespace}-${pvcName}-${pvName}. To note again, you must already have an NFS Server. With Helm Follow the instructions for the stable helm chart maintained at https://github.com/helm/charts/tree/master/stable/nfs-client-provisioner The tl;dr is $ helm install stable/nfs-client-provisioner --set nfs.server=x.x.x.x --set nfs.path=/exported/path Without Helm Step 1: Get connection information for your NFS server. Make sure your NFS server is accessible from your Kubernetes cluster and get the information you need to connect to it. At a minimum you will need its hostname. Step 2: Get the NFS-Client Provisioner files. To setup the provisioner you will download a set of YAML files, edit them to add your NFS server’s connection information and then apply each with the kubectl / oc command. Get all of the files in the deploy directory of this repository. These instructions assume that you have cloned the external-storage repository and have a bash-shell open in the nfs-client directory. Step 3: Setup authorization. If your cluster has RBAC enabled or you are running OpenShift you must authorize the provisioner. If you are in a namespace/project other than “default” edit deploy/rbac.yaml. Kubernetes: # Set the subject of the RBAC objects to the current namespace where the provisioner is being deployed $ NS=$(kubectl config get-contexts|grep -e \"^\\*\" |awk '{print $5}') $ NAMESPACE=${NS:-default} $ sed -i'' \"s/namespace:.*/namespace: $NAMESPACE/g\" ./deploy/rbac.yaml $ kubectl create -f deploy/rbac.yaml OpenShift: On some installations of OpenShift the default admin user does not have cluster-admin permissions. If these commands fail refer to the OpenShift documentation for User and Role Management or contact your OpenShift provider to help you grant the right permissions to your admin user. # Set the subject of the RBAC objects to the current namespace where the provisioner is being deployed $ NAMESPACE=`oc project -q` $ sed -i'' \"s/namespace:.*/namespace: $NAMESPACE/g\" ./deploy/rbac.yaml $ oc create -f deploy/rbac.yaml $ oadm policy add-scc-to-user hostmount-anyuid system:serviceaccount:$NAMESPACE:nfs-client-provisioner Step 4: Configure the NFS-Client provisioner Note: To deploy to an ARM-based environment, use: deploy/deployment-arm.yaml instead, otherwise use deploy/deployment.yaml. Next you must edit the provisioner’s deployment file to add connection information for your NFS server. Edit deploy/deployment.yaml and replace the two occurences of \u003cYOUR NFS SERVER HOSTNAME\u003e with your server’s hostname. kind:DeploymentapiVersion:apps/v1metadata:name:nfs-client-provisionerspec:replicas:1selector:matchLabels:app:nfs-client-provisionerstrategy:type:Recreatetemplate:metadata:labels:app:nfs-client-provisionerspec:serviceAccountName:nfs-client-provisionercontainers:- name:nfs-client-provisionerimage:quay.io/external_storage/nfs-client-provisioner:latestvolumeMounts:- name:nfs-client-rootmountPath:/persistentvolumesenv:- name:PROVISIONER_NAMEvalue:fuseim.pri/ifs- name:NFS_SERVERvalue:\u003cYOUR NFS SERVER HOSTNAME\u003e- name:NFS_PATHvalue:/var/nfsvolumes:- name:nfs-client-rootnfs:server:\u003cYOUR NFS SERVER HOSTNAME\u003epath:/var/nfs You may also want to change the PROVISIONER_NAME above from fuseim.pri/ifs to something more descriptive like nfs-storage, but if you do remember to also change the PROVISIONER_NAME in the storage class definition below: This is deploy/class.yaml which defines the NFS-Client’s Kubernetes Storage Class: apiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:managed-nfs-storageprovisioner:fuseim.pri/ifs# or choose another name, must match deployment's env PROVISIONER_NAME'parameters:archiveOnDelete:\"false\"#","date":"2019-10-19","objectID":"/posts/k8s/nfs-storageclsss-20191019-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:2:2","tags":null,"title":"Nfs-Storageclsss-20191019-官方文档阅读笔记","uri":"/posts/k8s/nfs-storageclsss-20191019-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":" 参考：https://cert-manager.readthedocs.io/en/latest/index.html cert-manager is a native Kubernetes certificate management controller. It can help with issuing certificates from a variety of sources, such as Let’s Encrypt, HashiCorp Vault, Venafi, a simple signing keypair, or self signed. It will ensure certificates are valid and up to date, and attempt to renew certificates at a configured time before expiry. It is loosely based upon the work of kube-lego and has borrowed some wisdom from other similar projects e.g. kube-cert-manager. This is the full technical documentation(技术文档) for the project, and should be used as a source of references when seeking help with the project. ","date":"2019-10-18","objectID":"/posts/k8s/cert-manager-v0.11-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:0:0","tags":null,"title":"Cert Manager v0.11 官方文档阅读笔记","uri":"/posts/k8s/cert-manager-v0.11-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Get started The guides in this section will explain how to install, set up, and uninstall cert-manager. ","date":"2019-10-18","objectID":"/posts/k8s/cert-manager-v0.11-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:1:0","tags":null,"title":"Cert Manager v0.11 官方文档阅读笔记","uri":"/posts/k8s/cert-manager-v0.11-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Install cert-manager cert-manager supports running on Kubernetes and OpenShift. The installation mechanism(机制) between the two platforms is similar, although there are a number of extra notes to be aware of per-platform. Installing on Kubernetes cert-manager runs within your Kubernetes cluster as a series of deployment resources. It utilises(利用) CustomResourceDefinitions to configure Certificate Authorities and request certificates. It is deployed using regular YAML manifests, like any other applications on Kubernetes. Once cert-manager has been deployed, you must configure Issuer or ClusterIssuer resources which represent certificate authorities. More information on configuring different Issuer types can be found in the respective setup guides. Note: From cert-manager v0.11.0 onwards, the minimum supported version of Kubernetes is v1.11.0. Users still running Kubernetes v1.10 or below should upgrade to a supported version before installing cert-manager. warning: You should not install multiple instances of cert-manager on a single cluster. This will lead to undefined behaviour and you may be banned from providers such as Let’s Encrypt. Installing with regular manifests In order to install cert-manager, we must first create a namespace to run it within. This guide will install cert-manager into the cert-manager namespace. It is possible to run cert-manager in a different namespace, although you will need to make modifications to the deployment manifests. # Create a namespace to run cert-manager in kubectl create namespace cert-manager As part of the installation, cert-manager also deploys a webhook deployment as an APIService. This can cause issues when uninstalling cert-manager if the API service still exists but the webhook is no longer running as the API server is unable to reach the validating webhook. Ensure to follow the documentation when uninstalling cert-manager. The webhook enables cert-manager to implement validation(验证) and mutating(更改) webhooks on cert-manager resources. A ValidatingWebhookConfiguration resource is deployed to validate cert-manager resources we will create after installation. No mutating webhooks are currently implemented. You can read more about the webhook on the webhook document. We can now go ahead and install cert-manager. All resources (the CustomResourceDefinitions, cert-manager, and the webhook component) are included in a single YAML manifest file: # Install the CustomResourceDefinitions and cert-manager itself kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v0.11.0/cert-manager.yaml Note: If you are running Kubernetes v1.15 or below, you will need to add the --validate=false flag to your kubectl apply command above else you will receive a validation error relating to the x-kubernetes-preserve-unknown-fields field in our CustomResourceDefinition resources. This is a benign error and occurs due to the way kubectl performs resource validation. Note: When running on GKE (Google Kubernetes Engine), you may encounter a permission denied' error when creating some of these resources. This is a nuance(细微差别) of the way GKE handles RBAC and IAM permissions, and as such you should ‘elevate’(提升) your own privileges to that of a ‘cluster-admin’ before running the above command. If you have already run the above command, you should run them again after elevating your permissions:: kubectl create clusterrolebinding cluster-admin-binding \\ --clusterrole=cluster-admin \\ --user=$(gcloud config get-value core/account) Installing with Helm As an alternative to the YAML manifests referenced above, we also provide an official Helm chart for installing cert-manager. Pre-requisites Helm and Tiller installed (or alternatively, use Tillerless Helm v2) cluster-admin privileges bound to the Tiller pod Foreword Before deploying cert-manager with Helm, you must ensure Tiller is up and running in your cluster. Tiller is the server side component to Helm. Your cluster administrator may have a","date":"2019-10-18","objectID":"/posts/k8s/cert-manager-v0.11-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:1:1","tags":null,"title":"Cert Manager v0.11 官方文档阅读笔记","uri":"/posts/k8s/cert-manager-v0.11-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Webhook component In order to provide advanced resource validation, cert-manager includes a ValidatingWebhookConfiguration resource which is deployed into the cluster. This allows cert-manager to validate that cert-manager API resources that are submitted to the apiserver are syntactically valid, and catch issues with your resources early on. If you disable the webhook component, cert-manager will still perform the same resource validation however it will not reject 'create' events when the resources are submitted to the apiserver if they are invalid. This means it may be possible for a user to submit a resource that renders(使) the controller inoperable(无法操作). For this reason, it is strongly advised to keep the webhook enabled. Note: This feature requires Kubernetes v1.9 or greater. How it works This sections walks through how the resource validation webhook is configured and explains the process required for it to provision. The webhook is a ValidatingWebhookConfiguration resource combined with an extra pod that is deployed alongside the cert-manager-controller. The ValidatingWebhookConfiguration instructs the Kubernetes apiserver to POST the contents of any Create or Update operations performed(执行的操作) on cert-manager resource types in order to validate that they are setting valid configurations. This allows us to ensure mis-configurations are caught early on and communicated to you. In order for this to work, the webhook requires a TLS certificate that the apiserver is configured to trust. This is created by the webhook itself and is implemented by the following two Secrets: secret/cert-manager-webhook-ca - A self-signed root CA certificate which is used to sign certificates for the webhook pod. secret/cert-manager-webhook-tls - A TLS certificate issued by the root CA above, served by the webhook. The webhook’s 'webhookbootstrap' controller is responsible for creating these secrets with no manual intervention(介入) needed. If errors occur around the webhook but the webhook is running then the webhook is most likely not reachable from the API server. In this case, ensure that the API server can communicate with the webhook by following the GKE private cluster explanation below. cainjector The cert-manager CA injector is responsible for injecting the two CA bundles above into the webhook’s ValidatingWebhookConfiguration and APIService resource in order to allow the Kubernetes apiserver to ‘trust’ the webhook apiserver. This component is configured using the cert-manager.io/inject-apiserver-ca: \"true\" and cert-manager.io/inject-apiserver-ca: \"true\" annotations on the APIService and ValidatingWebhookConfiguration resources. It copies across the CA defined in the ‘cert-manager-webhook-ca’ Secret generated above to the caBundle field on the APIService resource. It also sets the webhook’s clientConfig.caBundle field on the cert-manager-webhook ValidatingWebhookConfiguration resource to that of your Kubernetes API server in order to support Kubernetes versions earlier than v1.11. Known issues This section contains known issues with the webhook component. If you’re having problems, or receiving errors when creating cert-manager resources, please read through this section for help. Running on private GKE clusters When Google configure the control plane for private clusters, they automatically configure VPC peering between your Kubernetes cluster’s network and a separate Google managed project. In order to restrict what Google are able to access within your cluster, the firewall rules configured restrict access to your Kubernetes pods. This will mean that you will experience the webhook to not work and expierence errors such as Internal error occurred: failed calling admission webhook ... the server is currently unable to handle the request. In order to use the webhook component with a GKE private cluster, you must configure an additional firewall rule to allow the GKE control plane access to your webhook pod. You can read more informati","date":"2019-10-18","objectID":"/posts/k8s/cert-manager-v0.11-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:1:2","tags":null,"title":"Cert Manager v0.11 官方文档阅读笔记","uri":"/posts/k8s/cert-manager-v0.11-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Tutorials This section contains guides that help you get started using cert-manager for more specific use cases. For more information on performing individual tasks, read the tasks section. ","date":"2019-10-18","objectID":"/posts/k8s/cert-manager-v0.11-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:2:0","tags":null,"title":"Cert Manager v0.11 官方文档阅读笔记","uri":"/posts/k8s/cert-manager-v0.11-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"ACME Issuer Tutorials This sections contains tutorials relating to the ACME issuer. Quick-Start using Cert-Manager with NGINX Ingress Step 0 - Install Helm Client Skip this section if you have helm installed. The easiest way to install cert-manager is to use Helm, a templating and deployment tool for Kubernetes resources. First, ensure the Helm client is installed following the Helm installation instructions. For example, on macOS: brew install kubernetes-helm Step 1 - Installer Tiller Skip this section if you have Tiller set-up. Tiller is Helm’s server-side component, which the helm client uses to deploy resources. Deploying resources is a privileged operation; in the general case requiring arbitrary(任意的) privileges(特权). With this example, we give Tiller complete control of the cluster. View the documentation on securing helm for details on setting up appropriate permissions for your environment. Create the a ServiceAccount for tiller: $ kubectl create serviceaccount tiller --namespace=kube-system serviceaccount \"tiller\" created Grant the tiller service account cluster-admin privileges: $ kubectl create clusterrolebinding tiller-admin --serviceaccount=kube-system:tiller --clusterrole=cluster-admin clusterrolebinding.rbac.authorization.k8s.io \"tiller-admin\" created Install tiller with the tiller service account: $ helm init --service-account=tiller $HELM_HOME has been configured at /Users/myaccount/.helm. Tiller (the Helm server-side component) has been installed into your Kubernetes Cluster. Please note: by default, Tiller is deployed with an insecure 'allow unauthenticated users' policy. To prevent this, run `helm init` with the --tiller-tls-verify flag. For more information on securing your installation see: https://docs.helm.sh/using_helm/#securing-your-helm-installation Happy Helming! Update the helm repository with the latest charts: $ helm repo update Hang tight while we grab the latest from your chart repositories... ...Skip local chart repository ...Successfully got an update from the \"stable\" chart repository ...Successfully got an update from the \"coreos\" chart repository Update Complete. ⎈ Happy Helming!⎈ Step 2 - Deploy the NGINX Ingress Controller A kubernetes ingress controller is designed to be the access point for HTTP and HTTPS traffic to the software running within your cluster. The nginx-ingress controller does this by providing an HTTP proxy service supported by your cloud provider’s load balancer. You can get more details about nginx-ingress and how it works from the documentation for nginx-ingress. Use helm to install an Nginx Ingress controller: $ helm install stable/nginx-ingress --name quickstart NAME: quickstart LAST DEPLOYED: Sat Nov 10 10:25:06 2018 NAMESPACE: default STATUS: DEPLOYED RESOURCES: ==\u003e v1/ConfigMap NAME AGE quickstart-nginx-ingress-controller 0s ==\u003e v1beta1/ClusterRole quickstart-nginx-ingress 0s ==\u003e v1beta1/Deployment quickstart-nginx-ingress-controller 0s quickstart-nginx-ingress-default-backend 0s ==\u003e v1/Pod(related) NAME READY STATUS RESTARTS AGE quickstart-nginx-ingress-controller-6cfc45747-wcxrg 0/1 ContainerCreating 0 0s quickstart-nginx-ingress-default-backend-bf9db5c67-dkg4l 0/1 ContainerCreating 0 0s ==\u003e v1/ServiceAccount NAME AGE quickstart-nginx-ingress 0s ==\u003e v1beta1/ClusterRoleBinding quickstart-nginx-ingress 0s ==\u003e v1beta1/Role quickstart-nginx-ingress 0s ==\u003e v1beta1/RoleBinding quickstart-nginx-ingress 0s ==\u003e v1/Service quickstart-nginx-ingress-controller 0s quickstart-nginx-ingress-default-backend 0s NOTES: The nginx-ingress controller has been installed. It may take a few minutes for the LoadBalancer IP to be available. You can watch the status by running 'kubectl --namespace default get services -o wide -w quickstart-nginx-ingress-controller' An example Ingress that makes use of the controller: apiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx name: example namespace: foo spec: rules: - host: www.example.com http:","date":"2019-10-18","objectID":"/posts/k8s/cert-manager-v0.11-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:2:1","tags":null,"title":"Cert Manager v0.11 官方文档阅读笔记","uri":"/posts/k8s/cert-manager-v0.11-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Securing Ingresses with Venafi This guide walks you through how to secure a Kubernetes Ingress resource using the Venafi Issuer type. Whilst stepping through, you will learn how to: Create an EKS cluster using eksctl Install cert-manager into the EKS cluster Deploy nginx-ingress to expose applications running in the cluster Configure a Venafi Cloud issuer Configure cert-manager to secure your application traffic While this guide focuses on EKS(Amazon Elastic Kubernetes Service) as a Kubernetes provisioner and Venafi as a Certificate issuer, the steps here should be generally re-usable for other Issuer types. Prerequisites An AWS account kubectl installed Access to a publicly registered DNS zone A Venafi Cloud account and API credentials Create an EKS cluster If you already have a running EKS cluster you can skip this step and move onto deploying cert-manager. eksctl is a tool that makes it easier to deploy and manage an EKS cluster. Installation instructions for various platforms can be found in the eksctl installation instructions. Once installed, you can create a basic cluster by running: eksctl create cluster This process may take up to 20 minutes to complete. Complete instructions on using eksctl can be found in the eksctl usage section. Once your cluster has been created, you should verify that your cluster is running correctly by running the following command: kubectl get pods --all-namespaces NAME READY STATUS RESTARTS AGE aws-node-8xpkp 1/1 Running 0 115s aws-node-tflxs 1/1 Running 0 118s coredns-694d9447b-66vlp 1/1 Running 0 23s coredns-694d9447b-w5bg8 1/1 Running 0 23s kube-proxy-4dvpj 1/1 Running 0 115s kube-proxy-tpvht 1/1 Running 0 118s You should see output similar to the above, with all pods in a Running state. Installing cert-manager There are no special requirements to note when installing cert-manager on EKS, so the regular Running on Kubernetes guide can be used to install cert-manager. Please walk through the installation guide and return to this step once you have validated cert-manager is deployed correctly. Installing ingress-nginx A Kubernetes ingress controller is designed to be the access point for HTTP and HTTPS traffic to the software running within your cluster. The ingress-nginx controller does this by providing an HTTP proxy service supported by your cloud provider’s load balancer (in this case, a Network Load Balancer (NLB). You can get more details about nginx-ingress and how it works from the documentation for nginx-ingress. To deploy ingress-nginx using an ELB to expose the service, run the following: # Deploy the AWS specific pre-requisite manifest kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/provider/aws/service-nlb.yaml # Deploy the 'generic' ingress-nginx manifest kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/mandatory.yaml You may have to wait up to 5 minutes for all the required components in your cluster and AWS account to become ready. You can run the following command to determine the address that Amazon has assigned to your NLB: kubectl get service -n ingress-nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ingress-nginx LoadBalancer 10.100.52.175 a8c2870a5a8a311e9a9a10a2e7af57d7-6c2ec8ede48726ab.elb.eu-west-1.amazonaws.com 80:31649/TCP,443:30567/TCP 4m10s The EXTERNAL-IP field may say \u003cpending\u003e for a while. This indicates the NLB is still being created. Retry the command until an EXTERNAL-IP has been provisioned. Once the EXTERNAL-IP is available, you should run the following command to verify that traffic is being correctly routed to ingress-nginx: curl http://a8c2870a5a8a311e9a9a10a2e7af57d7-6c2ec8ede48726ab.elb.eu-west-1.amazonaws.com/ \u003chtml\u003e \u003chead\u003e\u003ctitle\u003e404 Not Found\u003c/title\u003e\u003c/head\u003e \u003cbody\u003e \u003ccenter\u003e\u003ch1\u003e404 Not Found\u003c/h1\u003e\u003c/center\u003e \u003chr\u003e\u003ccenter\u003eopenresty/1.15.8.1\u003c/center\u003e \u003c/body\u003e \u003c/html\u003e Whilst the above message would normally indicate an error (the page not being found), in thi","date":"2019-10-18","objectID":"/posts/k8s/cert-manager-v0.11-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:2:2","tags":null,"title":"Cert Manager v0.11 官方文档阅读笔记","uri":"/posts/k8s/cert-manager-v0.11-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Exposing and securing your application Now that we have issued a Certificate, we can expose our application using a Kubernetes Ingress resource. Create a file named application-ingress.yaml and save the following in it, replacing example.com with your own domain name: apiVersion:extensions/v1beta1kind:Ingressmetadata:name:frontend-ingressnamespace:demoannotations:kubernetes.io/ingress.class:\"nginx\"spec:tls:- hosts:- example.comsecretName:example-com-tlsrules:- host:example.comhttp:paths:- path:/backend:serviceName:hello-kubernetesservicePort:80 You can then apply this resource with: kubectl apply -n demo -f application-ingress.yaml Once this has been created, you should be able to visit your application at the configured hostname, here example.com! Navigate to the address in your web browser and you should see the certificate obtained via Venafi being used to secure application traffic. ","date":"2019-10-18","objectID":"/posts/k8s/cert-manager-v0.11-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:2:3","tags":null,"title":"Cert Manager v0.11 官方文档阅读笔记","uri":"/posts/k8s/cert-manager-v0.11-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Tasks This section contains guides on using specific features of cert-manager, such as configuring different Issuer types and any special settings that you may want to configure. ","date":"2019-10-18","objectID":"/posts/k8s/cert-manager-v0.11-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:3:0","tags":null,"title":"Cert Manager v0.11 官方文档阅读笔记","uri":"/posts/k8s/cert-manager-v0.11-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Setting up Issuers Before you can begin issuing certificates, you must configure at least one Issuer or ClusterIssuer resource in your cluster. These represent a certificate authority from which signed x509 certificates can be obtained, such as Let's Encrypt, or your own signing key pair stored in a Kubernetes Secret resource. They are referenced by Certificate resources in order to request certificates from them. An Issuer is scoped to a single namespace, and can only fulfill(履行，落实) Certificate resources within its own namespace. This is useful in a multi-tenant(多租户) environment where multiple teams or independent parties operate within a single cluster. On the other hand, a ClusterIssuer is a cluster wide version of an Issuer. It is able to be referenced by Certificate resources in any namespace. Users often create letsencrypt-staging and letsencrypt-prod ClusterIssuers if they operate a single-tenant environment and want to expose a cluster-wide mechanism for obtaining TLS certificates from Let’s Encrypt. Supported issuer types cert-manager supports a number of different issuer backends, each with their own different types of configuration. Please follow one of the below linked guides to learn how to set up the issuer types you require: CA - issue certificates signed by a X509 signing keypair, stored in a Secret in the Kubernetes API server. Self signed - issue self signed certificates. ACME - issue certificates obtained by performing challenge validations against an ACME server such as Let’s Encrypt. Vault - issue certificates from a Vault instance configured with the Vault PKI backend. Venafi - issue certificates from a Venafi Cloud or Trust Protection Platform instance. Additional information There are a few key things to know about Issuers, but for full information you can refer to the Issuer reference docs. Difference between Issuers and ClusterIssuers ClusterIssuers are a resource type similar to Issuers. They are specified in exactly the same way, but they do not belong to a single namespace and can be referenced by Certificate resources from multiple different namespaces. They are particularly useful when you want to provide the ability to obtain certificates from a central authority (e.g. Letsencrypt, or your internal CA) and you run single-tenant clusters. The resource spec is identical, and you should set the certificate.spec.issuerRef.kind field to ClusterIssuer when creating your Certificate resources. Setting up ACME Issuers The ACME Issuer type represents a single Account registered with the ACME server. When you create a new ACME Issuer, cert-manager will generate a private key which is used to identify you with the ACME server. To set up a basic ACME issuer, you should create a new Issuer or ClusterIssuer resource. You should read the guides linked at the bottom of this page to learn more about the ACME challenge validation mechanisms that cert-manager supports and how to configure the various DNS01 provider implementations. Creating a basic ACME Issuer The below example configures a ClusterIssuer named letsencrypt-staging that is configured to HTTP01 challenge solving with configuration suitable for ingress controllers such as ingress-nginx. You should copy and paste this example into a new file named letsencrypt-staging.yaml and update the spec.acme.email field to be your own email address. apiVersion:cert-manager.io/v1alpha2kind:ClusterIssuermetadata:name:letsencrypt-stagingspec:acme:# You must replace this email address with your own.# Let's Encrypt will use this to contact you about expiring# certificates, and issues related to your account.email:user@example.comserver:https://acme-staging-v02.api.letsencrypt.org/directoryprivateKeySecretRef:# Secret resource used to store the account's private key.name:example-issuer-account-key# Add a single challenge solver, HTTP01 using nginxsolvers:- http01:ingress:class:nginx You can then create this resource using kubectl apply: kubectl apply -f letsencrypt-stag","date":"2019-10-18","objectID":"/posts/k8s/cert-manager-v0.11-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:3:1","tags":null,"title":"Cert Manager v0.11 官方文档阅读笔记","uri":"/posts/k8s/cert-manager-v0.11-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Issuing Certificates The Certificate resource type is used to request certificates from different Issuers. In order to issue any certificates, you’ll need to configure an Issuer resource first. If you have not configured any issuers yet, you should read the Setting up Issuers guide. Creating Certificate resources A Certificate resource specifies fields that are used to generated certificate signing requests which are then fulfilled by the issuer type you have referenced. Certificates specify which issuer they want to obtain the certificate from by specifying the certificate.spec.issuerRef field. A basic Certificate resource, for the example.com and www.example.com DNS names, spiffe://cluster.local/ns/sandbox/sa/example URI Subject Alternative Name, that is valid for 90d and renews 15d before expiry is below: apiVersion:cert-manager.io/v1alpha2kind:Certificatemetadata:name:example-comnamespace:defaultspec:secretName:example-com-tlsduration:2160h# 90drenewBefore:360h# 15dcommonName:example.comdnsNames:- example.com- www.example.comuriSANs:- spiffe://cluster.local/ns/sandbox/sa/exampleissuerRef:name:ca-issuer# We can reference ClusterIssuers by changing the kind here.# The default value is Issuer (i.e. a locally namespaced Issuer)kind:Issuer The signed certificate will be stored in a Secret resource named example-com-tls once the issuer has successfully issued the requested certificate. The Certificate will be issued using the issuer named ca-issuer in the default namespace (the same namespace as the Certificate resource). note: If you want to create an Issuer that can be referenced by Certificate resources in all namespaces, you should create a ClusterIssuer resource and set the certificate.spec.issuerRef.kind field to ClusterIssuer. note: The renewBefore and duration fields must be specified using Golang’s time.Time string format, which does not allow the d (days) suffix. You must specify these values using s, m and h suffixes instead. Failing to do so without installing the webhook component can prevent cert-manager from functioning correctly (#1269). note: Take care when setting the renewBefore field to be very close to the duration as this can lead to a renewal loop, where the Certificate is always in the renewal period. Some Issuers set the notBefore field on their issued X.509 certificate before the issue time to fix clock-skew issues, leading to the working duration of a certificate to be less than the full duration of the certificate. For example, Let’s Encrypt sets it to be one hour before issue time, so the actual working duration of the certificate is 89 days, 23 hours (the full duration remains 90 days). A full list of the fields supported on the Certificate resource can be found in the API reference documentation. Temporary certificates whilst issuing With some Issuer types, certificates can take a few minutes to be issued. A temporary untrusted certificate will be issued whilst this process takes places if another certificate does not already exist in the target Secret resource. This helps to improve compatibility with certain ingress controllers (e.g. ingress-gce) which require a TLS certificate to be present at all times in order to function. After the real, valid certificate has been obtained, cert-manager will replace the temporary self signed certificate with the valid one, but will retain the same private key You can disable issuing temporary certificate by setting feature gate flag --feature-gates=IssueTemporaryCertificate=false Automatically creating Certificates for Ingress resources cert-manager can be configured to automatically provision TLS certificates for Ingress resources via annotations on your Ingresses. A small sub-component of cert-manager, ingress-shim, is responsible for this. How it works ingress-shim watches Ingress resources across your cluster. If it observes an Ingress with any of the annotations described in the ‘Usage’ section, it will ensure a Certificate resource with the same name as","date":"2019-10-18","objectID":"/posts/k8s/cert-manager-v0.11-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:3:2","tags":null,"title":"Cert Manager v0.11 官方文档阅读笔记","uri":"/posts/k8s/cert-manager-v0.11-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Backing up and restoring If you need to uninstall cert-manager, or transfer your installation to a new cluster, you can backup all of cert-manager’s configuration in order to later re-install. Backing up To backup all of your cert-manager configuration resources, run: kubectl get -o yaml \\--all-namespaces \\issuer,clusterissuer,certificates,orders,challenges,certificaterequests \u003e cert-manager-backup.yaml If you are transferring data to a new cluster, you may also need to copy across additional Secret resources that are referenced by your configured Issuers, such as: CA Issuers The root CA Secret referenced by issuer.spec.ca.secretName Vault Issuers The token authentication Secret referenced by issuer.spec.vault.auth.tokenSecretRef The approle configuration Secret referenced by issuer.spec.vault.auth.appRole.secretRef ACME Issuers The ACME account private key Secret referenced by issuer.acme.privateKeySecretRef Any Secrets referenced by DNS providers configured under the issuer.acme.dns01.providers and issuer.acme.solvers.dns01 fields. Restoring In order to restore your configuration, you can simply kubectl apply the files created above after installing cert-manager. kubectl apply -f cert-manager-backup.yaml If you have migrated from an old cluster, you will need to make sure to run a similar kubectl apply command to restore your Secret resources too. ","date":"2019-10-18","objectID":"/posts/k8s/cert-manager-v0.11-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:3:3","tags":null,"title":"Cert Manager v0.11 官方文档阅读笔记","uri":"/posts/k8s/cert-manager-v0.11-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Uninstalling cert-manager cert-manager supports running on Kubernetes and OpenShift. The uninstallation process between the two platforms is similar, although there are a number of extra notes to be aware of per-platform. Uninstalling on Kubernetes Below is the processes for uninstalling cert-manager on Kubernetes. There are two processes to chose depending on which method you used to install cert-manager - static manifests or helm. warning: To uninstall cert-manger you should always use the same process for installing but in reverse. Deviating from the following process whether cert-manager has been installed from static manifests or helm can cause issues and potentially broken states. Please ensure you follow the below steps when uninstalling to prevent this happening. Before continuing, ensure that all cert-manager resources that have been created by users have been deleted. You can check for any existing resources with the following command: kubectl get Issuers,ClusterIssuers,Certificates,CertificateRequests,Orders,Challenges --all-namespaces Once all these resources have been deleted you are ready to uninstall cert-manager using the procedure determined by how you installed. Uninstalling with regular manifests Uninstalling from an installation with regular manifests is a case of running the installation process, in reverse, using the delete command of kubectl. Delete the installation manifests using a link to your currently running version vX.Y.Z like so: kubectl delete -f https://github.com/jetstack/cert-manager/releases/download/vX.Y.Z/cert-manager.yaml Uninstalling with Helm Uninstalling cert-manager from a helm installation is a case of running the installation process, in reverse, using the delete command on both kubectl and helm. Firstly, delete the cert-manager installation using helm. Ensure the --purge flag is applied. helm delete cert-manager --purge Next, delete the cert-manager namespace: kubectl delete namespace cert-manager Finally, delete the cert-manger CustomResourceDefinitions using the link to the version vX.Y you installed: kubectl delete -f https://raw.githubusercontent.com/jetstack/cert-manager/release-X.Y/deploy/manifests/00-crds.yaml Namespace Stuck in Terminating State If the namespace has been marked for deletion without deleting the cert-manager installation first, the namespace may become stuck in a terminating state. This is typically due to the fact that the APIService resource still exists however the webhook is no longer running so is no longer reachable. To resolve this, ensure you have run the above commands correctly, and if you’re still experiencing issues then run kubectl delete apiservice v1beta1.webhook.cert-manager.io. Uninstalling on OpenShift Below is the processes for uninstalling cert-manager on OpenShift. warning: To uninstall cert-manger you should always use the same process for installing but in reverse. Deviating from the following process can cause issues and potentially broken states. Please ensure you follow the below steps when uninstalling to prevent this happening. Login to your OpenShift cluster Before you can uninstall cert-manager, you must first ensure your local machine is configured to talk to your OpenShift cluster using the oc tool. # Login to the OpenShift cluster as the system:admin user oc login -u system:admin Uninstalling with regular manifests Before continuing, ensure that all cert-manager resources that have been created by users have been deleted. You can check for any existing resources with the following command: oc get Issuers,ClusterIssuers,Certificates,CertificateRequests,Orders,Challenges --all-namespaces Once all these resources have been deleted you are ready to uninstall cert-manager. Uninstalling from an installation with regular manifests is a case of running the installation process, in reverse, using the delete command of oc. Delete the installation manifests using a link to your currently running version vX.Y.Z like so: oc delete -f http","date":"2019-10-18","objectID":"/posts/k8s/cert-manager-v0.11-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:3:4","tags":null,"title":"Cert Manager v0.11 官方文档阅读笔记","uri":"/posts/k8s/cert-manager-v0.11-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Upgrading cert-manager This section contains information on upgrading cert-manager. It also contains documents detailing breaking changes between cert-manager versions, and information on things to look out for when upgrading. note: Before performing upgrades of cert-manager, it is advised to take a backup of all your cert-manager resources just in case an issue occurs whilst upgrading. You can read how to backup and restore cert-manager in the Backing up and restoring guide.. Upgrading with Helm If you installed cert-manager using Helm, you can easily upgrade using the Helm CLI. note: Before upgrading, please read the relevant instructions at the links below for your from and to version. Once you have read the relevant upgrading notes and taken any appropriate actions, you can begin the upgrade process like so - replacing \u003crelease_name\u003e with the name of your Helm release for cert-manager (usually this is cert-manager) and replacing \u003cversion\u003e with the version number you want to install: # Install the cert-manager CustomResourceDefinition resources before # upgrading the Helm chart kubectl apply \\ --validate=false \\ -f https://raw.githubusercontent.com/jetstack/cert-manager/\u003cversion\u003e/deploy/manifests/00-crds.yaml # Add the Jetstack Helm repository if you haven't already helm repo add jetstack https://charts.jetstack.io # Ensure the local Helm chart repository cache is up to date helm repo update helm upgrade --version \u003cversion\u003e \u003crelease_name\u003e jetstack/cert-manager This will upgrade you to the latest version of cert-manager, as listed in the Jetstack Helm chart repository. note: You can find out your release name using helm list | grep cert-manager. Upgrading using static manifests If you installed cert-manager using the static deployment manifests published on each release, you can upgrade them in a similar way to how you first installed them. note: Before upgrading, please read the relevant instructions at the links below for your from and to version. Once you have read the relevant notes and taken any appropriate actions, you can begin the upgrade process like so - replacing \u003cversion\u003e with the version number you want to install: kubectl apply \\ --validate=false \\ -f https://github.com/jetstack/cert-manager/releases/download/\u003cversion\u003e/cert-manager.yaml note:: If you are running Kubernetes v1.15 or below, you will need to add the --validate=false flag to your kubectl apply command above else you will receive a validation error relating to the x-kubernetes-preserve-unknown-fields field in our CustomResourceDefinition resources. This is a benign error and occurs due to the way kubectl performs resource validation. Upgrading from v0.2 to v0.3 Upgrading from v0.3 to v0.4 Upgrading from v0.4 to v0.5 Upgrading from v0.5 to v0.6 Upgrading from v0.6 to v0.7 Upgrading from v0.7 to v0.8 Upgrading from v0.8 to v0.9 Upgrading from v0.9 to v0.10 Upgrading from v0.10 to v0.11 ","date":"2019-10-18","objectID":"/posts/k8s/cert-manager-v0.11-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:3:5","tags":null,"title":"Cert Manager v0.11 官方文档阅读笔记","uri":"/posts/k8s/cert-manager-v0.11-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Reference documentation This section contains detailed reference documentation about cert-manager’s types and how it operates. It also includes some simple example configurations in order to help users activate advanced functionality of cert-manager. Step by step user guides and tutorials can be found in the tutorials section. ","date":"2019-10-18","objectID":"/posts/k8s/cert-manager-v0.11-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:4:0","tags":null,"title":"Cert Manager v0.11 官方文档阅读笔记","uri":"/posts/k8s/cert-manager-v0.11-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Certificates cert-manager has the concept of ‘Certificates’ that define a desired X.509 certificate. A Certificate is a namespaced resource that references an Issuer or ClusterIssuer for information on how to obtain the certificate. A simple Certificate could be defined as: apiVersion:cert-manager.io/v1alpha2kind:Certificatemetadata:name:acme-crtspec:secretName:acme-crt-secretdnsNames:- foo.example.com- bar.example.comacme:config:- http01:ingressClass:nginxdomains:- foo.example.com- bar.example.comissuerRef:name:letsencrypt-prod# We can reference ClusterIssuers by changing the kind here.# The default value is Issuer (i.e. a locally namespaced Issuer)kind:Issuer This Certificate will tell cert-manager to attempt to use the Issuer named letsencrypt-prod to obtain a certificate key pair for the foo.example.com and bar.example.com domains. If successful, the resulting key and certificate will be stored in a secret named acme-crt-secret with keys of tls.key and tls.crt respectively. This secret will live in the same namespace as the Certificate resource. The dnsNames field specifies a list of Subject Alternative Names to be associated with the certificate. If the commonName field is omitted, the first element in the list will be the common name. The referenced Issuer must exist in the same namespace as the Certificate. A Certificate can alternatively reference a ClusterIssuer which is non-namespaced. Certificate Duration and Renewal Window cert-manager Certificate resources also support custom validity durations and renewal windows. Important: The backend service implementation can choose to generate a certificate with a different validity period than what is requested in the issuer. Although the duration and renewal periods are specified on the Certificate resources, the corresponding Issuer or ClusterIssuer must support this. The table below shows the support state of the different backend services used by issuer types: Issuer Description ACME Only ‘renewBefore’ supported CA Fully supported Vault Fully supported (although the requested duration must be lower than the configured Vault role’s TTL) Self Signed Fully supported Venafi Fully supported The default duration for all certificates is 90 days and the default renewal windows is 30 days. This means that certificates are considered valid for 3 months and renewal will be attempted within 1 month of expiration. The duration and renewBefore parameters must be given in the golang parseDuration string format. Example Usage Here an example of an issuer specifying the duration and renewal window. The certificate from the previous section is extended with a validity period of 24 hours and to begin trying to renew 12 hours before the certificate expiration. apiVersion:cert-manager.io/v1alpha2kind:Certificatemetadata:name:examplespec:secretName:example-tlsduration:24hrenewBefore:12hdnsNames:- foo.example.com- bar.example.comissuerRef:name:my-internal-cakind:Issuer Certificate Key Encoding cert-manager Certificate resources support two types of key encodings for its private key known as the private key cryptography standards (PKCS). The two key encodings are PKCS#1 and PKCS#8. The default encoding is PKCS#1, if the keyEncoding field of the Certificate spec is left empty. A limitation exists where once a Certificate resource is generated with a specific key encoding, it cannot be generated with a different key encoding. Example Usage Here is an example of a Certificate specifying the use of PKCS#8 encoding on its private key. apiVersion:cert-manager.io/v1alpha2kind:Certificatemetadata:name:example-pkcs8-certspec:secretName:example-pkcs8-secretkeyEncoding:pkcs8dnsNames:- foo.example.com- bar.example.comissuerRef:name:my-internal-cakind:Issuer ","date":"2019-10-18","objectID":"/posts/k8s/cert-manager-v0.11-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:4:1","tags":null,"title":"Cert Manager v0.11 官方文档阅读笔记","uri":"/posts/k8s/cert-manager-v0.11-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"CertificateRequests A ‘CertificateRequest’ is a resource in cert-manager that is used to request x509 certificates from an issuer. The resource contains a base64 encoded string of a PEM encoded certificate request which is sent to the referenced issuer. A successful issuance will return a signed certificate, based on the certificate signing request. ‘CertificateRequets’ are typically consumed and managed by controllers or other systems and should not be used by humans - unless specifically needed. A simple CertificateRequest looks like the following: apiVersion:cert-manager.io/v1alpha2kind:CertificateRequestmetadata:name:my-ca-crspec:csr:LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQzNqQ0NBY1lDQVFBd2daZ3hDekFKQmdOVkJBWVRBbHBhTVE4d0RRWURWUVFJREFaQmNHOXNiRzh4RFRBTApCZ05WQkFjTUJFMXZiMjR4RVRBUEJnTlZCQW9NQ0VwbGRITjBZV05yTVJVd0V3WURWUVFMREF4alpYSjBMVzFoCmJtRm5aWEl4RVRBUEJnTlZCQU1NQ0dwdmMyaDJZVzVzTVN3d0tnWUpLb1pJaHZjTkFRa0JGaDFxYjNOb2RXRXUKZG1GdWJHVmxkWGRsYmtCcVpYUnpkR0ZqYXk1cGJ6Q0NBU0l3RFFZSktvWklodmNOQVFFQkJRQURnZ0VQQURDQwpBUW9DZ2dFQkFLd01tTFhuQkNiRStZdTIvMlFtRGsxalRWQ3BvbHU3TlZmQlVFUWl1bDhFMHI2NFBLcDRZQ0c5Cmx2N2kwOHdFMEdJQUgydnJRQmxVd3p6ZW1SUWZ4YmQvYVNybzRHNUFBYTJsY2NMaFpqUlh2NEVMaER0aVg4N3IKaTQ0MWJ2Y01OM0ZPTlRuczJhRkJYcllLWGxpNG4rc0RzTEVuZmpWdXRiV01Zeis3M3ptaGZzclRJUjRzTXo3cQpmSzM2WFM4UkRjNW5oVVcyYU9BZ3lnbFZSOVVXRkxXNjNXYXVhcHg2QUpBR1RoZnJYdVVHZXlZUUVBSENxZmZmCjhyOEt3YTFYK1NwYm9YK1ppSVE0Nk5jQ043OFZnL2dQVHNLZmphZURoNWcyNlk1dEVidHd3MWdRbWlhK0MyRHIKWHpYNU13RzJGNHN0cG5kUnRQckZrU1VnMW1zd0xuc0NBd0VBQWFBQU1BMEdDU3FHU0liM0RRRUJDd1VBQTRJQgpBUUFXR0JuRnhaZ0gzd0N3TG5IQ0xjb0l5RHJrMUVvYkRjN3BJK1VVWEJIS2JBWk9IWEFhaGJ5RFFLL2RuTHN3CjJkZ0J3bmlJR3kxNElwQlNxaDBJUE03eHk5WjI4VW9oR3piN0FVakRJWHlNdmkvYTJyTVhjWjI1d1NVQmxGc28Kd005dE1QU2JwcEVvRERsa3NsOUIwT1BPdkFyQ0NKNnZGaU1UbS9wMUJIUWJSOExNQW53U0lUYVVNSFByRzJVMgpjTjEvRGNMWjZ2enEyeENjYVoxemh2bzBpY1VIUm9UWmV1ZEp6MkxmR0VHM1VOb2ppbXpBNUZHd0RhS3BySWp3ClVkd1JmZWZ1T29MT1dNVnFNbGRBcTlyT24wNHJaT3Jnak1HSE9tTWxleVdPS1AySllhaDNrVDdKU01zTHhYcFYKV0ExQjRsLzFFQkhWeGlKQi9Zby9JQWVsCi0tLS0tRU5EIENFUlRJRklDQVRFIFJFUVVFU1QtLS0tLQo=isCA:falseduraton:90dissuerRef:name:ca-issuer# We can reference ClusterIssuers by changing the kind here.# The default value is Issuer (i.e. a locally namespaced Issuer)kind:Issuergroup:cert-manager.io This CertificateRequest will make cert-manager attempt to make the Issuer letsencrypt-prod in the default issuer pool cert-manager.io, return a certificate based upon the certificate signing request. Other groups can be specified inside the issuerRef which will change the targeted issuers to other external, third party issuers you may have installed. The resource also exposes the option for stating the certificate as CA and requested validity duration. A successful issuance of the certificate signing request will cause an update to the resource, setting the status with the signed certificate, the CA of the certificate (if available), and setting the Ready condition to True. Whether issuance of the controller was successful or not, a retry of the issuance will not happen. It is the responsibility of some other controller to manage the logic and life cycle of CertificateRequets. Conditions CertificateRequests have a set of strongly defined conditions that should be used and relied upon by controllers or services to make decisions on what actions to take next on the resource. Each condition consists of the pair Ready - a boolean value, and Reason - a string. The set of values and meanings are as follows: Ready Reason Condition Meaning False Pending The CertificateRequest is currently pending, waiting for some other operation to take place. This could be that the Issuer does not exist yet or the Issuer is in the process of issuing a certificate. False Failed The certificate has failed to be issued - either the returned certificate failed to be decoded or an instance of the referenced issuer used for signing failed. No further action will be taken on the CertificateRequest by it’s controller. True Is","date":"2019-10-18","objectID":"/posts/k8s/cert-manager-v0.11-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:4:2","tags":null,"title":"Cert Manager v0.11 官方文档阅读笔记","uri":"/posts/k8s/cert-manager-v0.11-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Orders Order resources are used by the ACME issuer to manage the lifecycle of an ACME ‘order’ for a signed TLS certificate. When a Certificate resource is created that references an ACME issuer, cert-manager will create an Order resource in order to obtain a signed certificate. As an end-user, you will never need to manually create an Order resource. Once created, an Order cannot be changed. Instead, a new Order resource must be created. Debugging Order resources In order to debug why a Certificate isn’t being issued, we can first run kubectl describe on the Certificate resource we’re having issues with: $ kubectl describe certificate example-com ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Generated 1m cert-manager Generated new private key Normal OrderCreated 1m cert-manager Created Order resource \"example-com-1217431265\" We can see here that Certificate controller has created an Order resource to request a new certificate from the ACME server. Orders are a useful source of information when debugging failures issuing ACME certificates. By running kubectl describe order on a particular order, information can be gleaned about failures in the process: $ kubectl describe order example-com-1248919344 ... Reason: State: pending URL: https://acme-v02.api.letsencrypt.org/acme/order/41123272/265506123 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Created 1m cert-manager Created Challenge resource \"example-com-1217431265-0\" for domain \"test1.example.com\" Normal Created 1m cert-manager Created Challenge resource \"example-com-1217431265-1\" for domain \"test2.example.com\" Here we can see that cert-manager has created two Challenge resources in order to fulfil(满足 ) the requirements of the ACME order to obtain a signed certificate. You can then go on to run kubectl describe challenge example-com-1217431265-0 to further debug the progress of the Order. Once an Order is successful, you should see an event like the following: $ kubectl describe order example-com-1248919344 ... Reason: State: valid URL: https://acme-v02.api.letsencrypt.org/acme/order/41123272/265506123 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Created 72s cert-manager Created Challenge resource \"example-com-1217431265-0\" for domain \"test1.example.com\" Normal Created 72s cert-manager Created Challenge resource \"example-com-1217431265-1\" for domain \"test2.example.com\" Normal OrderValid 4s cert-manager Order completed successfully If the Order is not completing successfully, you can debug the challenges for the Order by running kubectl describe on the Challenge resource. For more information on debugging Challenge resources, read the challenge reference docs. ","date":"2019-10-18","objectID":"/posts/k8s/cert-manager-v0.11-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:4:3","tags":null,"title":"Cert Manager v0.11 官方文档阅读笔记","uri":"/posts/k8s/cert-manager-v0.11-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Challenges Challenge resources are used by the ACME issuer to manage the lifecycle of an ACME ‘challenge’ that must be completed in order to complete an ‘authorization’ for a single DNS name/identifier. When an Order resource is created, the order controller will create Challenge resources for each DNS name that is being authorized with the ACME server. As an end-user, you will never need to manually create a Challenge resource. Once created, a Challenge cannot be changed. Instead, a new Challenge resource must be created. Challenge lifecycle After a Challenge resource has been created, it will be initially queued for processing. Processing will not begin until the challenge has been ‘scheduled’ to start. This scheduling process prevents too many challenges being attempted at once, or multiple challenges for the same DNS name being attempted at once. For more information on how challenges are scheduled, read the challenge scheduling section. Once a challenge has been scheduled, it will first be ‘synced’ with the ACME server in order to determine its current state. If the challenge is already valid, its ‘state’ will be updated to ‘valid’, and also set status.processing = false to ‘unschedule’ itself. If the challenge is still ‘pending’, the challenge controller will ‘present’ the challenge using the configured solver, one of HTTP01 or DNS01. Once the challenge has been ‘presented’, it will set status.presented=true. Once ‘presented’, the challenge controller will perform a ‘self check’ to ensure that the challenge has ‘propagated’ (i.e. the authoritve DNS servers have been updated to respond correctly, or the changes to the ingress resources have been observed and in-use by the ingress controller). If the self check fails, cert-manager will retry the self check with a fixed 10 second retry interval. Challenges that do not ever complete the self check will continue retrying until the user intervenes. Once the self check is passing, the ACME ‘authorization’ associated with this challenge will be ‘accepted’ (TODO: add link to accepting challenges section of ACME spec). The final state of the authorization after accepting it will be copied across to the Challenge’s status.state field, as well as the ‘error reason’ if an error occurred whilst the ACME server attempted to validate the challenge. Once a Challenge has entered the valid, invalid, expired or revoked state, it will set status.processing=false to prevent any further processing of the ACME challenge, and to allow another challenge to be scheduled if there is a backlog of challenges to complete. Challenge scheduling Instead of attempting to process all challenges at once, challenges are ‘scheduled’ by cert-manager. This scheduler applies a cap on the maximum number of simultaneous challenges as well as disallows two challenges for the same DNS name and solver type (http-01 or dns-01) to be completed at once. The maximum number of challenges that can be processed at a time is 60 as of ddff78. Debugging Challenge resources In order to determine why an ACME Certificate is not being issued, we can debug using the ‘Challenge’ resources that cert-manager has created. In order to determine which Challenge is failing, you can run kubectl get challenges: $ kubectl get challenges NAME STATE DOMAIN REASON AGE example-com-1217431265-0 pending example.com Waiting for dns-01 challenge propagation 22s This shows that the challenge has been presented using the DNS01 solver successfully and now cert-manager is waiting for the ‘self check’ to pass. You can get more information about the challenge by using kubectl describe: $ kubectl describe challenge example-com-1217431265-0 ... Status: Presented: true Processing: true Reason: Waiting for dns-01 challenge propagation State: pending Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Started 19s cert-manager Challenge scheduled for processing Normal Presented 16s cert-manager Presented challenge using dns-01 challenge me","date":"2019-10-18","objectID":"/posts/k8s/cert-manager-v0.11-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:4:4","tags":null,"title":"Cert Manager v0.11 官方文档阅读笔记","uri":"/posts/k8s/cert-manager-v0.11-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Issuers Issuers (and ClusterIssuers) represent a certificate authority from which signed x509 certificates can be obtained, such as Let’s Encrypt. You will need at least one Issuer or ClusterIssuer in order to begin issuing certificates within your cluster. An example of an Issuer type is ACME. A simple ACME issuer could be defined as: apiVersion:cert-manager.io/v1alpha2kind:Issuermetadata:name:letsencrypt-prodnamespace:edge-servicesspec:acme:# The ACME server URLserver:https://acme-v02.api.letsencrypt.org/directory# Email address used for ACME registrationemail:user@example.com# Name of a secret used to store the ACME account private keyprivateKeySecretRef:name:letsencrypt-prodsolvers:# An empty 'selector' means that this solver matches all domains- selector:{}http01:ingress:class:nginx This is the simplest of ACME issuers - it specifies no DNS-01 challenge providers. HTTP-01 validation can be performed through using Ingress resources by enabling the HTTP-01 challenge mechanism (with the http01: {} field). More information on configuring ACME Issuers can be found here. Namespacing An Issuer is a namespaced resource, and it is not possible to issue certificates from an Issuer in a different namespace. This means you will need to create an Issuer in each namespace you wish to obtain Certificates in. If you want to create a single issuer than can be consumed in multiple namespaces, you should consider creating a ClusterIssuer resource. This is almost identical to the Issuer resource, however is non-namespaced and so it can be used to issue Certificates across all namespaces. Ambient Credentials Some API clients are able to infer credentials to use from the environment they run within. Notably, this includes cloud instance-metadata stores and environment variables. In cert-manager, the term ‘ambient credentials’ refers to such credentials. They are always drawn from the environment of the ‘cert-manager-controller’ deployment. Example Usage If cert-manager is deployed in an environment with ambient AWS credentials, such as with a kube2iam_ role, the following ClusterIssuer would make use of those credentials to perform the ACME DNS01 challenge with route53. apiVersion:cert-manager.io/v1alpha2kind:ClusterIssuermetadata:name:letsencrypt-prodspec:acme:server:https://acme-v02.api.letsencrypt.org/directoryemail:user@example.comprivateKeySecretRef:name:letsencrypt-prodsolvers:# An empty 'selector' means that this solver matches all domains- selector:{}dns01:providers:- name:route53route53:region:us-east-1 It is important to note that the route53 section does not specify any accessKeyID or secretAccessKeySecretRef. If either of these are specified, ambient credentials will not be used. When are Ambient Credentials used Ambient credentials are supported for the ‘route53’ ACME DNS01 challenge provider. They will only be used if no credentials are supplied, even if the supplied credentials are invalid. By default, ambient credentials may be used by ClusterIssuers, but not regular issuers. The --issuer-ambient-credentials and --cluster-issuer-ambient-credentials=false flags on cert-manager may be used to override this behavior. Note that ambient credentials are disabled for regular Issuers by default to ensure unprivileged users who may create issuers cannot issue certificates using any credentials cert-manager incidentally has access to. Supported Issuer types cert-manager has been designed to support pluggable Issuer backends. The currently supported Issuer types are: Name Description ACME Supports obtaining certificates from an ACME server, validating with HTTP01 or DNS01 CA Supports issuing certificates using a simple signing keypair, stored in a Secret in the Kubernetes API server Vault Supports issuing certificates using HashiCorp Vault. Self signed Supports issuing self signed certificates Venafi Supports issuing certificates from Venafi Cloud \u0026 TPP Each Issuer resource is of one, and only one type. The type of an Issuer is inferred b","date":"2019-10-18","objectID":"/posts/k8s/cert-manager-v0.11-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:4:5","tags":null,"title":"Cert Manager v0.11 官方文档阅读笔记","uri":"/posts/k8s/cert-manager-v0.11-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"ClusterIssuers ClusterIssuers are a resource type similar to Issuers. They are specified in exactly the same way, but they do not belong to a single namespace and can be referenced by Certificate resources from multiple different namespaces. They are particularly useful when you want to provide the ability to obtain certificates from a central authority (e.g. Letsencrypt, or your internal CA) and you run single-tenant clusters. The docs for Issuer resources apply equally to ClusterIssuers. You can specify a ClusterIssuer resource by changing the kind attribute of an Issuer to ClusterIssuer, and removing the metadata.namespace attribute: apiVersion:cert-manager.io/v1alpha2kind:ClusterIssuermetadata:name:letsencrypt-prodspec:... We can then reference a ClusterIssuer from a Certificate resource by setting the spec.issuerRef.kind field to ClusterIssuer: apiVersion:cert-manager.io/v1alpha2kind:Certificatemetadata:name:my-certificatenamespace:my-namespacespec:secretName:my-certificate-secretissuerRef:name:letsencrypt-prodkind:ClusterIssuer... When referencing a Secret resource in ClusterIssuer resources (eg apiKeySecretRef) the Secret needs to be in the same namespace as the cert-manager controller pod. You can optionally override this by using the --cluster-resource-namespace argument to the controller. ","date":"2019-10-18","objectID":"/posts/k8s/cert-manager-v0.11-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:4:6","tags":null,"title":"Cert Manager v0.11 官方文档阅读笔记","uri":"/posts/k8s/cert-manager-v0.11-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"cainjector controller The cainjector controller injects a Certificate into the caBundle field of ValidatingWebhookConfiguration, MutatingWebhookConfiguration or APIService resources annotated with: cert-manager.io/inject-apiserver-ca: “true” Injects the cluster CA. cert-manager.io/inject-ca-from: \u003cNAMESPACE\u003e/\u003cCERTIFICATE\u003e Injects the CA from the specified certificate. ","date":"2019-10-18","objectID":"/posts/k8s/cert-manager-v0.11-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:4:7","tags":null,"title":"Cert Manager v0.11 官方文档阅读笔记","uri":"/posts/k8s/cert-manager-v0.11-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"API documentation https://cert-manager.readthedocs.io/en/latest/reference/api-docs/index.html ","date":"2019-10-18","objectID":"/posts/k8s/cert-manager-v0.11-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:4:8","tags":null,"title":"Cert Manager v0.11 官方文档阅读笔记","uri":"/posts/k8s/cert-manager-v0.11-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":" 参考： https://metallb.universe.tf/ https://github.com/danderson/metallb ","date":"2019-10-16","objectID":"/posts/k8s/metallb-v0.8.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:0:0","tags":null,"title":"MetalLB-v0.8.1-官方文档阅读笔记","uri":"/posts/k8s/metallb-v0.8.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Introduction MetalLB is a load-balancer implementation for bare metal Kubernetes clusters, using standard routing protocols. MetalLB is a young project. You should treat it as a beta system. The project maturity(项目进度) page explains what that implies. Why? Kubernetes does not offer an implementation of network load-balancers (Services of type LoadBalancer) for bare metal clusters. The implementations of Network LB that Kubernetes does ship with are all glue code that calls out to various IaaS platforms(Kubernetes随附的Network LB的实现都是调用各种IaaS平台的粘合代码) (GCP, AWS, Azure…). If you’re not running on a supported IaaS platform (GCP, AWS, Azure…), LoadBalancers will remain in the “pending” state indefinitely when created. Bare metal cluster operators are left with two lesser tools to bring user traffic into their clusters, “NodePort” and “externalIPs” services. Both of these options have significant(重大的) downsides(缺点) for production use, which makes bare metal clusters second class citizens(公民) in the Kubernetes ecosystem(生态系统). MetalLB aims to redress(纠正) this imbalance(不平衡) by offering a Network LB implementation that integrates(整合) with standard network equipment, so that external services on bare metal clusters also “just work” as much as possible. Requirements MetalLB requires the following to function: A Kubernetes cluster, running Kubernetes 1.13.0 or later, that does not already have network load-balancing functionality. A cluster network configuration that can coexist(共存) with MetalLB. Some IPv4 addresses for MetalLB to hand out. Depending on the operating mode, you may need one or more routers capable of speaking BGP. Usage The concepts section will give you a primer on what MetalLB does in your cluster. When you’re ready to deploy to a Kubernetes cluster, head to the installation and usage guides. ","date":"2019-10-16","objectID":"/posts/k8s/metallb-v0.8.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:1:0","tags":null,"title":"MetalLB-v0.8.1-官方文档阅读笔记","uri":"/posts/k8s/metallb-v0.8.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Concepts MetalLB hooks into(钩入) your Kubernetes cluster, and provides a network load-balancer implementation. In short, it allows you to create Kubernetes services of type “LoadBalancer” in clusters that don’t run on a cloud provider, and thus cannot simply hook into(使用) paid products(付费产品) to provide load-balancers. It has two features that work together to provide this service: address allocation(地址分配), and external announcement(外部公告). ","date":"2019-10-16","objectID":"/posts/k8s/metallb-v0.8.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:2:0","tags":null,"title":"MetalLB-v0.8.1-官方文档阅读笔记","uri":"/posts/k8s/metallb-v0.8.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Address allocation In a cloud-enabled Kubernetes cluster, you request a load-balancer, and your cloud platform assigns an IP address to you. In a bare metal cluster, MetalLB is responsible for that allocation. MetalLB cannot create IP addresses out of thin air(凭空), so you do have to give it pools of IP addresses that it can use. MetalLB will take care of assigning and unassigning individual addresses as services come and go, but it will only ever hand out IPs that are part of its configured pools. How you get IP address pools for MetalLB depends on your environment. If you’re running a bare metal cluster in a colocation facility(代管设施), your hosting provider probably offers IP addresses for lease(出租). In that case, you would lease, say, a /26 of IP space (64 addresses), and provide that range to MetalLB for cluster services. Alternatively, your cluster might be purely private, providing services to a nearby LAN but not exposed to the internet. In that case, you could pick a range of IPs from one of the private adress spaces (so-called RFC1918 addresses), and assign those to MetalLB. Such addresses are free, and work fine as long as you’re only providing cluster services to your LAN(这样的地址是免费的，并且只要您仅向局域网提供群集服务就可以正常工作). Or, you could do both! MetalLB lets you define as many address pools as you want, and doesn’t care what “kind” of addresses you give it. ","date":"2019-10-16","objectID":"/posts/k8s/metallb-v0.8.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:2:1","tags":null,"title":"MetalLB-v0.8.1-官方文档阅读笔记","uri":"/posts/k8s/metallb-v0.8.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"External announcement Once MetalLB has assigned an external IP address to a service, it needs to make the network beyond the cluster aware that the IP “lives” in the cluster. MetalLB uses standard routing protocols to achieve this: ARP, NDP, or BGP. Layer 2 mode (ARP/NDP) In layer 2 mode, one machine in the cluster takes ownership of the service(集群中的一台机器获得服务的所有权), and uses standard address discovery protocols (ARP for IPv4, NDP for IPv6) to make those IPs reachable on the local network. From the LAN’s point of view, the announcing machine simply has multiple IP addresses. The layer 2 mode sub-page has more details on the behavior and limitations of layer 2 mode. BGP In BGP mode, all machines in the cluster establish(建立) BGP peering sessions with nearby routers that you control, and tell those routers how to forward traffic to the service IPs. Using BGP allows for true load balancing across multiple nodes, and fine-grained traffic control thanks to BGP’s policy mechanisms. 在BGP模式下，群集中的所有计算机都与您控制的附近路由器建立 BGP 对等会话，并告诉这些路由器如何将流量转发至 service 的 ip。 借助BGP的策略机制，使用BGP可以在多个节点之间实现真正的负载平衡，并实现细粒度的流量控制。 The BGP mode sub-page has more details on BGP mode’s operation and limitations. ","date":"2019-10-16","objectID":"/posts/k8s/metallb-v0.8.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:2:2","tags":null,"title":"MetalLB-v0.8.1-官方文档阅读笔记","uri":"/posts/k8s/metallb-v0.8.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"MetalLB in layer 2 mode In layer 2 mode, one node assumes the responsibility of advertising a service to the local network. From the network’s perspective, it simply looks like that machine has multiple IP addresses assigned to its network interface. Under the hood, MetalLB responds to ARP requests for IPv4 services, and NDP requests for IPv6. The major advantage of the layer 2 mode is its universality(通用性): it will work on any ethernet network, with no special hardware required, not even fancy routers. 第2层模式的主要优点是它的通用性：它可以在任何以太网网络上运行，不需要特殊的硬件，甚至不需要花哨的路由器。 Load-balancing behavior In layer 2 mode, all traffic for a service IP goes to one node. From there, kube-proxy spreads the traffic to all the service’s pods. In that sense, layer 2 does not implement a load-balancer. Rather, it implements a failover mechanism so that a different node can take over should the current leader node fail for some reason. 从这个意义上讲，第2层没有实现负载平衡器。 相反，它实现了故障转移机制，以便当当前引导节点由于某种原因发生故障时，其他节点可以接管。 If the leader node fails for some reason, failover is automatic: the old leader’s lease(租约) times out after 10 seconds, at which point another node becomes the leader and takes over ownership of the service IP. Limitations Layer 2 mode has two main limitations you should be aware of: single-node bottlenecking(单节点瓶颈), and potentially slow failover(潜在的缓慢故障转移). As explained above, in layer2 mode a single leader-elected node receives all traffic for a service IP. This means that your service’s ingress bandwidth(带宽) is limited to the bandwidth of a single node. This is a fundamental limitation of using ARP and NDP to steer traffic(这是使用ARP和NDP引导流量的基本限制). In the current implementation, failover(故障转移) between nodes depends on cooperation from the clients. When a failover occurs, MetalLB sends a number of gratuitou layer 2 packets (a bit of a misnomer - it should really be called “unsolicited layer 2 packets”) to notify clients that the MAC address associated with the service IP has changed. Most operating systems handle “gratuitous” packets correctly, and update their neighbor caches promptly. In that case, failover happens within a few seconds. However, some systems either don’t implement gratuitous handling at all, or have buggy implementations that delay the cache update. All modern versions of major OSes (Windows, Mac, Linux) implement layer 2 failover correctly, so the only situation where issues may happen is with older or less common OSes. To minimize the impact of planned failover on buggy clients, you should keep the old leader node up for a couple of minutes after flipping leadership, so that it can continue forwarding traffic for old clients until their caches refresh. During an unplanned failover, the service IPs will be unreachable until the buggy clients refresh their cache entries. If you encounter a situation where layer 2 mode failover is slow (more than about 10s), please [file a bug] (https://github.com/google/metallb/issues/new)! We can help you investigate and determine if the issue is with the client, or a bug in MetalLB. Comparison to Keepalived(与 keepalived 对比)) MetalLB’s layer2 mode has a lot of similarities to Keepalived, so if you’re familiar with Keepalived, this should all sound fairly familiar. However, there are also a few differences worth mentioning. If you aren’t familiar with Keepalived, you can skip this section. Keepalived uses the Virtual Router Redundancy Protocol (VRRP 虚拟路由冗余协议). Instances of Keepalived continuously exchange VRRP messages with each other, both to select a leader and to notice when that leader goes away. MetalLB on the other hand relies on Kubernetes to know when pods and nodes go up and down. It doesn’t need to speak a separate protocol to select leaders, instead it just lets Kubernetes do most of the work of deciding which pods are healthy, and which nodes are ready. Keepalived and MetalLB “look” the same from the client’s perspective: the service IP address seems to migrate from one machine to another when failove","date":"2019-10-16","objectID":"/posts/k8s/metallb-v0.8.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:2:3","tags":null,"title":"MetalLB-v0.8.1-官方文档阅读笔记","uri":"/posts/k8s/metallb-v0.8.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"MetalLB in BGP mode In BGP mode, each node in your cluster establishes(建立) a BGP peering session with your network routers, and uses that peering session to advertise the IPs of external cluster services. 在BGP模式下，群集中的每个节点都与网络路由器建立BGP对等会话，并使用该对等会话来通告外部群集服务的IP。 Assuming your routers are configured to support multipath, this enables true load-balancing: the routes published by MetalLB are equivalent to each other, except for their nexthop. This means that the routers will use all nexthops together, and load-balance between them. 假设您的路由器配置为支持多路径，这将实现真正的负载平衡：MetalLB发布的路由彼此等效，除了它们的下一跳。 这意味着路由器将一起使用所有下一跳，并在它们之间进行负载平衡。 Once the packets arrive at the node, kube-proxy is responsible for the final hop of traffic routing, to get the packets to one specific pod in the service. Load-balancing behavior The exact behavior of the load-balancing depends on your specific router model and configuration, but the common behavior is to balance per-connection, based on a packet hash. What does this mean? 负载平衡的确切行为取决于您的特定路由器型号和配置，但是常见的行为是基于数据包哈希值来平衡每个连接。 这是什么意思？ Per-connection means that all the packets for a single TCP or UDP session will be directed to a single machine in your cluster. The traffic spreading only happens between different connections, not for packets within one connection. 每次连接意味着单个TCP或UDP会话的所有数据包都将定向到群集中的单个计算机。 流量只在不同的连接之间传播，而不是在一个连接中的数据包之间。 This is a good thing, because spreading packets across multiple cluster nodes would result in poor behavior on several levels: Spreading a single connection across multiple paths results in packet reordering on the wire, which drastically impacts performance at the end host. 将单个连接分布在多个路径上会导致数据包在网络上重新排序，这将严重影响最终主机的性能。 On-node traffic routing in Kubernetes is not guaranteed to be consistent across nodes. This means that two different nodes could decide to route packets for the same connection to different pods, which would result in connection failures. Kubernetes中单节点流量路由在各个节点之间不保证是一致的。 这意味着两个不同的节点可能决定将同一连接的数据包路由到不同的Pod，这将导致连接失败 Packet hashing is how high performance routers can statelessly spread connections across multiple backends. For each packet, they extract some of the fields, and use those as a “seed” to deterministically pick one of the possible backends. If all the fields are the same, the same backend will be chosen. 数据包哈希是高性能路由器如何无状态地在多个后端分布连接的方式。 对于每个数据包，他们提取一些字段，并将其用作“种子”，以确定性地选择一个可能的后端。 如果所有字段都相同，则将选择相同的后端。 The exact hashing methods available depend on the router hardware and software. Two typical options are 3-tuple and 5-tuple hashing. 3-tuple uses (protocol, source-ip, dest-ip) as the key, meaning that all packets between two unique IPs will go to the same backend. 5-tuple hashing adds the source and destination ports to the mix, which allows different connections from the same clients to be spread around the cluster. In general, it’s preferable to put as much entropy as possible into the packet hash, meaning that using more fields is generally good. This is because increased entropy brings us closer to the “ideal” load-balancing state, where every node receives exactly the same number of packets. We can never achieve that ideal state because of the problems we listed above, but what we can do is try and spread connections as evenly as possible, to try and prevent hotspots from forming. 通常，最好将尽可能多的熵放入数据包哈希中，这意味着使用更多字段通常是好的。 这是因为熵的增加使我们更接近“理想的”负载平衡状态，在该状态下，每个节点都接收完全相同数量的数据包。 由于上面列出的问题，我们永远无法达到理想状态，但是我们可以做的是尝试并尽可能均匀地分布连接，以尝试防止热点的形成。 Limitations Using BGP as a load-balancing mechanism has the advantage that you can use standard router hardware, rather than bespoke load-balancers. However, this comes with downsides as well. 将BGP用作负载平衡机制的优势在于，您可以使用标准路由器硬件，而不是定制的负载平衡器。 但是，这也有缺点。 The biggest is that BGP-based load balancing does not react gracefully to changes in the backend set for an address. What this means is that when a cluster node goes down, you should expect all active connections to your service to be broken (users will see “Connection reset by peer”). 最大的问题是，基于BGP的负载平衡无法对地址后端集","date":"2019-10-16","objectID":"/posts/k8s/metallb-v0.8.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:2:4","tags":null,"title":"MetalLB-v0.8.1-官方文档阅读笔记","uri":"/posts/k8s/metallb-v0.8.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Installation Before starting with installation, make sure you meet all the requirements. In particular, you should pay attention to network addon compatibility. If you’re trying to run MetalLB on a cloud platform, you should also look at the cloud compatibility page and make sure your cloud platform can work with MetalLB (most cannot). There are two supported ways to install MetalLB: using Kubernetes manifests, or using the Helm package manager. ","date":"2019-10-16","objectID":"/posts/k8s/metallb-v0.8.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:3:0","tags":null,"title":"MetalLB-v0.8.1-官方文档阅读笔记","uri":"/posts/k8s/metallb-v0.8.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Installation with Kubernetes manifests To install MetalLB, simply apply the manifest: kubectl apply -f https://raw.githubusercontent.com/google/metallb/master/manifests/metallb.yaml This will deploy MetalLB to your cluster, under the metallb-system namespace. The components in the manifest are: The metallb-system/controller deployment. This is the cluster-wide controller that handles IP address assignments. The metallb-system/speaker daemonset. This is the component that speaks the protocol(s) of your choice to make the services reachable. Service accounts for the controller and speaker, along with the RBAC permissions that the components need to function. The installation manifest does not include a configuration file. MetalLB’s components will still start, but will remain idle until you define and deploy a configmap. ","date":"2019-10-16","objectID":"/posts/k8s/metallb-v0.8.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:3:1","tags":null,"title":"MetalLB-v0.8.1-官方文档阅读笔记","uri":"/posts/k8s/metallb-v0.8.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Installation with kustomize You can install MetalLB with kustomize by pointing on the remote kustomization file : # kustomization.ymlnamespace:metallb-systemresources:- github.com/danderson/metallb//manifests?ref=v0.8.2- configmap.yml If you want to use a configMapGenerator for config file, you want to tell kustomize not to append a hash to the configMap, as MetalLB is waiting for a configMap named config (see https://github.com/kubernetes-sigs/kustomize/blob/master/examples/generatorOptions.md): # kustomization.ymlnamespace:metallb-systemresources:- github.com/danderson/metallb//manifests?ref=v0.8.2configMapGenerator:- name:configfiles:- configs/configgeneratorOptions:disableNameSuffixHash:true ","date":"2019-10-16","objectID":"/posts/k8s/metallb-v0.8.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:3:2","tags":null,"title":"MetalLB-v0.8.1-官方文档阅读笔记","uri":"/posts/k8s/metallb-v0.8.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Installation with Helm Note: Due to code review turnaround time, it usually takes a few days after each MetalLB release before the Helm chart is updated in the stable repository. If you’re coming here shortly after a new release, you may end up installing an older version of MetalLB if you use Helm. This mismatch usually gets fixed within 2-3 days. MetalLB maintains a Helm package in the stable package repository. If you use the Helm package manager in your cluster, you can install MetalLB that way. helm install --name metallb stable/metallb Although Helm allows you to easily deploy multiple releases at the same time, you should not do this with MetalLB! Multiple copies of MetalLB will conflict with each other and lead to cluster instability. By default, the helm chart looks for MetalLB configuration in the metallb-config ConfigMap, in the namespace you deployed to. It’s up to you to define and deploy that configuration. Alternatively, you can manage the configuration with Helm itself, by putting the configuration under the configInline key in your values.yaml. ","date":"2019-10-16","objectID":"/posts/k8s/metallb-v0.8.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:3:3","tags":null,"title":"MetalLB-v0.8.1-官方文档阅读笔记","uri":"/posts/k8s/metallb-v0.8.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Network Addon Compatibility Generally speaking, MetalLB doesn’t care which network addon you choose to run in your cluster, as long as it provides the standard behaviors that Kubernetes expects from network addons. The following is a list of network addons that have been tested with MetalLB, for your reference. The list is presented in alphabetical order, we express no preference for one addon over another. Addons that are not on this list probably work, we just haven’t tested them. Please send us a patch if you have information on network addons that aren’t listed! Network addon Compatible Calico Mostly (see known issues) Canal Yes Cilium Yes Flannel Yes Kube-router Mostly (see known issues) Romana Yes (see guide for advanced integration) Weave Net Mostly (see known issues) IPVS mode in kube-proxy Starting in Kubernetes 1.9, kube-proxy has beta support for a more efficient “IPVS mode”, in addition to the default “iptables mode.” MetalLB might work with IPVS mode in kube-proxy, in Kubernetes 1.13 or later. However, it is not explicitly tested yet, so it’s at your own risk. See our tracking bug for details. ","date":"2019-10-16","objectID":"/posts/k8s/metallb-v0.8.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:3:4","tags":null,"title":"MetalLB-v0.8.1-官方文档阅读笔记","uri":"/posts/k8s/metallb-v0.8.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Configuration MetalLB remains idle(空闲的) until configured. This is accomplished by creating and deploying a configmap into the same namespace (metallb-system) as the deployment. There is an example configmap inmanifests/example-config.yaml, annotated with explanatory(解释性的) comments(注释). If you’ve named the configmap config.yaml, you can deploy the manifest with kubectl apply -f config.yaml. If you installed MetalLB with Helm, you will need to change the namespace of the ConfigMap to match the namespace in which MetalLB was deployed, and change the ConfigMap’s name from config to metallb-config. The specific configuration depends on the protocol(s) you want to use to announce service IPs. Jump to: Layer 2 configuration BGP configuration Advanced configuration ","date":"2019-10-16","objectID":"/posts/k8s/metallb-v0.8.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:4:0","tags":null,"title":"MetalLB-v0.8.1-官方文档阅读笔记","uri":"/posts/k8s/metallb-v0.8.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Layer 2 configuration Layer 2 mode is the simplest to configure: in many cases, you don’t need any protocol-specific configuration, only IP addresses. Layer 2 mode does not require the IPs to be bound to the network interfaces of your worker nodes. It works by responding to ARP requests on your local network directly, to give the machine’s MAC address to clients. For example, the following configuration gives MetalLB control over IPs from 192.168.1.240 to 192.168.1.250, and configures Layer 2 mode: apiVersion:v1kind:ConfigMapmetadata:namespace:metallb-systemname:configdata:config:|address-pools: - name: default protocol: layer2 addresses: - 192.168.1.240-192.168.1.250 ","date":"2019-10-16","objectID":"/posts/k8s/metallb-v0.8.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:4:1","tags":null,"title":"MetalLB-v0.8.1-官方文档阅读笔记","uri":"/posts/k8s/metallb-v0.8.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"BGP configuration For a basic configuration featuring one BGP router and one IP address range, you need 4 pieces of information: The router IP address that MetalLB should connect to, The router’s AS number, The AS number MetalLB should use, An IP address range expressed as a CIDR prefix. As an example, if you want to give MetalLB the range 192.168.10.0/24 and AS number 64500, and connect it to a router at 10.0.0.1 with AS number 64501, your configuration will look like: apiVersion:v1kind:ConfigMapmetadata:namespace:metallb-systemname:configdata:config:|peers: - peer-address: 10.0.0.1 peer-asn: 64501 my-asn: 64500 address-pools: - name: default protocol: bgp addresses: - 192.168.10.0/24 Advertisement configuration By default, BGP mode advertises each allocated IP to the configured peers with no additional BGP attributes. The peer router(s) will receive one /32 route for each service IP, with the BGP localpref set to zero and no BGP communities. You can configure more elaborate advertisements by adding a bgp-advertisements section that lists one or more custom advertisements. In addition to specifying localpref and communities, you can use this to advertise aggregate routes. The aggregation-length advertisement option lets you “roll up” the /32s into a larger prefix. Combined with multiple advertisement configurations, this lets you create elaborate advertisements that interoperate with the rest of your BGP network. For example, let’s say you have a leased /24 of public IP space, and you’ve allocated it to MetalLB. By default, MetalLB will advertise each IP as a /32, but your transit provider rejects routes more specific than /24. So, you need to somehow advertise a /24 to your transit provider, but still have the ability to do per-IP routing internally. Here’s a configuration that implements this: apiVersion:v1kind:ConfigMapmetadata:namespace:metallb-systemname:configdata:config:|peers: - peer-address: 10.0.0.1 peer-asn: 64501 my-asn: 64500 address-pools: - name: default protocol: bgp addresses: - 198.51.100.0/24 bgp-advertisements: - aggregation-length: 32 localpref: 100 communities: - no-advertise - aggregation-length: 24 bgp-communities: no-advertise: 65535:65282 With this configuration, if we create a service with IP 198.51.100.10, the BGP peer(s) will receive two routes: 198.51.100.10/32, with localpref=100 and the no-advertise community, which tells the peer router(s) that they can use this route, but they shouldn’t tell anyone else about it. 198.51.100.0/24, with no custom attributes. With this configuration, the peer(s) will propagate(传播) the 198.51.100.0/24 route to your transit provider, but once traffic shows up locally, the 198.51.100.10/32 route will be used to forward into your cluster. As you define more services, the router will receive one “local” /32 for each of them, as well as the covering /24. Each service you define “generates” the /24 route, but MetalLB deduplicates them all down to one BGP advertisement before talking to its peers. The above configuration also showcases the bgp-communities configuration section, which lets you define readable names for BGP communities that you can reuse in your advertisement configurations. This is completely optional, you could just specify 65535:65281 directly in the configuration of the /24 if you prefer. Limiting peers to certain nodes By default, every node in the cluster connects to all the peers listed in the configuration. In more advanced cluster topologies, you may want each node to connect to different routers. For example, if you have a “rack and spine” network topology, you likely want each machine to peer with its top-of-rack router, but not the routers in other racks. graph BT subgraph \"node1\" metallbA(\"MetalLB\u003cbr\u003eSpeaker\") calicoA(\"Calico\") end subgraph \"node2\" calicoB(\"Calico\") metallbB(\"MetalLB\u003cbr\u003eSpeaker\") end subgraph Router torA(\"Router VRF 1\") torB(\"Router VRF 2\") end calicoA--\u003etorA calicoB--\u003etorA metallbA--\u003etorB metallbB--\u003etorB torB-. \"Careful rout","date":"2019-10-16","objectID":"/posts/k8s/metallb-v0.8.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:4:2","tags":null,"title":"MetalLB-v0.8.1-官方文档阅读笔记","uri":"/posts/k8s/metallb-v0.8.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Advanced address pool configuration Controlling automatic address allocation In some environments, you’ll have some large address pools of “cheap” IPs (e.g. RFC1918), and some smaller pools of “expensive” IPs (e.g. leased public IPv4 addresses). By default, MetalLB will allocate IPs from any configured address pool with free addresses. This might end up using “expensive” addresses for services that don’t require it. To prevent this behaviour you can disable automatic allocation for a pool by setting the auto-assign flag to false: # Rest of config omitted for brevityaddress-pools:- name:cheapprotocol:bgpaddresses:- 192.168.144.0/20- name:expensiveprotocol:bgpaddresses:- 42.176.25.64/30auto-assign:false Addresses can still be specifically allocated from the “expensive” pool with the methods described in the usage section. Handling buggy networks Some old consumer network equipment mistakenly blocks IP addresses ending in .0 and .255, because of misguided smurf protection. If you encounter this issue with your users or networks, you can set avoid-buggy-ips: true on an address pool to mark .0 and .255 addresses as unusable. ","date":"2019-10-16","objectID":"/posts/k8s/metallb-v0.8.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:4:3","tags":null,"title":"MetalLB-v0.8.1-官方文档阅读笔记","uri":"/posts/k8s/metallb-v0.8.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Issues with Calico Simple setups with Calico don’t require anything special, you can just install and configure MetalLB as usual and enjoy. However, if you are using Calico’s external BGP peering capability to advertise your cluster prefixes over BGP, and also want to use BGP in MetalLB, you will need to jump through some hoops. ","date":"2019-10-16","objectID":"/posts/k8s/metallb-v0.8.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:4:4","tags":null,"title":"MetalLB-v0.8.1-官方文档阅读笔记","uri":"/posts/k8s/metallb-v0.8.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"The problem BGP only allows one session to be established per pair of nodes. So, if Calico has a session established with your BGP router, MetalLB cannot establish its own session – it’ll get rejected as a duplicate by BGP’s conflict resolution algorithm. graph BT subgraph \"node1\" metallbA calicoA end subgraph \"node2\" metallbB calicoB end metallbA(MetalLB\u003cbr\u003espeaker)-. \"LB routes\u003cbr\u003e(doesn't work)\" .-\u003erouter(BGP Router) calicoA(\"Calico\")-- Cluster routes --\u003erouter metallbB(MetalLB\u003cbr\u003espeaker)-. \"LB routes\u003cbr\u003e(doesn't work)\" .-\u003erouter calicoB(Calico)-- Cluster routes --\u003erouter Unfortunately, Calico does not currently provide the extension points we would need to make MetalLB coexist peacefully. There are bugs filed with Calico to add these extension points, but in the meantime, we can only offer some hacky workarounds. Workaround: Peer with spine routers If you are deploying to a cluster using a traditional “rack and spine” router architecture, you can work around the limitation imposed by BGP with some clever choice of peering. Let’s start with the network architecture, and see how we can add in MetalLB: graph BT subgraph \"node1\" metallbA(\"MetalLB\u003cbr\u003eSpeaker\") calicoA end subgraph \"node2\" calicoB metallbB(\"MetalLB\u003cbr\u003eSpeaker\") end subgraph \"node3\" metallbC(\"MetalLB\u003cbr\u003eSpeaker\") calicoC end subgraph \"node4\" calicoD metallbD(\"MetalLB\u003cbr\u003eSpeaker\") end calicoA(\"Calico\")--\u003etorA(ToR Router) calicoB(\"Calico\")--\u003etorA calicoC(\"Calico\")--\u003etorB(ToR Router) calicoD(\"Calico\")--\u003etorB torA--\u003espine(Spine Router) torB--\u003espine(Spine Router) In this architecture, we have 4 machines in our Kubernetes cluster, spread across 2 racks. Each rack has a top-of-rack (ToR) router, and both ToRs connect to an upstream “spine” router. The arrows represent BGP peering sessions: Calico has been configured to not automatically mesh with itself, but to instead peer with the ToRs. The ToRs in turn peer with the spine, which propagates routes throughout the cluster. Ideally, we would like MetalLB to connect to the ToRs in the same way that Calico does. However, Calico is already “consuming” the one allowed BGP session between machine and ToR. The alternative is to make MetalLB peer with the spine router(s): graph BT subgraph \"node1\" metallbA(\"MetalLB\u003cbr\u003eSpeaker\") calicoA end subgraph \"node2\" calicoB metallbB(\"MetalLB\u003cbr\u003eSpeaker\") end subgraph \"node3\" metallbC(\"MetalLB\u003cbr\u003eSpeaker\") calicoC end subgraph \"node4\" calicoD metallbD(\"MetalLB\u003cbr\u003eSpeaker\") end calicoA(\"Calico\")--\u003etorA(ToR Router) calicoB(\"Calico\")--\u003etorA calicoC(\"Calico\")--\u003etorB(ToR Router) calicoD(\"Calico\")--\u003etorB torA--\u003espine(Spine Router) torB--\u003espine(Spine Router) metallbA--\u003espine metallbB--\u003espine metallbC--\u003espine metallbD--\u003espine Properly configured, the spine can redistribute MetalLB’s routes to anyone that needs them. And, because there are no preexisting BGP sessions between the machines and the spine, there is no conflict between Calico and MetalLB. The downside of this option is additional configuration complexity, and a loss of scalability: instead of scaling the number of spine BGP sessions by the number of racks in your cluster, you’re once again scaling by the total number of machines. In some deployments, this may not be acceptable. In large clusters, another compromise might be to dedicate only certain racks to externally facing services: constrain the MetalLB speaker daemonset to schedule only on those racks, and either use the “Cluster” externalTrafficPolicy, or also constrain the pods of the externally facing services to run on those racks. graph BT subgraph \"node1\" metallbA(\"MetalLB\u003cbr\u003eSpeaker\") calicoA end subgraph \"node2\" calicoB end calicoA(\"Calico\")--\u003etorA(ToR Router) calicoB(\"Calico\")--\u003etorB(ToR Router) torA--\u003espine(Spine Router) torB--\u003espine(Spine Router) metallbA--\u003espine Workaround: Router VRFs If your networking hardware supports VRFs (Virtual Routing and Forwarding), you may be able to “split” your router in two, and peer Calico and MetalLB to separate halves of the same rout","date":"2019-10-16","objectID":"/posts/k8s/metallb-v0.8.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:4:5","tags":null,"title":"MetalLB-v0.8.1-官方文档阅读笔记","uri":"/posts/k8s/metallb-v0.8.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Usage Once MetalLB is installed and configured, to expose a service externally, simply create it with spec.type set to LoadBalancer, and MetalLB will do the rest. MetalLB attaches informational events to the services that it’s controlling. If your LoadBalancer is misbehaving, run kubectl describe service \u003cservice name\u003e and check the event log. ","date":"2019-10-16","objectID":"/posts/k8s/metallb-v0.8.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:5:0","tags":null,"title":"MetalLB-v0.8.1-官方文档阅读笔记","uri":"/posts/k8s/metallb-v0.8.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Requesting specific IPs MetalLB respects the spec.loadBalancerIP parameter, so if you want your service to be set up with a specific address, you can request it by setting that parameter. If MetalLB does not own the requested address, or if the address is already in use by another service, assignment will fail and MetalLB will log a warning event visible in kubectl describe service \u003cservice name\u003e. MetalLB also supports requesting a specific address pool, if you want a certain kind of address but don’t care which one exactly. To request assignment from a specific pool, add the metallb.universe.tf/address-pool annotation to your service, with the name of the address pool as the annotation value. For example: apiVersion:v1kind:Servicemetadata:name:nginxannotations:metallb.universe.tf/address-pool:production-public-ipsspec:ports:- port:80targetPort:80selector:app:nginxtype:LoadBalancer ","date":"2019-10-16","objectID":"/posts/k8s/metallb-v0.8.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:5:1","tags":null,"title":"MetalLB-v0.8.1-官方文档阅读笔记","uri":"/posts/k8s/metallb-v0.8.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Traffic policies MetalLB understands and respects the service’s externalTrafficPolicy option, and implements different announcements modes depending on the policy and announcement protocol you select. Layer2 When announcing in layer2 mode, one node in your cluster will attract traffic for the service IP. From there, the behavior depends on the selected traffic policy. “Cluster” traffic policy With the default Cluster traffic policy, kube-proxy on the node that received the traffic does load-balancing, and distributes the traffic to all the pods in your service. This policy results in uniform traffic distribution across all pods in the service. However, kube-proxy will obscure the source IP address of the connection when it does load-balancing, so your pod logs will show that external traffic appears to be coming from the service’s leader node. “Local” traffic policy With the Local traffic policy, kube-proxy on the node that received the traffic sends it only to the service’s pod(s) that are on the same node. There is no “horizontal” traffic flow between nodes. Because kube-proxy doesn’t need to send traffic between cluster nodes, your pods can see the real source IP address of incoming connections. The downside of this policy is that incoming traffic only goes to some pods in the service. Pods that aren’t on the current leader node receive no traffic, they are just there as replicas in case a failover is needed. BGP When announcing over BGP, MetalLB respects(遵守) the service’s externalTrafficPolicy option, and implements two different announcement modes depending on what policy you select. If you’re familiar with Google Cloud’s Kubernetes load balancers, you can probably skip this section: MetalLB’s behaviors and tradeoffs(权衡) are identical(相同). “Cluster” traffic policy With the default Cluster traffic policy, every node in your cluster will attract traffic for the service IP. On each node, the traffic is subjected to a second layer of load-balancing (provided by kube-proxy), which directs the traffic to individual pods. This policy results in uniform(均匀的) traffic distribution across all nodes in your cluster, and across all pods in your service. However, it results in two layers of load-balancing (one at the BGP router, one at kube-proxy on the nodes), which can cause inefficient(低效的) traffic flows. For example, a particular user’s connection might be sent to node A by the BGP router, but then node A decides to send that connection to a pod running on node B. The other downside of the “Cluster” policy is that kube-proxy will obscure(朦胧) the source IP address of the connection when it does its load-balancing, so your pod logs will show that external traffic appears to be coming from your cluster’s nodes. “Local” traffic policy With the Local traffic policy, nodes will only attract traffic if they are running one or more of the service’s pods locally. The BGP routers will load-balance incoming traffic only across those nodes that are currently hosting the service. On each node, the traffic is forwarded only to local pods by kube-proxy, there is no “horizontal” traffic flow between nodes. This policy provides the most efficient flow of traffic to your service. Furthermore, because kube-proxy doesn’t need to send traffic between cluster nodes, your pods can see the real source IP address of incoming connections. The downside of this policy is that it treats each cluster node as one “unit” of load-balancing, regardless of how many of the service’s pods are running on that node. This may result in traffic imbalances to your pods. For example, if your service has 2 pods running on node A and one pod running on node B, the Local traffic policy will send 50% of the service’s traffic to each node. Node A will split the traffic it receives evenly between its two pods, so the final per-pod load distribution is 25% for each of node A’s pods, and 50% for node B’s pod. In contrast, if you used the Cluster traffic policy, each pod would recei","date":"2019-10-16","objectID":"/posts/k8s/metallb-v0.8.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:5:2","tags":null,"title":"MetalLB-v0.8.1-官方文档阅读笔记","uri":"/posts/k8s/metallb-v0.8.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"IP address sharing By default, Services do not share IP addresses. If you have a need to colocate services on a single IP, you can enable selective IP sharing by adding the metallb.universe.tf/allow-shared-ip annotation to services. The value of the annotation is a “sharing key.” Services can share an IP address under the following conditions: They both have the same sharing key. They request the use of different ports (e.g. tcp/80 for one and tcp/443 for the other). They both use the Cluster external traffic policy, or they both point to the exact same set of pods (i.e. the pod selectors are identical). If these conditions are satisfied, MetalLB may colocate the two services on the same IP, but does not have to. If you want to ensure that they share a specific address, use the spec.loadBalancerIP functionality described above. There are two main reasons to colocate services in this fashion: to work around a Kubernetes limitation, and to work with limited IP addresses. Kubernetes does not currently allow multiprotocol LoadBalancer services. This would normally make it impossible to run services like DNS, because they have to listen on both TCP and UDP. To work around this limitation of Kubernetes with MetalLB, create two services (one for TCP, one for UDP), both with the same pod selector. Then, give them the same sharing key and spec.loadBalancerIP to colocate the TCP and UDP serving ports on the same IP address. The second reason is much simpler: if you have more services than available IP addresses, and you can’t or don’t want to get more addresses, the only alternative is to colocate multiple services per IP address. ","date":"2019-10-16","objectID":"/posts/k8s/metallb-v0.8.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:5:3","tags":null,"title":"MetalLB-v0.8.1-官方文档阅读笔记","uri":"/posts/k8s/metallb-v0.8.1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"部署k8s集群 参考： https://github.com/opsnull/follow-me-install-kubernetes-cluster https://zhangguanzhang.github.io/2019/03/03/kubernetes-1-13-4/ 集群信息： 主机名 IP 系统版本 角色 k8s-m1 192.168.15.5 CentOS Linux release 7.6.1810 master+worker k8s-m2 192.168.15.6 CentOS Linux release 7.6.1810 master+worker k8s-m3 192.168.15.7 CentOS Linux release 7.6.1810 master+worker c48-1 192.168.15.161 CentOS Linux release 7.6.1810 worker c48-2 192.168.15.143 CentOS Linux release 7.6.1810 worker 网络信息： 名称 地址 Cluster IP CIDR 172.30.0.0/16 Service Cluster IP CIDR 10.96.0.0/12 Service DNS IP 10.96.0.10 DNS DN cluster.local ","date":"2019-10-13","objectID":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/:1:0","tags":["docker","k8s","devops"],"title":"k8s v1.15.0 二进制方式部署","uri":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/"},{"categories":null,"content":"准备工作 所有主机彼此网络互通 所有主机的hosts文件中添加主机名与IP的映射。 执行后，在 k8s-m1 上查看，结果如下所示： [root@k8s-m1 ~]# cat /etc/hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 192.168.15.5 k8s-m1 192.168.15.6 k8s-m2 192.168.15.7 k8s-m3 192.168.15.161 c48-1 192.168.15.143 c48-2 执行后，在 k8s-m1 上查看，结果如下所示： [root@k8s-m1 ~]# cat /etc/hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 192.168.15.5 k8s-m1 192.168.15.6 k8s-m2 192.168.15.7 k8s-m3 192.168.15.161 c48-1 192.168.15.143 c48-2 为方便部署，在 k8s-m1 上配置无密码 ssh 登录其他节点。 方法一：在 k8s-m1 上执行如下命令： ssh-keygen -t rsa ssh-copy-id root@k8s-1 ssh-copy-id root@k8s-2 ssh-copy-id root@k8s-3 ssh-copy-id root@c48-1 ssh-copy-id root@c48-2 方法二：在 k8s-m1 上安装sshpass，然后通过配置别名来让 ssh 和 scp 不输入密码，其中 123456 为所有主机的密码。 yum install -y sshpass alias ssh='sshpass -p 123456 ssh -o StrictHostKeyChecking=no' alias scp='sshpass -p 123456 scp -o StrictHostKeyChecking=no' 所有主机关闭防火墙和 SELinux 所有主机关闭防火墙和 SELinux，否则后续挂载目录时可能报错：Permission denied。 systemctl disable --now firewalld NetworkManager setenforce 0 sed -ri '/^[^#]*SELINUX=/s#=.+$#=disabled#' /etc/selinux/config 关闭 dnsmasq linux 系统开启了 dnsmasq 后（如GUI环境），将系统 DNS Server 设置为127.0.0.1，这会导致 docker 容器无法解析域名，需要关闭它。（最小安装的 centos7 系统默认不启动 dnsmasq ） systemctl disable --now dnsmasq 关闭 swap 分区 Kubernetes v1.8+要求关闭系统Swap，若不关闭则需要修改kubelet设定参数（–fail-swap-on设置为false来忽略swap on），在所有主机上使用如下指令关闭swap并注释掉/etc/fstab中swap的行。 swapoff -a \u0026\u0026 sysctl -w vm.swappiness=0 sed -ri '/^[^#]*swap/s@^@#@' /etc/fstab 打开 swap 分区 ：执行下 sysctl -w vm.swappiness=1 然后编辑 vi /etc/fstab 去掉注释 在执行下这个 swapon -a 升级内核（可选） 更新系统，不升级内核，执行如下命令： yum install -y epel-release yum install -y wget git jq psmisc socat yum update -y reboot 更新系统，升级内核 更新系统 yum install -y epel-release yum install -y wget git jq psmisc socat yum update -y --exclude=kernel* perl 是内核的依赖包，如下命令检测是否存在 perl，如果不存在则安装 [ ! -f /usr/bin/perl ] \u0026\u0026 yum install perl -y 升级内核需要使用 elrepo 的 yum 源，首先我们导入 elrepo 的 key 并安装 elrepo 源 rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm 查看可用的内核 yum --disablerepo=\"*\" --enablerepo=\"elrepo-kernel\" list available --showduplicates 在 yum 的 elrepo 源中， mainline 为最新版本的内核， ipvs 依赖 nf_conntrack_ipv4 内核模块， 4.19即后续的内核版本将该内核模块改名为 nf_conntrack。 下面的链接可以下载到其他的归档版本： ubuntu http://kernel.ubuntu.com/~kernel-ppa/mainline/ RHEL http://mirror.rc.usf.edu/compute_lock/elrepo/kernel/el7/x86_64/RPMS/ 下面是 ml 的内核和上面归档内核版本任选其一的安装方法： 自选内核版本安装 export Kernel_Version=4.18.9-1 wget http://mirror.rc.usf.edu/compute_lock/elrepo/kernel/el7/x86_64/RPMS/kernel-ml{,-devel}-${Kernel_Version}.el7.elrepo.x86_64.rpm yum localinstall -y kernel-ml* 最新内核版本安装 yum --disablerepo=\"*\" --enablerepo=\"elrepo-kernel\" list available --showduplicates | grep -Po '^kernel-ml.x86_64\\s+\\K\\S+(?=.el7)' yum --disablerepo=\"*\" --enablerepo=elrepo-kernel install -y kernel-ml{,-devel} 修改内核启动顺序，默认启动的顺序应该为 1，升级后内核是往前插入，为 0 grub2-set-default 0 \u0026\u0026 grub2-mkconfig -o /etc/grub2.cfg 使用下面的命令确认下启动的默认内核是否指向上面安装的内核 grubby --default-kernel docker官方的内核检查脚本建议（RHEL7/CentOS7: User namespaces disabled; add ‘user_namespace.enable=1’ to boot command line），使用下面命令开启 grubby --args=\"user_namespace.enable=1\" --update-kernel=\"$(grubby --default-kernel)\" 重启加载新内核 reboot 所有主机安装 ipvs centos: yum install -y ipvsadm ipset sysstat conntrack libseccomp ubuntu sudo apt-get install -y wget git conntrack ipvsadm ipset jq sysstat curl iptables libseccomp 所有主机配置开机加载的内核模块 每台主机上执行如下命令： :\u003e /etc/modules-load.d/ipvs.conf module=( ip_vs ip_vs_rr ip_vs_wrr ip_vs_sh nf_conntrack br_netfilter rbd ) for kernel_module in ${module[@]};do /sbin/modinfo -F filename $kernel_module |\u0026 grep -qv ERROR \u0026\u0026 echo $kernel_module \u003e\u003e /etc/modules-load.d/ipvs.conf || : done systemctl enable --now systemd-modules-load.service 执行后生成的文件内容如下所示： [root@k8s-m1 ~]# cat /etc/modules-load.d/ipvs.conf ip_vs ip_vs_rr ip_vs","date":"2019-10-13","objectID":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/:1:1","tags":["docker","k8s","devops"],"title":"k8s v1.15.0 二进制方式部署","uri":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/"},{"categories":null,"content":"配置 NTP 时间同步 略 ","date":"2019-10-13","objectID":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/:1:2","tags":["docker","k8s","devops"],"title":"k8s v1.15.0 二进制方式部署","uri":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/"},{"categories":null,"content":"使用环境变量声明集群信息 将集群信息写进 env.sh 文件中，使用 source 命令将文件中的信息加载到环境变量中。 env.sh 内容如下所示： [root@k8s-m1 k8s]# pwd /home/k8s [root@k8s-m1 k8s]# cat env.sh # 声明集群成员信息 declare -A MasterArray otherMaster NodeArray AllNode Other MasterArray=(['k8s-m1']=192.168.15.5 ['k8s-m2']=192.168.15.6 ['k8s-m3']=192.168.15.7) otherMaster=(['k8s-m2']=192.168.15.6 ['k8s-m3']=192.168.15.7) NodeArray=(['k8s-m1']=192.168.15.5 ['k8s-m2']=192.168.15.6 ['k8s-m3']=192.168.15.7 ['c48-1']=192.168.15.161 ['c48-2']=192.168.15.143) # 下面复制上面的信息粘贴即可 AllNode=(['k8s-m1']=192.168.15.5 ['k8s-m2']=192.168.15.6 ['k8s-m3']=192.168.15.7 ['c48-1']=192.168.15.161 ['c48-2']=192.168.15.143) Other=(['k8s-m2']=192.168.15.6 ['k8s-m3']=192.168.15.7 ['c48-1']=192.168.15.161 ['c48-2']=192.168.15.143) export VIP=127.0.0.1 [ \"${#MasterArray[@]}\" -eq 1 ] \u0026\u0026 export VIP=${MasterArray[@]} || export API_PORT=8443 export KUBE_APISERVER=https://${VIP}:${API_PORT:=6443} #声明需要安装的的k8s版本 export KUBE_VERSION=v1.15.0 # 网卡名 export interface=ens33 # cni export CNI_URL=\"https://github.com/containernetworking/plugins/releases/download\" export CNI_VERSION=v0.7.5 # etcd export ETCD_version=v3.3.10 # DNS DN export CLUSTER_DNS_DOMAIN=cluster.local # DNS IP export CLUSTER_DNS_SVC_IP=10.96.0.10 注意：API Server的负载均衡和高可用的方案若使用 haproxy+keepalived ，需要将 VIP 指定为当前局域网内未使用的 ip 。 网卡名 interface 为 keepalived 绑定的网卡。 使用如下命令将文件中的信息加载到环境变量中 source env.sh ","date":"2019-10-13","objectID":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/:1:3","tags":["docker","k8s","devops"],"title":"k8s v1.15.0 二进制方式部署","uri":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/"},{"categories":null,"content":"分发二进制文件 在 k8s-m1 上通过 git 获取部署需要的二进制配置和yml文件 git clone https://github.com/msdemt/k8s-manual-files.git /home/k8s/k8s-manual-files -b v1.15.0 在 k8s-m1 上下载 k8s v1.15.0 server端的发布包，并将 k8s 的二进制文件复制到 /usr/local/bin 目录下 wget https://storage.googleapis.com/kubernetes-release/release/${KUBE_VERSION}/kubernetes-server-linux-amd64.tar.gz tar -zxvf kubernetes-server-linux-amd64.tar.gz --strip-components=3 -C /usr/local/bin kubernetes/server/bin/kube{let,ctl,-apiserver,-controller-manager,-scheduler,-proxy} 需要翻墙下载 也可以使用 k8s-manual-files 中提供的 v1.15.0 的二进制文件 cp /home/k8s/k8s-manual-files/master/bin/* /usr/local/bin/ 将 k8s master 相关的二进制组件分发到其它 master 上 for NODE in \"${!otherMaster[@]}\"; do echo \"--- $NODE${otherMaster[$NODE]}---\" scp /usr/local/bin/kube{let,ctl,-apiserver,-controller-manager,-scheduler,-proxy} ${otherMaster[$NODE]}:/usr/local/bin/ done 将 k8s worker 相关的二进制组件分发到其他的 worker 上 for NODE in \"${!NodeArray[@]}\"; do echo \"--- $NODE${NodeArray[$NODE]}---\" scp /usr/local/bin/kube{let,-proxy} ${NodeArray[$NODE]}:/usr/local/bin/ done 在 k8s-m1 上下载 kubernetes cni 二进制文件并分发到其他主机 mkdir -p /opt/cni/bin wget \"${CNI_URL}/${CNI_VERSION}/cni-plugins-amd64-${CNI_VERSION}.tgz\" tar -zxf cni-plugins-amd64-${CNI_VERSION}.tgz -C /opt/cni/bin # 分发cni文件 for NODE in \"${!Other[@]}\"; do echo \"--- $NODE${Other[$NODE]}---\" ssh ${Other[$NODE]} 'mkdir -p /opt/cni/bin' scp /opt/cni/bin/* ${Other[$NODE]}:/opt/cni/bin/ done ","date":"2019-10-13","objectID":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/:1:4","tags":["docker","k8s","devops"],"title":"k8s v1.15.0 二进制方式部署","uri":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/"},{"categories":null,"content":"建立集群 CA keys 与 Certificates 在这个部分，将需要产生多个元件的 Certificates，这包含 Etcd、 Kubernetes 元件等,并且每个集群都会有一个根数位凭证认证机构（Root Certificate Authority）被用在认证 API Server 与 Kubelet 端的凭证。 PS: 注意 CA JSON 中的 CN(Common Name) 与 O(Organization) 等内容是会影响 Kubernetes 元件认证的。 CN (Common Name): apiserver 会从证书中提取该字段作为请求的用户名 (User Name) O (Organization)： apiserver 会从证书中提取该字段作为请求用户所属的组 (Group) CA (Certificate Authority) 是自签名的根证书，用来签名后续创建的其它证书。 本文使用openssl创建所有证书。 准备 openssl 证书配置文件 将 IP 信息注入到 openssl.cnf 文件中 mkdir -p /etc/kubernetes/pki/etcd sed -i \"/IP.2/a IP.3 = $VIP\" /home/k8s/k8s-manual-files/pki/openssl.cnf sed -ri '/IP.3/r '\u003c( paste -d '' \u003c(seq -f 'IP.%g = ' 4 $[${#AllNode[@]}+3]) \u003c(xargs -n1\u003c\u003c\u003c${AllNode[@]} | sort) ) /home/k8s/k8s-manual-files/pki/openssl.cnf sed -ri '$r '\u003c( paste -d '' \u003c(seq -f 'IP.%g = ' 2 $[${#MasterArray[@]}+1]) \u003c(xargs -n1\u003c\u003c\u003c${MasterArray[@]} | sort) ) /home/k8s/k8s-manual-files/pki/openssl.cnf cp /home/k8s/k8s-manual-files/pki/openssl.cnf /etc/kubernetes/pki/ cd /etc/kubernetes/pki 生成证书 生成根证书 Path Default CN Description ca.crt/key kubernetes-ca Kubernetes general CA etcd/ca.crt/key etcd-ca For all etcd-related functions front-proxy-ca.crt/key kubernetes-front-proxy-ca For the front-end proxy kubernetes-ca openssl genrsa -out ca.key 2048 openssl req -x509 -new -nodes -key ca.key -config openssl.cnf -subj \"/CN=kubernetes-ca\" -extensions v3_ca -out ca.crt -days 10000 etcd-ca openssl genrsa -out etcd/ca.key 2048 openssl req -x509 -new -nodes -key etcd/ca.key -config openssl.cnf -subj \"/CN=etcd-ca\" -extensions v3_ca -out etcd/ca.crt -days 10000 front-proxy-ca openssl genrsa -out front-proxy-ca.key 2048 openssl req -x509 -new -nodes -key front-proxy-ca.key -config openssl.cnf -subj \"/CN=kubernetes-ca\" -extensions v3_ca -out front-proxy-ca.crt -days 10000 使用根证书签发证书 生成所有的证书信息 Default CN Parent CA O (in Subject) kind kube-etcd etcd-ca server, client kube-etcd-peer etcd-ca server, client kube-etcd-healthcheck-client etcd-ca client kube-apiserver-etcd-client etcd-ca system:masters client kube-apiserver kubernetes-ca server kube-apiserver-kubelet-client kubernetes-ca system:masters client front-proxy-client kubernetes-front-proxy-ca client 证书路径 Default CN recommend key path recommended cert path command key argument cert argument etcd-ca etcd/ca.crt kube-apiserver –etcd-cafile etcd-client apiserver-etcd-client.key apiserver-etcd-client.crt kube-apiserver –etcd-keyfile –etcd-cafile kubernetes-ca ca.crt kube-apiserver –client-ca-file kube-apiserver apiserver.key apiserver.crt kube-apiserver –tls-private-key-file –tls-cert-file apiserver-kubelet-client apiserver-kubelet-client.crt kube-apiserver –kubelet-client-certificate front-proxy-ca front-proxy-ca.crt kube-apiserver –requestheader-client-ca-file front-proxy-client front-proxy-client.key front-proxy-client.crt kube-apiserver –proxy-client-key-file –proxy-client-cert-file etcd-ca etcd/ca.crt etcd –trusted-ca-file, –peer-trusted-ca-file kube-etcd etcd/server.key etcd/server.crt etcd –key-file –cert-file kube-etcd-peer etcd/peer.key etcd/peer.crt etcd –peer-key-file –peer-cert-file etcd-ca etcd/ca.crt etcdctl -cacert kube-etcd-healthcheck-client etcd/healthcheck-client.key etcd/healthcheck-client.crt etcdctl –key –cert 生成证书 apiserver-etcd-client openssl genrsa -out apiserver-etcd-client.key 2048 openssl req -new -key apiserver-etcd-client.key -subj \"/CN=apiserver-etcd-client/O=system:masters\" -out apiserver-etcd-client.csr openssl x509 -in apiserver-etcd-client.csr -req -CA etcd/ca.crt -CAkey etcd/ca.key -CAcreateserial -extensions v3_req_etcd -extfile openssl.cnf -out apiserver-etcd-client.crt -days 10000 kube-etcd openssl genrsa -out etcd/server.key 2048 openssl req -new -key etcd/server.key -subj \"/CN=etcd-server\" -out etcd/server.csr openssl x509 -in etcd/server.csr -req -CA etcd/ca.crt -CAkey etcd/ca.key -CAcreateserial -extensions v3_req_etcd -extfile openssl.cnf -out etcd/server.crt -days 10000 kube-etcd-peer openssl genrsa -out etcd/peer.key 2048 openssl req -new -key etcd/peer.key -subj \"/CN=","date":"2019-10-13","objectID":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/:1:5","tags":["docker","k8s","devops"],"title":"k8s v1.15.0 二进制方式部署","uri":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/"},{"categories":null,"content":"安装 etcd etcd 是用来保存集群所有状态的 Key/Value 存储系统,所有 Kubernetes 组件会通过 API Server 来跟 Etcd 进行沟通从而保存或读取资源状态。 在 k8s 集群中的 master 节点上部署 etcd 集群。 在 k8s-m1 上下载 etcd 的发布包，并将二进制文件解压到 /usr/local/bin 目录下 wget https://github.com/etcd-io/etcd/releases/download/${ETCD_version}/etcd-${ETCD_version}-linux-amd64.tar.gz tar -zxvf etcd-${ETCD_version}-linux-amd64.tar.gz --strip-components=1 -C /usr/local/bin etcd-${ETCD_version}-linux-amd64/etcd{,ctl} 在 k8s-m1 上将 etcd 的二进制文件分发到其他 master 上 for NODE in \"${!otherMaster[@]}\"; do echo \"--- $NODE${otherMaster[$NODE]}---\" scp /usr/local/bin/etcd* ${otherMaster[$NODE]}:/usr/local/bin/ done 在 k8s-m1 上修改 etcd 配置文件,注入 etcd 集群 ip 到 yml 文件中 cd /home/k8s/k8s-manual-files/master/ etcd_servers=$( xargs -n1\u003c\u003c\u003c${MasterArray[@]} | sort | sed 's#^#https://#;s#$#:2379#;$s#\\n##' | paste -d, -s - ) etcd_initial_cluster=$( for i in ${!MasterArray[@]};do echo $i=https://${MasterArray[$i]}:2380; done | sort | paste -d, -s - ) sed -ri \"/initial-cluster:/s#'.+'#'${etcd_initial_cluster}'#\" etc/etcd/config.yml 参考: https://github.com/etcd-io/etcd/blob/master/etcd.conf.yml.sample 将相关文件分发到其他 master 上 for NODE in \"${!MasterArray[@]}\"; do echo \"--- $NODE${MasterArray[$NODE]}---\" ssh ${MasterArray[$NODE]} \"mkdir -p /etc/etcd /var/lib/etcd\" scp systemd/etcd.service ${MasterArray[$NODE]}:/usr/lib/systemd/system/etcd.service scp etc/etcd/config.yml ${MasterArray[$NODE]}:/etc/etcd/etcd.config.yml ssh ${MasterArray[$NODE]} \"sed -i \"s/{HOSTNAME}/$NODE/g\" /etc/etcd/etcd.config.yml\" ssh ${MasterArray[$NODE]} \"sed -i \"s/{PUBLIC_IP}/${MasterArray[$NODE]}/g\" /etc/etcd/etcd.config.yml\" ssh ${MasterArray[$NODE]} 'systemctl daemon-reload' done 在 k8s-m1 上启动所有 master 节点的 etcd for NODE in \"${!MasterArray[@]}\"; do echo \"--- $NODE${MasterArray[$NODE]}---\" ssh ${MasterArray[$NODE]} 'systemctl enable --now etcd' \u0026 done wait 输出到终端的时候多按几下回车直到等光标回到终端状态 etcd 进程首次启动时会等待其它节点的 etcd 加入集群，命令 systemctl start etcd 会卡住一段时间，为正常现象。 k8s-m1上执行下面命令验证 ETCD 集群状态 etcdctl \\ --cert-file /etc/kubernetes/pki/etcd/healthcheck-client.crt \\ --key-file /etc/kubernetes/pki/etcd/healthcheck-client.key \\ --ca-file /etc/kubernetes/pki/etcd/ca.crt \\ --endpoints $etcd_servers cluster-health 使用 etcd v3版 API 查看当前 etcd 中存在的 key ETCDCTL_API=3 \\ etcdctl \\ --cert /etc/kubernetes/pki/etcd/healthcheck-client.crt \\ --key /etc/kubernetes/pki/etcd/healthcheck-client.key \\ --cacert /etc/kubernetes/pki/etcd/ca.crt \\ --endpoints $etcd_servers get / --prefix --keys-only ","date":"2019-10-13","objectID":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/:1:6","tags":["docker","k8s","devops"],"title":"k8s v1.15.0 二进制方式部署","uri":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/"},{"categories":null,"content":"Kubernetes Master 本部分将说明如何建立与设定 Kubernetes Master 角色，过程中会部署以下元件： kubelet: 负责管理容器的生命周期，定期从 API Server 获取节点上的预期状态（如网络、存储等等配置）资源，并让对应的容器插件（CRI、CNI 等）来达成这个状态。任何 Kubernetes 节点（node）都会拥有这个元件。 关闭只读端口，在安全端口 10250 接收 https 请求，对请求进行认证和授权，拒绝匿名访问和非授权访问。 使用 kubeconfig 访问 apiserver 的安全端口 kube-apiserver: 以 REST APIs 提供 Kubernetes 资源的 CRUD，如授权、认证、存取控制与 API 注册等机制。 关闭非安全端口，在安全端口 6443 接收 https 请求 严格的认证和授权策略 （x509、token、RBAC） 开启 bootstrap token 认证，支持 kubelet TLS bootstrapping 使用 https 访问 kubelet、etcd，加密通信 kube-controller-manager: 通过核心控制循环（Core Control Loop）监听 Kubernetes API 的资源来维护集群的状态，这些资源会被不同的控制器所管理，如 Replication Controller、Namespace Controller 等等。而这些控制器会处理着自动扩展、滚动更新等等功能。 关闭非安全端口，在安全端口 10252 接收 https 请求。 使用 kubeconfig 访问 apiserver 的安全端口。 kube-scheduler: 负责将一个(或多个)容器依据调度策略分配到对应节点上让容器引擎(如 Docker)执行。而调度受到 QoS 要求、软硬性约束、亲和性(Affinity)等等因素影响。 HAProxy: 提供多个 API Server 的负载均衡(Load Balance),确保haproxy的端口负载到所有的apiserver的6443端口 Keepalived: 提供虚拟IP位址(VIP),来让vip落在可用的master主机上供所有组件都能访问到可用的master,结合haproxy能访问到master上的apiserver的6443端口 kube-nginx 使用 nginx 4 层透明代理功能实现 K8S 节点（ master 节点和 worker 节点）高可用访问 kube-apiserver 的策略。（等于 Haproxy + Keepalived） kube-nginx 方案和 haproxy+keepalived 方案选一个即可。 kube-nginx 方案需要在每一台主机上部署，而 haproxy+keepalived 方案只需要部署在 master 节点的主机上。 kube-nginx 方案不需要额外的 IP ，而 haproxy+keepalived 方案需要额外的一个当前局域网未使用的 IP 做为 VIP。 若使用 haproxy + keepalived 方案，则 master 节点的主机的网卡名需要相同。 本文档使用 kube-nginx 方案，haproxy + keepalived 方案只做介绍。 安装 haproxy + keepalived 在 k8s-m1 上执行如下命令，在 master 节点上安装 haproxy 和 keepalived for NODE in \"${!MasterArray[@]}\"; do echo \"--- $NODE${MasterArray[$NODE]}---\" ssh ${MasterArray[$NODE]} 'yum install haproxy keepalived -y' \u0026 done wait 如果没输出的话，多按几次 在 k8s-m1 上把相关配置文件修改和分发 cd /home/k8s/k8s-manual-files/master/etc # 修改haproxy.cfg配置文件 sed -i '$r '\u003c(paste \u003c( seq -f' server k8s-api-%g' ${#MasterArray[@]} ) \u003c( xargs -n1\u003c\u003c\u003c${MasterArray[@]} | sort | sed 's#$#:6443 check#')) haproxy/haproxy.cfg # 修改keepalived(网卡和VIP写进去,使用下面命令) sed -ri \"s#\\{\\{ VIP \\}\\}#${VIP}#\" keepalived/* sed -ri \"s#\\{\\{ interface \\}\\}#${interface}#\" keepalived/keepalived.conf sed -i '/unicast_peer/r '\u003c(xargs -n1\u003c\u003c\u003c${MasterArray[@]} | sort | sed 's#^#\\t#') keepalived/keepalived.conf # 分发文件 for NODE in \"${!MasterArray[@]}\"; do echo \"--- $NODE${MasterArray[$NODE]}---\" scp -r haproxy/ ${MasterArray[$NODE]}:/etc scp -r keepalived/ ${MasterArray[$NODE]}:/etc ssh ${MasterArray[$NODE]} 'systemctl enable --now haproxy keepalived' done 等待四五秒后，ping 下 vip 看看是否能通 ping $VIP 如果 vip ping 不通，就是 keepalived 未启动或启动失败，在每个 master 节点上 restart 下 keepalived 或者确认下配置文件 /etc/keepalived/keepalived.conf 里网卡名和 ip 是否注入成功。 for NODE in \"${!MasterArray[@]}\"; do echo \"--- $NODE${MasterArray[$NODE]}---\" ssh ${MasterArray[$NODE]} 'systemctl restart haproxy keepalived' done 在所有节点安装 kube-nginx 基于 nginx 代理的 kube-apiserver 高可用方案： 控制节点（master 节点）的 kube-controller-manager、kube-scheduler 是多实例部署，所以只要有一个实例正常，就可以保证高可用； 集群内的 Pod 使用 K8S 服务域名 kubernetes 访问 kube-apiserver， kube-dns 会自动解析出多个 kube-apiserver 节点的 IP，所以也是高可用的； 在每个节点起一个 nginx 进程，后端对接多个 apiserver 实例，nginx 对它们做健康检查和负载均衡； kubelet、kube-proxy、controller-manager、scheduler 通过本地的 nginx（监听 127.0.0.1）访问 kube-apiserver，从而实现 kube-apiserver 的高可用； 下载和编译 nginx ： 下载源码 cd /home/k8s wget http://nginx.org/download/nginx-1.15.3.tar.gz tar -xzvf nginx-1.15.3.tar.gz 配置编译参数 cd /home/k8s/nginx-1.15.3 mkdir nginx-prefix ./configure --with-stream --without-http --prefix=$(pwd)/nginx-prefix --without-http_uwsgi_module --without-http_scgi_module --without-http_fastcgi_module --with-stream：开启 4 层透明转发(TCP Proxy)功能； --without-xxx：关闭所有其他功能，这样生成的动态链接二进制程序依赖最小； 编译和安装 cd /home/k8s/nginx-1.15.3 make \u0026\u0026 make install 验证编译的 nginx cd /home/k8s/nginx-1.15.3 ./nginx-prefix/sbin/nginx -v 输出： nginx version: nginx/1.15.3 查看 nginx 动态链接的库： ldd ./nginx-prefix/sbin/nginx 输出： linux-vdso.so.1 =\u003e (0x00007ffc945e7000) libdl.so.2 =\u003e /lib64/libdl.so.2 (0x00007f4385072000) libpthread.so.0 =\u003e /lib64/libpthread.so.0 (0x00007f4384e56000) libc.so.6 =\u003e /lib64/libc.so.6 (0x00007f4384a89000) /lib64/ld-linux-x86-64.so.2 (0x00007f4385276000)","date":"2019-10-13","objectID":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/:1:7","tags":["docker","k8s","devops"],"title":"k8s v1.15.0 二进制方式部署","uri":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/"},{"categories":null,"content":"All Kubernetes Nodes 本部分将说明如何建立与设定 Kubernetes Node 角色，Node 是主要执行容器实例（Pod）的工作节点。 在开始部署前，先在 k8-m1 将需要用到的文件复制到所有其他节点上。 kubelet的配置选项官方建议大多数的参数写一个 yaml 里用 –config 去指定，详见： https://godoc.org/k8s.io/kubernetes/pkg/kubelet/apis/config#KubeletConfiguration for NODE in \"${!Other[@]}\"; do echo \"--- $NODE${Other[$NODE]}---\" ssh ${Other[$NODE]} \"mkdir -p /etc/kubernetes/pki /etc/kubernetes/manifests /var/lib/kubelet/\" for FILE in /etc/kubernetes/pki/ca.crt /etc/kubernetes/bootstrap.kubeconfig; do scp ${FILE} ${Other[$NODE]}:${FILE} done done 在 k8s-m1 节点分发 kubelet.service 文件和其他配置文件到每台主机上管理kubelet： cd /home/k8s/k8s-manual-files/ for NODE in \"${!AllNode[@]}\"; do echo \"--- $NODE${AllNode[$NODE]}---\" scp master/systemd/kubelet.service ${AllNode[$NODE]}:/lib/systemd/system/kubelet.service scp master/etc/kubelet/kubelet-conf.yml ${AllNode[$NODE]}:/etc/kubernetes/kubelet-conf.yml ssh ${AllNode[$NODE]} \"sed -ri '/0.0.0.0/s#\\S+\\$#${MasterArray[$NODE]}#' /etc/kubernetes/kubelet-conf.yml\" ssh ${AllNode[$NODE]} \"sed -ri '/127.0.0.1/s#\\S+\\$#${MasterArray[$NODE]}#' /etc/kubernetes/kubelet-conf.yml\" done 在 k8s-m1 上去启动每个 node 节点的 kubelet 服务: for NODE in \"${!AllNode[@]}\"; do echo \"--- $NODE${AllNode[$NODE]}---\" ssh ${AllNode[$NODE]} 'systemctl enable --now kubelet.service' done 在任意一台master节点并通过简单指令验证集群状态： [root@k8s-m1 k8s-manual-files]# kubectl get nodes NAME STATUS ROLES AGE VERSION c48-1 Ready \u003cnone\u003e 19s v1.15.0 c48-2 Ready \u003cnone\u003e 20s v1.15.0 k8s-m1 Ready \u003cnone\u003e 32s v1.15.0 k8s-m2 Ready \u003cnone\u003e 31s v1.15.0 k8s-m3 Ready \u003cnone\u003e 31s v1.15.0 [root@k8s-m1 k8s-manual-files]# kubectl get csr NAME AGE REQUESTOR CONDITION csr-cwsqq 70s system:bootstrap:e006fa Approved,Issued csr-dqw9s 86s system:bootstrap:e006fa Approved,Issued csr-lchwz 69s system:bootstrap:e006fa Approved,Issued csr-pg4tq 86s system:bootstrap:e006fa Approved,Issued csr-s2ts5 86s system:bootstrap:e006fa Approved,Issued kubelet 中的 –allow-privileged 参数在 1.15.0 版本已被废弃，详见：https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.15.md#v1154 若不想让（没有声明容忍该污点的）pod 跑在 master 上，则执行如下命令给 master 节点加上污点 traint kubectl taint nodes ${!MasterArray[@]} node-role.kubernetes.io/master=\"\":NoSchedule 容忍与污点详见：https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/ 为 node 打上标签，标记其为 master 或 worker （master 节点也作为 worker节点） kubectl label node ${!MasterArray[@]} node-role.kubernetes.io/master=\"\" kubectl label node ${!NodeArray[@]} node-role.kubernetes.io/worker=worker 查看当前集群 node 的状态 [root@k8s-m1 k8s-manual-files]# kubectl get nodes NAME STATUS ROLES AGE VERSION c48-1 Ready worker 2m47s v1.15.0 c48-2 Ready worker 2m48s v1.15.0 k8s-m1 Ready master,worker 3m v1.15.0 k8s-m2 Ready master,worker 2m59s v1.15.0 k8s-m3 Ready master,worker 2m59s v1.15.0 kubelet 的数据存储目录默认为 /var/lib/kubelet，为了避免 /var 目录所在的分区空间不足，可以通过 –root-dir 参数（使用 kubelet –help 查看）修改 kubelet 的默认数据存储目录，但是由于 rook-ceph 等插件默认读取的是 /var/lib/kubelet 目录，所以方便起见，本文使用软链接的方式修改 kubelet 的存储目录。 systemctl stop kubelet rm -rf /var/lib/kubelet mkdir -p /home/data/kubelet ln -s /home/data/kubelet /var/lib/kubelet 上述命令将 /home/data/kubelet 软链接到 /var/lib/kubelet。 若删除 /var/lib/kubelet 时报错，提示挂载问题，可以手动使用 umount 命令卸载正在挂载的目录。 ","date":"2019-10-13","objectID":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/:1:8","tags":["docker","k8s","devops"],"title":"k8s v1.15.0 二进制方式部署","uri":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/"},{"categories":null,"content":"Kubernetes Core Addons ","date":"2019-10-13","objectID":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/:2:0","tags":["docker","k8s","devops"],"title":"k8s v1.15.0 二进制方式部署","uri":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/"},{"categories":null,"content":"kube-proxy Kube-proxy 是实现 Service 的关键插件，kube-proxy 会在运行在每台 worker 节点，通过监听 API Server 的 Service 与 Endpoint 资源物件的改变，来依据变化执行 iptables 来实现网路的转发。 本文档使用 ipvs 模式部署 kube-proxy。提供了二进制部署方式和 daemonset 部署方式。建议使用 daemonset 方式进行部署。 二进制方式部署 kube-proxy 在 k8s-m1 上新建一个 kube-proxy 的 serviceaccount： kubectl -n kube-system create serviceaccount kube-proxy 将 kube-proxy 的 serviceaccount 绑定到 clusterrole system:node-proxier 以运行 RBAC ： kubectl create clusterrolebinding kubeadm:kube-proxy \\ --clusterrole system:node-proxier \\ --serviceaccount kube-system:kube-proxy 创建 kube-proxy 的 kubeconfig CLUSTER_NAME=\"kubernetes\" KUBE_CONFIG=\"kube-proxy.kubeconfig\" SECRET=$(kubectl -n kube-system get sa/kube-proxy \\ --output=jsonpath='{.secrets[0].name}') JWT_TOKEN=$(kubectl -n kube-system get secret/$SECRET \\ --output=jsonpath='{.data.token}' | base64 -d) kubectl config set-cluster ${CLUSTER_NAME} \\ --certificate-authority=/etc/kubernetes/pki/ca.crt \\ --embed-certs=true \\ --server=${KUBE_APISERVER} \\ --kubeconfig=/etc/kubernetes/${KUBE_CONFIG} kubectl config set-context ${CLUSTER_NAME} \\ --cluster=${CLUSTER_NAME} \\ --user=${CLUSTER_NAME} \\ --kubeconfig=/etc/kubernetes/${KUBE_CONFIG} kubectl config set-credentials ${CLUSTER_NAME} \\ --token=${JWT_TOKEN} \\ --kubeconfig=/etc/kubernetes/${KUBE_CONFIG} kubectl config use-context ${CLUSTER_NAME} --kubeconfig=/etc/kubernetes/${KUBE_CONFIG} kubectl config view --kubeconfig=/etc/kubernetes/${KUBE_CONFIG} 在 k8s-m1 上将相关文件分发到所有 worker 节点 cd /home/k8s/k8s-manual-files/ for NODE in \"${!Other[@]}\"; do echo \"--- $NODE${Other[$NODE]}---\" scp /etc/kubernetes/kube-proxy.kubeconfig ${Other[$NODE]}:/etc/kubernetes/kube-proxy.kubeconfig done for NODE in \"${!AllNode[@]}\"; do echo \"--- $NODE${AllNode[$NODE]}---\" scp addons/kube-proxy/kube-proxy.conf ${AllNode[$NODE]}:/etc/kubernetes/kube-proxy.conf scp addons/kube-proxy/kube-proxy.service ${AllNode[$NODE]}:/usr/lib/systemd/system/kube-proxy.service ssh ${AllNode[$NODE]} \"sed -ri '/0.0.0.0/s#\\S+\\$#${MasterArray[$NODE]}#' /etc/kubernetes/kube-proxy.conf\" done 在 k8s-m1 上启动所有 worker 节点的 kube-proxy 服务 for NODE in \"${!AllNode[@]}\"; do echo \"--- $NODE${AllNode[$NODE]}---\" ssh ${AllNode[$NODE]} 'systemctl enable --now kube-proxy' done daemonset 方式部署 kube-proxy 在 k8s-m1 上执行如下命令： cd /home/k8s/k8s-manual-files source /home/k8s/env.sh # 注入变量 sed -ri \"/server:/s#(: ).+#\\1${KUBE_APISERVER}#\" addons/kube-proxy/kube-proxy.yml sed -ri \"/image:.+kube-proxy/s#:[^:]+\\$#:$KUBE_VERSION#\" addons/kube-proxy/kube-proxy.yml kubectl apply -f addons/kube-proxy/kube-proxy.yml 输出内容如下： serviceaccount \"kube-proxy\" created clusterrolebinding.rbac.authorization.k8s.io \"system:kube-proxy\" created configmap \"kube-proxy\" created daemonset.apps \"kube-proxy\" created 查看当前 kube-proxy pod 的状态 [root@k8s-m1 k8s-manual-files]# kubectl -n kube-system get po -l k8s-app=kube-proxy -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES kube-proxy-52hkz 1/1 Running 0 54s 192.168.15.161 c48-1 \u003cnone\u003e \u003cnone\u003e kube-proxy-54f8d 1/1 Running 0 54s 192.168.15.143 c48-2 \u003cnone\u003e \u003cnone\u003e kube-proxy-757j7 1/1 Running 0 54s 192.168.15.6 k8s-m2 \u003cnone\u003e \u003cnone\u003e kube-proxy-c2ht9 1/1 Running 0 54s 192.168.15.7 k8s-m3 \u003cnone\u003e \u003cnone\u003e kube-proxy-fdx48 1/1 Running 0 54s 192.168.15.5 k8s-m1 \u003cnone\u003e \u003cnone\u003e 通过 ipvsadm 查看当前 proxy 的规则 [root@k8s-m1 k8s-manual-files]# ipvsadm -ln IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -\u003e RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 10.96.0.1:443 rr -\u003e 192.168.15.5:6443 Masq 1 0 0 -\u003e 192.168.15.6:6443 Masq 1 0 0 -\u003e 192.168.15.7:6443 Masq 1 0 0 确认使用的是 ipvs 模式 [root@k8s-m1 k8s-manual-files]# curl localhost:10249/proxyMode ipvs ","date":"2019-10-13","objectID":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/:2:1","tags":["docker","k8s","devops"],"title":"k8s v1.15.0 二进制方式部署","uri":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/"},{"categories":null,"content":"网络插件 flannel 或 calico Kubernetes 在默认情況下与 Docker 的网络有所不同。在 Kubernetes 中有四个问题是需要被解決的,分別为： 高耦合的容器到容器通信：通过 Pods 内 localhost 的來解決。 Pod 到 Pod 的通信：通过实现网络模型来解决。 Pod 到 Service 通信：由 Service objects 结合 kube-proxy 解決。 外部到 Service 通信：一样由 Service objects 结合 kube-proxy 解決。 而 Kubernetes 对于任何网络的实现都需要满足以下基本要求(除非是有意调整的网络分段策略)： 所有容器能够在沒有 NAT 的情況下与其他容器通信。 所有节点能够在沒有 NAT 情況下与所有容器通信(反之亦然)。 容器看到的 IP 与其他人看到的 IP 是一样的。 庆幸的是 Kubernetes 已经有非常多种的网络模型作为网络插件（Network Plugins）方式被实现，因此可以选用满足自己需求的网络功能来使用。另外 Kubernetes 中的网络插件有以下两种形式： CNI plugins：以 appc/CNI 标准规范所实现的网络，详细可以阅读 CNI Specification。 Kubenet plugin：使用 CNI plugins 的 bridge 与 host-local 来实现基本的 cbr0。这通常被用在公有云服务上的 Kubernetes 集群网络。 如果想了解如何选择可以如阅读 Chris Love 的 Choosing a CNI Network Provider for Kubernetes 文章。 flannel flannel 使用 vxlan 技术为各节点创建一个可以互通的 Pod 网络，使用的端口为 UDP 8472，需要开放该端口（如公有云 AWS 等）。 flannel 第一次启动时，从 etcd 获取 Pod 网段信息，为本节点分配一个未使用的 /24 段地址，然后创建 flannel.1（也可能是其它名称，如 flannel1 等） 接口。 查看使用的 flannel 容器 $ grep -Pom1 'image:\\s+\\K\\S+' addons/flannel/kube-flannel.yml quay.io/coreos/flannel:v0.11.0-amd64 使用 daemonset 创建 flannel，yaml 来源于官方，删除了非 amd64 的 daemonset 将环境变量中的 interface 配置到 yml 中，然后安装 flannel sed -ri \"s#\\{\\{ interface \\}\\}#${interface}#\" addons/flannel/kube-flannel.yml kubectl apply -f addons/flannel/kube-flannel.yml 检查 flannel 启动是否成功 $ kubectl -n kube-system get po -l k8s-app=flannel NAME READY STATUS RESTARTS AGE kube-flannel-ds-27jwl 2/2 Running 0 59s kube-flannel-ds-4fgv6 2/2 Running 0 59s kube-flannel-ds-mvrt7 2/2 Running 0 59s kube-flannel-ds-p2q9g 2/2 Running 0 59s kube-flannel-ds-zchsz 2/2 Running 0 59s calico Calico 是一款纯 Layer 3 的网络，其好处是它整合了各种云原生平台（Docker、Mesos 与 OpenStack 等），且 Calico 不采用 vSwitch，而是在每个 Kubernetes 节点使用 vRouter 功能，并通过 Linux Kernel 既有的 L3 forwarding 功能，而当资料中心复杂度增加时，Calico 也可以利用 BGP route reflector 来达成。 想了解 Calico 与传统 overlay networks 的差异，可以阅读 Difficulties with traditional overlay networks 文章 calico 官方安装指南 calico 官方关于使用 calico 提供策略和网络的安装方式（Installing Calico for policy and networking）有三种，根据数据存储类型（datastore type）和节点数量（number of nodes）进行划分： 使用 kubernetes api 存储数据 且节点数量少于50 使用 kubernetes api 存储数据，且节点数量大于50 使用 etcd 存储数据 本文使用 etcd 作为 calico 后端存储数据。 下载 calico 适用于 etcd 后端存储的清单文件 curl https://docs.projectcalico.org/v3.9/manifests/calico-etcd.yaml -o calico-etcd.yaml 如果你使用的 pod 的 CIDR 为 10.244.0.0/16 ，则跳过该步骤。如果你使用的 pod 的 CIDR 不同，使用如下命令设置一个名为 POD_CIDR 的环境变量，将清单文件中的 10.244.0.0/16 替换为你的 pod 的 CIDR。 POD_CIDR=\"\u003cyour-pod-cidr\u003e\" sed -i -e \"s?10.244.0.0/16?$POD_CIDR?g\" calico-etcd.yaml 注意：实际的清单文件中，pod cidr 为 192.168.0.0/16 因为本文中 pod 的 CIDR 为 172.30.0.0/16，所以需要替换下。 POD_CIDR=\"172.30.0.0/16\" sed -i -e \"s?192.168.0.0/16?$POD_CIDR?g\" calico-etcd.yaml 在清单文件（calico-etcd.yaml）中的名为 calico-config 的 ConfigMap中，设置你的 etcd 服务的 ip 地址和端口，可以通过逗号（英文逗号）作为分隔符指定多个 etcd 服务。 source /home/k8s/env.sh etcd_servers=$( xargs -n1\u003c\u003c\u003c${MasterArray[@]} | sort | sed 's#^#https://#;s#$#:2379#;$s#\\n##' | paste -d, -s - ) sed -ri \"s#http:\\/\\/\u003cETCD_IP\u003e:\u003cETCD_PORT\u003e#${etcd_servers}#\" calico-etcd.yaml 修改后如下所示 # Source: calico/templates/calico-config.yaml # This ConfigMap is used to configure a self-hosted Calico installation. kind: ConfigMap apiVersion: v1 metadata: name: calico-config namespace: kube-system data: # Configure this with the location of your etcd cluster. etcd_endpoints: \"https://192.168.15.5:2379,https://192.168.15.6:2379,https://192.168.15.7:2379\" 因为本文的 etcd 服务使用的是 https，所以需要在清单文件中配置下证书，这样 calico 才可以访问 etcd。 etcd_key=$(cat /etc/kubernetes/pki/etcd/server.key | base64 | tr -d '\\n') etcd_cert=$(cat /etc/kubernetes/pki/etcd/server.crt | base64 | tr -d '\\n') etcd_ca=$(cat /etc/kubernetes/pki/etcd/ca.crt | base64 | tr -d '\\n') sed -i \"s/# etcd-key: null/etcd-key: ${etcd_key}/\" calico-etcd.yaml sed -i \"s/# etcd-cert: null/etcd-cert: ${etcd_cert}/\" calico-etcd.yaml sed -i \"s/# etcd-ca: null/etcd-ca: ${etcd_ca}/\" calico-etcd.yaml sed -i 's?etcd_ca: \"\"?etcd_ca: \"/calico-secrets/etcd-ca\"?' calico-etcd.yaml sed -i 's?etcd_cert: \"\"?etcd_cert: \"/calico-secrets/etcd-cert\"?' calico-etcd.yaml sed -i 's?etcd_key: \"\"?etcd_key: \"/calico-secr","date":"2019-10-13","objectID":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/:2:2","tags":["docker","k8s","devops"],"title":"k8s v1.15.0 二进制方式部署","uri":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/"},{"categories":null,"content":"CoreDNS: DNS and Service Discovery 1.11 后 CoreDNS 已取代 Kube DNS 作为集群服务发现元件，由于 Kubernetes 需要让 Pod 与 Pod 之间能够互相通信，要能够通信需要知道彼此的 IP，通常是通过 Kubernetes API 来获取，但是 Pod IP 会因为生命周期变化而改变，因此这种做法无法弹性使用，且还会增加 API Server 负担，基于此问题 Kubernetes 提供了 DNS 服务来作为查询，让 Pod 能够以 Service 名称作为域名来查询 IP 位址，因此使用者就再不需要关心实际 Pod IP，而且 DNS 也会根据 Pod 变化更新资源记录（Record resources）。 执行如下命令，修改 coredns.yaml 文件中的 dns 信息和镜像名（k8s.gcr.io的镜像需要翻墙才能下载）。 source /home/k8s/env.sh sed -i -e \"s/__PILLAR__DNS__DOMAIN__/${CLUSTER_DNS_DOMAIN}/\" -e \"s/__PILLAR__DNS__SERVER__/${CLUSTER_DNS_SVC_IP}/\" -e \"s/__PILLAR__DNS__MEMORY__LIMIT__/170Mi/\" /home/k8s/k8s-manual-files/addons/coredns/coredns.yaml sed -i \"s#k8s.gcr.io/coredns:1.3.1#coredns/coredns:1.4.0#\" /home/k8s/k8s-manual-files/addons/coredns/coredns.yaml 修改 coredns.yaml 中容器的时区，使用宿主机的时区，从而使时间和宿主机保持一致（可选）。修改后与原文对比差异如下： [root@k8s-m1 coredns]# diff coredns.yaml coredns.yaml.base 67c67 \u003c kubernetes cluster.local in-addr.arpa ip6.arpa { --- \u003e kubernetes __PILLAR__DNS__DOMAIN__ in-addr.arpa ip6.arpa { 119c119 \u003c image: coredns/coredns:1.4.0 --- \u003e image: k8s.gcr.io/coredns:1.3.1 123c123 \u003c memory: 170Mi --- \u003e memory: __PILLAR__DNS__MEMORY__LIMIT__ 132,134d131 \u003c - name: host-time \u003c mountPath: /etc/localtime \u003c readOnly: true 175,177d171 \u003c - name: host-time \u003c hostPath: \u003c path: /etc/localtime 195c189 \u003c clusterIP: 10.96.0.10 --- \u003e clusterIP: __PILLAR__DNS__SERVER__ coredns.yaml 原文（coredns.yaml.base）地址：https://github.com/kubernetes/kubernetes/blob/release-1.15/cluster/addons/dns/coredns/coredns.yaml.base 执行 coredns.yaml 清单文件，安装 coredns kubectl apply -f /home/k8s/k8s-manual-files/addons/coredns/coredns.yaml 检查 coredns 是否启动成功 [root@k8s-m1 coredns]# kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system calico-kube-controllers-c84b7d67c-hbb65 1/1 Running 0 53m kube-system calico-node-8gxlw 1/1 Running 0 53m kube-system calico-node-95ltb 1/1 Running 0 53m kube-system calico-node-9s7jk 1/1 Running 0 53m kube-system calico-node-ntbnw 1/1 Running 0 52m kube-system calico-node-t2tfx 1/1 Running 0 53m kube-system coredns-677dfcd7d8-qcqfw 1/1 Running 0 2m42s kube-system kube-proxy-52hkz 1/1 Running 0 10h kube-system kube-proxy-54f8d 1/1 Running 0 10h kube-system kube-proxy-757j7 1/1 Running 0 10h kube-system kube-proxy-c2ht9 1/1 Running 0 10h kube-system kube-proxy-fdx48 1/1 Running 0 10h 测试 coredns 是否正常工作 新建一个 dnstool 的 pod，执行如下命令 cat\u003c\u003cEOF | kubectl apply -f - apiVersion: v1 kind: Pod metadata: name: busybox namespace: default spec: containers: - name: busybox image: busybox:1.28 command: - sleep - \"3600\" imagePullPolicy: IfNotPresent restartPolicy: Always EOF 该 pod 启动成功后，执行如下命令，查看是否能解析到对应的 ip 地址 [root@k8s-m1 coredns]# kubectl get pods NAME READY STATUS RESTARTS AGE busybox 1/1 Running 0 2m7s [root@k8s-m1 coredns]# kubectl exec -ti busybox -- nslookup kubernetes Server: 10.96.0.10 Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local Name: kubernetes Address 1: 10.96.0.1 kubernetes.default.svc.cluster.local ","date":"2019-10-13","objectID":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/:2:3","tags":["docker","k8s","devops"],"title":"k8s v1.15.0 二进制方式部署","uri":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/"},{"categories":null,"content":"dns-horizontal-autoscaler 部署 coredns 时未指定副本数，默认 1 个 coredns 的 pod 运行，可以安装 dns-horizontal-autoscaler 实现 dns 横向自动扩容。 Get the name of your DNS Deployment List the DNS deployments in your cluster in the kube-system namespace: kubectl get deployment -l k8s-app=kube-dns --namespace=kube-system The output is similar to this: NAME READY UP-TO-DATE AVAILABLE AGE coredns 1/1 1 1 18m If you don’t see a Deployment for DNS services, you can also look for it by name: kubectl get deployment --namespace=kube-system and look for a deployment named coredns or kube-dns. Your scale target is Deployment/\u003cyour-deployment-name\u003e where \u003cyour-deployment-name\u003e is the name of your DNS Deployment. For example, if the name of your Deployment for DNS is coredns, your scale target is Deployment/coredns. Note: CoreDNS is the default DNS service for Kubernetes. CoreDNS sets the label k8s-app=kube-dns so that it can work in clusters that originally used kube-dns. 本文的 target 为 Deployment/coredns Enable DNS horizontal autoscaling In this section, you create a new Deployment. The Pods in the Deployment run a container based on the cluster-proportional-autoscaler-amd64 image. 执行如下命令，将上一步获取的 target 更新到 dns 横向自动扩容的清单文件中，默认的镜像为 k8s.gcr.io/cluster-proportional-autoscaler-amd64:1.6.0，该镜像需要国内无法下载，改为hekai/k8s.gcr.io_cluster-proportional-autoscaler-amd64_1.6.0 sed -ri \"s#\\{\\{.Target\\}\\}#Deployment\\/coredns#\" /home/k8s/k8s-manual-files/addons/dns-horizontal-autoscaler/dns-horizontal-autoscaler.yaml sed -ri \"s#k8s.gcr.io/cluster-proportional-autoscaler-amd64:1.6.0#hekai/k8s.gcr.io_cluster-proportional-autoscaler-amd64_1.6.0#\" /home/k8s/k8s-manual-files/addons/dns-horizontal-autoscaler/dns-horizontal-autoscaler.yaml 使用 hostpath 将宿主机时区文件 /etc/localetime 挂载到容器中，使容器内的时间和宿主机时间保持同步（可选），修改后与原文件对比差异如下： [root@k8s-m1 dns-horizontal-autoscaler]# diff dns-horizontal-autoscaler.yaml dns-horizontal-autoscaler.yaml.orig 88c88 \u003c image: hekai/k8s.gcr.io_cluster-proportional-autoscaler-amd64_1.6.0 --- \u003e image: k8s.gcr.io/cluster-proportional-autoscaler-amd64:1.6.0 98c98 \u003c - --target=Deployment/coredns --- \u003e - --target={{.Target}} 104,107d103 \u003c volumeMounts: \u003c - name: host-time \u003c mountPath: /etc/localtime \u003c readOnly: true 112,115d107 \u003c volumes: \u003c - name: host-time \u003c hostPath: \u003c path: /etc/localtime 执行如下命令新建 dns 横向自动扩容的 Deployment kubectl apply -f /home/k8s/k8s-manual-files/addons/dns-horizontal-autoscaler/dns-horizontal-autoscaler.yaml 更多操作请参考官方文档：https://kubernetes.io/docs/tasks/administer-cluster/dns-horizontal-autoscaling/ ","date":"2019-10-13","objectID":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/:2:4","tags":["docker","k8s","devops"],"title":"k8s v1.15.0 二进制方式部署","uri":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/"},{"categories":null,"content":"metrics-server Metrics Server 是实现了 Metrics API 的元件,其目标是取代 Heapster 作为 Pod 与 Node 提供资源的 Usage metrics，该元件会从每个 Kubernetes 节点上的 Kubelet 所公开的 Summary API 中收集 Metrics Horizontal Pod Autoscaler（HPA）控制器用于实现基于CPU使用率进行自动Pod伸缩的功能。 HPA 控制器基于 Master 的 kube-controller-manager 服务启动参数 –horizontal-pod-autoscaler-sync-period 定义是时长（默认30秒），周期性监控目标 Pod 的 CPU 使用率，并在满足条件时对 ReplicationController 或 Deployment 中的 Pod 副本数进行调整,以符合用户定义的平均 Pod CPU 使用率。 在新版本的 kubernetes 中 Pod CPU 使用率不在来源于 heapster，而是来自于 metrics-server，官网原话是 The –horizontal-pod-autoscaler-use-rest-clients is true or unset. Setting this to false switches to Heapster-based autoscaling, which is deprecated. yml 文件来自于github https://github.com/kubernetes-incubator/metrics-server/tree/master/deploy/1.8+ 若启用 metrics-server，则 kube-apiserver 的 systemed unit 文件（kube-apiserver.service）需要配置如下参数（本文默认已配置）： --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem \\ --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem \\ --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem \\ --requestheader-allowed-names=aggregator \\ --requestheader-group-headers=X-Remote-Group \\ --requestheader-extra-headers-prefix=X-Remote-Extra- \\ --requestheader-username-headers=X-Remote-User \\ 官方 metric-server 使用的镜像为 k8s.gcr.io/metrics-server-amd64:v0.3.5，该镜像在国内无法下载，替换为 hekai/k8s.gcr.io_metrics-server-amd64_v0.3.5 sed -ri \"s#k8s.gcr.io/metrics-server-amd64:v0.3.5#hekai/k8s.gcr.io_metrics-server-amd64_v0.3.5#\" /home/k8s/k8s-manual-files/addons/metric-server/metrics-server-deployment.yaml 使用 hostpath 将宿主机时区文件 /etc/localetime 挂载到容器中，使容器内的时间和宿主机时间保持同步（可选），修改后与原文件对比差异如下： [root@k8s-m1 metric-server]# diff metrics-server-deployment.yaml metrics-server-deployment.yaml.orig 30,32d29 \u003c - name: host-time \u003c hostPath: \u003c path: /etc/localtime 35c32 \u003c image: hekai/k8s.gcr.io_metrics-server-amd64_v0.3.5 --- \u003e image: k8s.gcr.io/metrics-server-amd64:v0.3.5 40,42d36 \u003c - name: host-time \u003c mountPath: /etc/localtime \u003c readOnly: true 执行如下命令，部署 metric-server kubectl apply -f /home/k8s/k8s-manual-files/addons/metric-server/ 查看 pod 的状态 kubectl -n kube-system get po -l k8s-app=metrics-server 过一段时间就可以查看 node 的 top 信息 kubectl top nodes ","date":"2019-10-13","objectID":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/:2:5","tags":["docker","k8s","devops"],"title":"k8s v1.15.0 二进制方式部署","uri":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/"},{"categories":null,"content":"Kubernetes Extra Addons ","date":"2019-10-13","objectID":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/:3:0","tags":["docker","k8s","devops"],"title":"k8s v1.15.0 二进制方式部署","uri":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/"},{"categories":null,"content":"kubernetes-dashboard 阅读 Dashboard 官方文档 参考：https://github.com/kubernetes/dashboard 安装 Dashboard 注意： k8s 1.15.0 server 包中含有 dashboard 的清单文件，位置为 kubernetes/cluster/addons/dashboard ，其中，使用的是 dashboard 1.10.1 版本，因为 dashboard 发布页面上显示 dashboard 1.10.1 版本不支持 k8s 1.15.0，所以此次安装 dashboard 使用的是 dashboard v2.0.0-beta4 在安装新Beta之前，请通过删除其名称空间来删除以前的版本： kubectl delete ns kubernetes-dashboard 执行以下命令进行安装 kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta4/aio/deploy/recommended.yaml 访问 dashboard 访问方式： 使用 ingress 访问（看下文） 使用 nodePort 访问 查看 dashboard 的 Service [root@k8s-m1 ~]# kubectl get svc -n kubernetes-dashboard NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE dashboard-metrics-scraper ClusterIP 10.99.202.100 \u003cnone\u003e 8000/TCP 30m kubernetes-dashboard ClusterIP 10.103.89.91 \u003cnone\u003e 443/TCP 30m 修改 dashboard 的 Service 的 服务类型（type） [root@k8s-m1 ~]# kubectl edit svc -n kubernetes-dashboard kubernetes-dashboard 将 type: ClusterIP 修改为 type: NodePort 查看该 Service 映射的端口 [root@k8s-m1 ~]# kubectl get svc -n kubernetes-dashboard NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE dashboard-metrics-scraper ClusterIP 10.99.202.100 \u003cnone\u003e 8000/TCP 35m kubernetes-dashboard NodePort 10.103.89.91 \u003cnone\u003e 443:30732/TCP 35m 可以看到，映射到了节点的30732端口。 访问 dashboard https://192.168.15.5:30732 chrome 浏览器无法访问，因为 dashboard 使用的根证书在本地浏览器不存在，所以不被信任。使用 firefox 浏览器可以访问。配置 https 请查看 ingress-nginx 一节。 登录 dashboard 使用 token 登录 dashboard 在 dashboard 的命名空间（本文为 kubernetes-dashboard）内新建一个 sa （Service Account） kubectl create sa admin-user -n kubernetes-dashboard 将新建的名为 dashboard-admin 的 sa 与 名为 cluster-admin 的 clusterrole 绑定 kubectl create clusterrolebinding admin-user --clusterrole=cluster-admin --serviceaccount=kubernetes-dashboard:admin-user 输出可以用来登录的 token DASHBOARD_LOGIN_TOKEN=$(kubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret | grep admin-user | awk '{print $1}') | grep -E '^token' | awk '{print $2}') echo ${DASHBOARD_LOGIN_TOKEN} 使用 kubeconfig 登录 dashboard，需要使用上面的 token source /home/k8s/env.sh # 设置集群参数 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/pki/ca.crt \\ --embed-certs=true \\ --server=${KUBE_APISERVER} \\ --kubeconfig=dashboard.kubeconfig # 设置客户端认证参数，使用上面创建的 Token kubectl config set-credentials admin_user \\ --token=${DASHBOARD_LOGIN_TOKEN} \\ --kubeconfig=dashboard.kubeconfig # 设置上下文参数 kubectl config set-context default \\ --cluster=kubernetes \\ --user=admin_user \\ --kubeconfig=dashboard.kubeconfig # 设置默认上下文 kubectl config use-context default --kubeconfig=dashboard.kubeconfig https证书是 dashboard 自动生成的，因为清单文件（recommended.yaml）中配置了 --auto-generate-certificates ","date":"2019-10-13","objectID":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/:3:1","tags":["docker","k8s","devops"],"title":"k8s v1.15.0 二进制方式部署","uri":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/"},{"categories":null,"content":"Helm 阅读 Helm 官方文档 Helm v2.14.3 官方文档阅读笔记 Helm 介绍 helm 是 kubernetes 的包管理器。 helm 通过 kubernetes 的配置文件（通常是 $HOME/.kube/config ）来断定将 tiller （helm 服务端）安装到哪个 context 中。 如果想弄清楚 tiller 将要安装到哪个 context 中，可以执行 kubectl config current-context 或 kubectl cluster-info 查看。 [root@k8s-m1 ~]# kubectl config current-context kubernetes-admin@kubernetes [root@k8s-m1 ~]# kubectl cluster-info Kubernetes master is running at https://127.0.0.1:8443 CoreDNS is running at https://127.0.0.1:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy Metrics-server is running at https://127.0.0.1:8443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. 本文是在拥有全部权限，完全控制的 k8s 集群中使用 helm，即在私有网络中使用，不用考虑额外的安全配置。如果 k8s 集群暴露在一个更大的网络中，或和其他人共享集群（正式环境属于这一类），必须采取额外的步骤保护已安装的 helm ，避免其他用户无意或恶意的操作损坏 helm 或它的数据，在正式环境或多租户环境中保护 helm 的部署，参考：https://helm.sh/docs/using_helm/#securing-your-helm-installation 如果 k8s 集群中启用了 RBAC ，则需要在安装 helm 前配置 service account 。 本次部署默认开启了 RBAC （见 kube-apiserver.service），安装之前，需要配置 service account 安装 Helm 安装 helm 客户端 下载期望安装的 helm 版本，下载地址：https://github.com/helm/helm/releases 将下载得到的 tar 包解压 tar -zxvf helm-v2.14.1-linux-amd64.tar.gz 将 helm 二进制文件放到可执行目录中。 mv linux-amd64/helm /usr/local/bin/helm 安装 helm 服务端（tiller） 在 namespace 为 kube-system 中新建一个 sa （Service Account），名为 tiller kubectl -n kube-system create sa tiller 将上一步新建的 sa 与名为 cluster-admin 的clusterrole 进行绑定（clusterrolebinding），该绑定（clusterrolebinding）名为 tiller kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller Note: 名为 cluster-admin 的 clusterrole 是 kubernetes 集群默认创建的，不需要手动创建。 使用 新建的 sa 安装 helm 服务端（tiller） helm init --service-account tiller --history-max 200 --tiller-image hekai/gcr.io_kubernetes-helm_tiller_v2.14.1 Note: tiller 默认会被安装到 kube-system 命名空间内。默认情况下，tiller 会使用 gcr.io/kubernetes-helm/tiller:v2.14.1 镜像，由于国内墙的原因，该镜像无法下载，可以使用 --tiller-image 参数指定 tiller 的镜像，此处使用 hekai/gcr.io_kubernetes-helm_tiller_v2.14.1 官网推荐初始化 helm 时使用 --history-max 参数，因为在 helm 的历史记录中，configmaps 和其他对象如果在初始化时不指定 --history-max 就会无限地增大，不方便维护。 查看 tiller 是否安装成功 $ kubectl get pods -n kube-system -l app=helm NAME READY STATUS RESTARTS AGE tiller-deploy-799f6d8f7-dsv4r 1/1 Running 0 40s $ helm version Client: \u0026version.Version{SemVer:\"v2.14.1\", GitCommit:\"5270352a09c7e8b6e8c9593002a73535276507c0\", GitTreeState:\"clean\"} Server: \u0026version.Version{SemVer:\"v2.14.1\", GitCommit:\"5270352a09c7e8b6e8c9593002a73535276507c0\", GitTreeState:\"clean\"} ","date":"2019-10-13","objectID":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/:3:2","tags":["docker","k8s","devops"],"title":"k8s v1.15.0 二进制方式部署","uri":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/"},{"categories":null,"content":"MetalLB 阅读 MetalLB官方文档 MetalLB-v0.8.1-官方文档阅读笔记 MetalLB 介绍 MetalLB 是为裸机部署的 k8s 集群提供的使用标准路由协议的负载均衡器（load-balancer）的实现。 Kubernetes 没有为 k8s 的裸机集群提供网络负载均衡器（对应的 Service 的类型为 LoadBalancer）的实现，Kubernetes 实现的网络负载均衡器都是调用各种 Iaas 平台的粘合代码（代码耦合性很高），如果你没有运行在支持 load-balancer 的 Iaas 平台上，LoadBalancers 被创建后将会一直处于 pending 状态。 裸机部署的 k8s 集群操作者只剩下两种方式将用户的流量引入集群内部， NodePort 和 externalIPs 服务（还有 ingress 呢），这两种方式如果在生产环境使用都有很大的缺点，从而使裸机部署的 k8s 集群成为 k8s 生态系统的二等公民。 MetalLB 通过提供一个整合标准网络设备的网络负载均衡器的实现纠正了这种不平衡，因此在裸机部署的集群外的服务也能够像实现了网络负载均衡器的 Iaas 平台一样工作。 一旦 MetalLB 为服务分配了外部 IP 地址，它就需要使群集之外的网络意识到该 IP 在群集中“存在”。 MetalLB 使用标准路由协议来实现此目的：ARP，NDP或BGP。 2层协议模式（ARP/NDP）: 在 2 层协议模式中，集群中的一台机器拥有服务的所有权，并且使用标准地址发现协议（ipv4 使用 ARP，ipv6 使用 NDP）来让那些 ip 能够在本地网络中可达。从局域网的角度看，该机器上有多个 IP 地址。 优点：2层协议模式的优势是它的通用性，它可以在任何以太网网络上运行，不需要特殊的硬件，甚至不需要特殊的路由器。在2层协议模式中，所有外部服务的流量都会流向一个节点（机器），在该节点上，k8s 的 kube-proxy 组件将流量转发到对应服务的 pods 中。这就意味着，2层协议模式没有实现负载均衡器。相反地，它实现了一个故障转移的机制，以便当前节点如果因为某些原因发生故障时，其他节点可以接替工作。如果 leader 节点因为某些原因发生故障，故障转移会自动进行：旧 leader 的租约超过 10 秒的超时时间后，另一个节点会变成 leader 并且接管服务 ip 的所有权（和 keepalived 比较相似）。 缺点：2层协议模式有两个主要的限制需要被注意，单节点瓶颈和潜在的缓慢故障转移。就像上面描述的一样，2层协议模式中，被选举出的单节点的 leader 会接收该服务ip的所有的流量，这意味着服务的带宽被单节点的带宽限制，这个是使用 ARP 和 NDP 引导流量的基本限制。在当前的 MetalLB 实现中，节点间的故障转移依靠客户端的协作。当一个故障转移发生时， MetalLB 会发送一些2层协议的 gratuitou 数据包通知客户端，让客户端知道和服务 ip 关联的 mac 地址已经发生改变。大多数操作系统知道如何正确的处理gratuitou数据包并且正确地更新缓存，在这种情况下，故障转移可以在几秒内完成。然而，一些系统没有实现gratuitou数据包的处理机制，或者存在错误的操作导致更新缓存缓慢。所有的主流的操作系统(Windows, Mac, Linux）都正确地实现了二层故障转移，所以那些版本较老或使用较少的操作系统可能会存在问题。 BGP 模式：在 BGP 模式中，机器中的所有机器都会与您控制的附近的路由器建立 BGP 对等会话，并且告诉这些路由器如何转发服务的 ip。使用 BGP 模式能够实现真正的不同节点的负载均衡，并实现细粒度的流量控制。 优点：假设您的路由器配置为支持多路径，这将实现真正的负载平衡：MetalLB发布的路由彼此等效，除了它们的下一跳。 这意味着路由器将一起使用所有下一跳，并在它们之间进行负载平衡。数据包到达节点后，kube-proxy负责流量路由的最后一跳，以将数据包到达服务中的一个特定容器。负载平衡的确切实现取决于您的特定路由器模式和配置，但是常见的行为是基于数据包哈希值来平衡每个连接的流量。每个连接意味着一个单独的 tcp 或 udp 会话的所有的数据包都会定向到机器中的某一个单独机器上。流量分配仅在不同的连接进行，而不是在同一个连接的不同包。这个是一个很好的设计，因为在不同的集群节点中分发数据包将会导致在多个级别上产生不良的行为：1. 将单个连接分布在多个路径上会导致数据包在网络上重新排序，这将严重影响最终主机的性能。2. Kubernetes中单节点流量路由在各个节点之间不保证是一致的。 这意味着两个不同的节点可能决定将同一连接的数据包路由到不同的Pod，这将导致连接失败 缺点：将BGP用作负载平衡机制的优势在于，您可以使用标准路由器硬件，而不是定制的负载平衡器。 但是，这也有缺点。最大的问题是，基于BGP的负载平衡无法对地址后端集的更改做出优雅的响应。 这意味着，当群集节点出现故障时，您应该想到与你的服务的所有活动连接被断开（用户将看到“对等方重置连接”）。基于BGP的路由器实现无状态负载平衡。 他们通过散列数据包头中的某些字段，并使用该散列作为可用后端数组的索引，将给定的数据包分配给特定的下一跳。问题在于路由器中使用的哈希值通常不稳定，因此，只要后端集的大小发生变化（例如，当节点的BGP会话断开时），现有连接就会随机有效地进行哈希刷新，这意味着大多数现有连接会突然结束被转发到另一后端，该后端不知道所讨论的连接。这样做的结果是，每当服务的IP→节点映射发生更改时，您都应该会看到连接到服务的大多数活动的连接断开。 没有持续的数据包丢失或黑洞，只有一次干净的清理。BGP 模式无法与其他的 BGP 兼容（如 calico 的 bgp 模式） 安装 MetalLB 安装前检查是否满足要求 一个版本最低为1.13.0的没有网络负载均衡器的 kubernetes 集群 一个能与 MetalLB 共存的集群网络配置 一些 ipv4 地址供 MetalLB 使用 根据操作模式，您可能需要一个或多个能够使用BGP的路由器。 本文中 calico 使用的是默认的 IPIP 模式 执行清单文件，在 metallb-system 的命名空间内安装 MetalLB kubectl apply -f https://raw.githubusercontent.com/google/metallb/master/manifests/metallb.yaml 安装清单不包括配置文件。 MetalLB的组件仍将启动，但在定义和部署配置映射之前将保持 idle 状态。 定义配置文件，在配置文件中指定使用二层协议模式或者 BGP 模式，配置文件示例：https://raw.githubusercontent.com/google/metallb/master/manifests/example-config.yaml 本文使用二层协议模式 二层协议模式是最简单的，适合大部分情况，不需要任何协议对应的配置，只需要 IP 即可。第2层模式不需要将IP绑定到工作节点的网络接口。 它的工作原理是直接响应本地网络上的ARP请求，从而将计算机的MAC地址提供给客户端。例如，以下配置使 MetalLB 控制从 192.168.15.144 到 192.168.15.148 的 IP ，并配置第2层模式： apiVersion:v1kind:ConfigMapmetadata:namespace:metallb-systemname:configdata:config:|address-pools: - name: default protocol: layer2 addresses: - 192.168.15.144-192.168.15.148 对于具有一个BGP路由器和一个IP地址范围的基本配置，您需要4条信息： MetalLB应该连接的路由器IP地址， 路由器的AS号， MetalLB应该使用的AS号， 以CIDR前缀表示的IP地址范围。 例如，如果要给MetalLB提供192.168.10.0/24的范围和AS编号64500，并将其连接到AS编号为64501的地址为10.0.0.1的路由器，则配置如下所示： apiVersion:v1kind:ConfigMapmetadata:namespace:metallb-systemname:configdata:config:|peers: - peer-address: 10.0.0.1 peer-asn: 64501 my-asn: 64500 address-pools: - name: default protocol: bgp addresses: - 192.168.10.0/24 执行配置文件 kubectl apply -f config.yaml 测试 LoadBalancer 新建清单文件，nginx-test.yaml，内容如下 apiVersion:apps/v1kind:Deploymentmetadata:name:nginxspec:selector:matchLabels:app:nginxtemplate:metadata:labels:app:nginxspec:containers:- name:nginximage:nginxports:- name:httpcontainerPort:80---apiV","date":"2019-10-13","objectID":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/:3:3","tags":["docker","k8s","devops"],"title":"k8s v1.15.0 二进制方式部署","uri":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/"},{"categories":null,"content":"ingress-nginx 阅读 ingress-nginx 官方文档 Ingress-Nginx-20191015-官方文档阅读笔记 ingress-nginx 介绍 Ingress 将 k8s 集群内部的 Service 通过 HTTP 和 HTTPS 路由暴露到集群外部。 可以将Ingress配置为提供服务外部可访问的URL，负载平衡流量，终止SSL / TLS并提供基于名称的虚拟主机。 Ingress Controller 通常结合负载平衡器实现 Ingress，尽管它也可以配置边缘路由器或其他前端以帮助处理流量。 Ingress 不会公开任意端口或协议。 若将除 HTTP 和 HTTPS 以外的服务暴露到 Internet，通常使用 Service.Type = NodePort 或 Service.Type = LoadBalancer 类型的服务。 为了使 Ingress 资源正常工作，集群必须运行一个 Ingress Controller 。 本文使用 ingress-nginx 作为 k8s 集群的 Ingress Controller 。 ingress-nginx 是使用 nginx 实现的 Ingress Controller ，使用 nginx 的重点是 nginx 的配置文件（nginx.conf），若 nginx 的配置文件发生变化必然会涉及 nginx configuration reload，导致 ingress controller 短暂的不可用。需要注意的是，nginx 配置文件中，上游（upstream）的变化不会导致 nginx reload（如部署的服务的 Endpoints 发生变化，不会导致 reload） 以下情况会导致 nginx configuration reload 创建新的 Ingress 现有 Ingress 中添加 TLS 部分 Ingress annotations 的更改影响的不仅仅是上游（upstream）配置。 例如，load-balance annotation不需要重新加载。 从 Ingress 中添加/删除路径。 删除 Ingress、Service 或 Secret Ingress 中缺少的一些的引用对象变为可用，例如 Service 或 Secret。 Secret 被更新。 部署 Ingress Controller load-balancer 方式部署 使用 metalLB 提供的 load-balancer 部署 ingress-nginx 本文使用的是该方式部署的 Ingress Controller 安装ingress-nginx 清单文件 kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/mandatory.yaml 下载 Service 文件，将文件中的 type: LoadBalancer 修改为 type: NodePort，然后执行安装 wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/provider/baremetal/service-nodeport.yaml 修改 type kubectl create -f service-nodeport.yaml 检查是否分配了 EXTERNAL-IP $ kubectl get svc -n ingress-nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ingress-nginx LoadBalancer 10.107.168.72 192.168.15.144 80:32435/TCP,443:31885/TCP 4m41s 新建 Ingress 文件测试 注意 Ingress 文件的 namespace 必须与对应 service 的 namespace 保持一致，否则会报错503 hostNetwork 方式部署 注意： 默认配置从所有名称空间监视Ingress对象。要更改此行为，请使用标志 –watch-namespace 将范围限制为特定的名称空间。 如果多个 Ingress 为同一主机定义了不同的路径，则 Ingress Controller 将合并这些定义。 使用 hostnetwork 的方式进行部署，这种方法的好处是，NGINX Ingress控制器可以将端口80和443直接绑定到Kubernetes节点的网络接口，而无需NodePort Services施加额外的网络转换。此部署方法的一个主要限制是，在每个群集节点上只能调度单个NGINX Ingress控制器Pod，因为从技术上讲，在同一网络接口上多次绑定同一端口是不可能的。 缺点是只能部署一个 ingress controller，并且存在单点问题，如新建 ingress ，域名为 example.com，在 dns中配置域名与ip 的映射后，如配置 192.168.15.5 example.com ，若对应的ip（192.168.15.5）宕机了，会导致该ingress 不可用。 下载 ingress-nginx 清单文件 wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/mandatory.yaml 修改清单文件 将 Deployment 改为使用 DaemonSet 进行部署（保证一个节点上运行一个 ingress controller 容器） 去除清单文件中的副本数限制（replicas: 1） 添加 hostNetwork: true ，启用 hostNetwork 添加 dnsPolicy: ClusterFirstWithHostNet ，允许 pod 使用 k8s 集群内的 dns 服务 去除参数 - --publish-service=$(POD_NAMESPACE)/ingress-nginx ，因为使用 hostNetwork 模式不需要新建 Service，hostNetwork模式下此参数会导致 Ingress 对象的 status 为空 添加参数 - --report-node-internal-ip-address ，将所有Ingress对象的状态设置为运行NGINX Ingress控制器的所有节点的内部IP地址。 修改后和原清单内容对比如下： [root@k8s-m1 ingress-nginx]# diff mandatory.yaml mandatory.yaml.orig 191c191 \u003c kind: DaemonSet --- \u003e kind: Deployment 199c199 \u003c #replicas: 1 --- \u003e replicas: 1 216,217d215 \u003c hostNetwork: true \u003c dnsPolicy: ClusterFirstWithHostNet 228c226 \u003c #- --publish-service=$(POD_NAMESPACE)/ingress-nginx --- \u003e - --publish-service=$(POD_NAMESPACE)/ingress-nginx 230d227 \u003c - --report-node-internal-ip-address 执行修改后的清单文件，完成 nginx-ingress 的部署 kubectl apply -f mandatory.yaml 测试 Ingress 将上文中的 nginx-test.yaml 修改 Service 的 tpye ，改为 ClusterIP， ClusterIP 模式只能在集群内访问，集群外无法访问。修改后如下所示： apiVersion:apps/v1kind:Deploymentmetadata:name:nginxspec:selector:matchLabels:app:nginxtemplate:metadata:labels:app:nginxspec:containers:- name:nginximage:nginximagePullPolicy:IfNotPresentports:- name:httpcontainerPort:80---apiVersion:v1kind:Servicemetadata:name:nginxspec:ports:- name:httpport:80protocol:TCPtargetPort:80selector:app:nginxtype:ClusterIP 部署该清单文件 kubectl create -f nginx-test.yaml 查看部署是否成功 [root@k8s-m1 test-nginx]# kubectl get pods NAME READY STATUS RESTARTS AGE nginx-6877889877-jcsrj 1/1 Running 0 56s [root@k8s-m1 test-nginx]# kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes Clu","date":"2019-10-13","objectID":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/:3:4","tags":["docker","k8s","devops"],"title":"k8s v1.15.0 二进制方式部署","uri":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/"},{"categories":null,"content":"cert-manager 阅读 cert-manager官方文档 Cert-Manager-v0.11-官方文档阅读笔记 cert-manager 介绍 cert-manager 是一个基于 kubernetes 的证书管理控制器。它可以帮助你从不同的来源签发证书，如 Let’s Encrypt ， HashiCorp Vault ， Venafi ，简单的签名密钥对或自签名。 它将确保证书有效并且是最新的，并在到期前尝试在配置的时间续订证书。 cert-manager 支持运行在 kubernetes 或 openshif 上，本文只介绍基于 kubernetes 的安装。 cert-manager 可以使用 清单文件安装 或使用 helm 安装。它利用自定义资源对象（CustomResourceDefinitions）来配置证书颁发机构和请求证书。cert-manager 部署完成后，需要你配置代表证书颁发机构的 Issuer 或 ClusterIssuer 。 在你可以使用 cert-manager 签发证书之前，必须配置至少一个 Issuer 或 ClusterIssuer。 Issuer 与 ClusterIssuer 的区别是 Issuer 的作用域为单个命名空间，并且只能在其命名空间内签发证书，这在多租户环境中非常有用。而 ClusterIssuer 是 Issuer 的集群范围版本，证书资源可以在任何命名空间中引用它。 cert-manager 支持的 Issuer 类型有： CA-颁发由X509签名密钥对签名的证书，该证书存储在Kubernetes API server 的 secret 中。 自签名-颁发自签名证书。 ACME-颁发通过对诸如 Let’s Encrypt 之类的 ACME 服务器执行挑战验证而获得的证书。 Vault-颁发从配置了 Vault PKI 后端的 Vault 实例中的获得的证书。 Venafi-颁发从 Venafi Cloud 或 Trust Protection Platform 实例获得的证书。 概念理解： Issuer（证书颁发者） ClusterIssuers（集群范围内的证书颁发者） Certificates（证书） 本文只介绍 CA 和 自签名 两种 Issuer。 配置 CA Issuers 参考：https://cert-manager.readthedocs.io/en/latest/tasks/issuers/setup-ca.html cert-manager 可用于使用存储在 Kubernetes Secret 资源中的任意签名密钥对来获取证书。 本指南将向您展示如何基于存储在 secret 资源中的签名密钥对来配置和创建基于 CA 的 issuer。 生成一个签名密钥对（如果你已有签名密钥对，可不执行此步骤） CA Issuer不会自动为您创建和管理签名密钥对。 所以，您将需要提供自己的 CA 或使用诸如 openssl 或 cfssl 之类的工具生成自签名的 CA 。 本步骤将说明如何生成新的签名密钥对，但是如果你的签名密钥对是 CA ，你就可以使用自己的代替，跳过这一步。 # Generate a CA private key $ openssl genrsa -out ca.key 2048 # Create a self signed Certificate, valid for 10yrs with the 'signing' option set $ openssl req -x509 -new -nodes -key ca.key -subj \"/CN=${COMMON_NAME}\" -days 3650 -reqexts v3_req -extensions v3_ca -out ca.crt 上述命令会生成两个文件， ca.key 和 ca.crt ，分别为你的签名证书对的密钥和证书。如果你已经有了自己的签名密钥对，你应该分别命名私钥和证书为 ca.key 和 ca.crt 。 将签名密钥对以 secret 形式保存 我们将创建一个 Issuer ，它将使用此密钥对生成签名证书。 您可以在Issuer参考文档中阅读有关 Issuer 资源的更多信息。 为了允许 Issuer 引用签名密钥对，我们将其存储在Kubernetes Secret资源中。 Issuer 是命名空间范围的资源，因此它们只能在自己的命名空间中引用 Secrets。 因此，我们将密钥对放入与 Issuer 相同的名称空间中。 我们也可以创建一个ClusterIssuer，它是Issuer的群集范围版本。 有关ClusterIssuers的更多信息，请阅读ClusterIssuer参考文档。 以下命令将在默认名称空间中创建一个包含签名密钥对的Secret： kubectl create secret tls ca-key-pair \\ --cert=ca.crt \\ --key=ca.key \\ --namespace=default 引用 secret 创建一个 Issuer 现在，我们可以创建一个引用刚刚创建的 Secret 资源的 Issuer ： apiVersion:cert-manager.io/v1alpha2kind:Issuermetadata:name:ca-issuernamespace:defaultspec:ca:secretName:ca-key-pair 我们现在准备获得证书！ 获取签名证书 现在，我们可以创建以下证书资源，以指定所需的证书。 您可以在参考文档中阅读有关证书资源的更多信息。 apiVersion:cert-manager.io/v1alpha2kind:Certificatemetadata:name:example-comnamespace:defaultspec:secretName:example-com-tlsissuerRef:name:ca-issuer# We can reference ClusterIssuers by changing the kind here.# The default value is Issuer (i.e. a locally namespaced Issuer)kind:IssuercommonName:example.comorganization:- Example CAdnsNames:- example.com- www.example.com 为了使用 Issuer 获得证书，我们必须在与 Issuer 相同的名称空间中创建证书资源，因为 Issuer 是命名空间范围的资源。 如果我们想在多个命名空间之间重用签名密钥对，则可以选择创建 ClusterIssuer 。 一旦我们创建了 Certificate 资源，cert-manager 将尝试使用 Issuer ca-issuer获取证书。 如果成功，则证书将存储在名为example-com-tls 的 Secret 资源中，位于与 Certificate 资源相同的名称空间中（default）。 上面的示例将 commonName 字段显式设置为 example.com 。 如果 dnsNames 字段中尚未包含 commonName 字段，则 cert-manager 会自动将 commonName 字段添加为DNS SAN。 如果我们未指定 commonName 字段，则将使用第一个指定的DNS SAN（在 dnsNames 下）作为证书的通用名称。 创建上述证书后，我们可以检查是否已成功获得证书，如下所示： $ kubectl describe certificate example-com Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning ErrorCheckCertificate 26s cert-manager-controller Error checking existing TLS certificate: secret \"example-com-tls\" not found Normal PrepareCertificate 26s cert-manager-controller Preparing certificate with issuer Normal IssueCertificate 26s cert-manager-controller Issuing certificate... Normal CertificateIssued 25s cert-manager-controller Certificate issued successfully 你还可以通过执行 kubectl get secret example-com-tls -o yaml 检查 issuer 生成的证书是否成功，你应该看到一个base64编码的签名TLS密钥对。 一旦获得证书，cert-manager 将继续检查其有效性，并在证书即将到期时尝试对其进行更新。 当证书上的 “Not After” 字段小于当前时间加上30天时，cert-manager 认为证书即将到期。 对于基于 CA 的颁发者，cert-manager 将颁发 “Not After” 字段设置为当前时间加上365天的证书。 配置自签名 Issuer 自签名 Is","date":"2019-10-13","objectID":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/:3:5","tags":["docker","k8s","devops"],"title":"k8s v1.15.0 二进制方式部署","uri":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/"},{"categories":null,"content":"nfs-storageclass 安装 NFS 服务器 略 安装 NFS 客户端 获取 NFS 服务器的连接信息 在发布版本页面下载最新的发布包 wget https://github.com/kubernetes-incubator/external-storage/archive/v5.5.0.tar.gz 下载后解压 tar -zxvf external-storage-5.5.0.tar.gz 进入 nfs-client 目录 cd external-storage-5.5.0/nfs-client/ 设置权限和修改命名空间 新建 nfs-storageclass 命名空间 kubectl create ns nfs-storageclass 修改 deploy/rbac.yaml 中的ServiceAccount，Role 等的命名空间，修改后内容如下 kind:ServiceAccountapiVersion:v1metadata:name:nfs-client-provisionernamespace:nfs-storageclass---kind:ClusterRoleapiVersion:rbac.authorization.k8s.io/v1metadata:name:nfs-client-provisioner-runnerrules:- apiGroups:[\"\"]resources:[\"persistentvolumes\"]verbs:[\"get\",\"list\",\"watch\",\"create\",\"delete\"]- apiGroups:[\"\"]resources:[\"persistentvolumeclaims\"]verbs:[\"get\",\"list\",\"watch\",\"update\"]- apiGroups:[\"storage.k8s.io\"]resources:[\"storageclasses\"]verbs:[\"get\",\"list\",\"watch\"]- apiGroups:[\"\"]resources:[\"events\"]verbs:[\"create\",\"update\",\"patch\"]---kind:ClusterRoleBindingapiVersion:rbac.authorization.k8s.io/v1metadata:name:run-nfs-client-provisionersubjects:- kind:ServiceAccountname:nfs-client-provisionernamespace:nfs-storageclassroleRef:kind:ClusterRolename:nfs-client-provisioner-runnerapiGroup:rbac.authorization.k8s.io---kind:RoleapiVersion:rbac.authorization.k8s.io/v1metadata:name:leader-locking-nfs-client-provisionernamespace:nfs-storageclassrules:- apiGroups:[\"\"]resources:[\"endpoints\"]verbs:[\"get\",\"list\",\"watch\",\"create\",\"update\",\"patch\"]---kind:RoleBindingapiVersion:rbac.authorization.k8s.io/v1metadata:name:leader-locking-nfs-client-provisionernamespace:nfs-storageclasssubjects:- kind:ServiceAccountname:nfs-client-provisioner# replace with namespace where provisioner is deployednamespace:nfs-storageclassroleRef:kind:Rolename:leader-locking-nfs-client-provisionerapiGroup:rbac.authorization.k8s.io 执行如下命令部署 rbac.yaml kubectl create -f deploy/rbac.yaml 修改 deployment.yaml 配置 NFS server 连接信息 将 deploy/deployment.yaml 中的 \u003cYOUR NFS SERVER HOSTNAME\u003e 修改为 nfs server 的 hostname 或 ip，同时也修改下NFS_PATH 和 PROVISIONER_NAME 对应的值。若修改 deploy/deployment.yaml 中 PROVISIONER_NAME 对应的值，必须同时修改下 deploy/class.yaml 中 provisioner 的值，使两个值保持一致。 因为修改了默认的命名空间，默认命名空间为 default，改为了 nfs-storageclass，所以，也需要修改下 deploy/deployment.yaml 中 Deployment 和 ServiceAccount 的命名空间。 deploy/class.yaml 这个 StorageClass 定义文件中，添加注解 storageclass.kubernetes.io/is-default-class: \"true\"， 该注解将该 storageclass 配置为默认的 StorageClass。参考：https://kubernetes.io/docs/tasks/administer-cluster/change-default-storage-class/ nfs server 地址为 192.168.15.143， 路径为 /home/data/nfs/data 修改后的 deploy/deployment.yaml 内容如下 #apiVersion: v1#kind: ServiceAccount#metadata:# name: nfs-client-provisioner# namespace: nfs-storageclass---kind:DeploymentapiVersion:extensions/v1beta1metadata:name:nfs-client-provisionernamespace:nfs-storageclassspec:replicas:1strategy:type:Recreatetemplate:metadata:labels:app:nfs-client-provisionerspec:serviceAccountName:nfs-client-provisionercontainers:- name:nfs-client-provisionerimage:quay.io/external_storage/nfs-client-provisioner:latestvolumeMounts:- name:nfs-client-rootmountPath:/persistentvolumesenv:- name:PROVISIONER_NAMEvalue:nfs-storage- name:NFS_SERVERvalue:192.168.15.143- name:NFS_PATHvalue:/home/data/nfs/datavolumes:- name:nfs-client-rootnfs:server:192.168.15.143path:/home/data/nfs/data 注释掉了 ServiceAccount，因为在 deploy/rbac.yaml 中已经定义了该 ServiceAccount 修改后的 deploy/class.yaml 内容如下 kind:StorageClassmetadata:name:managed-nfs-storageannotations:storageclass.kubernetes.io/is-default-class:\"true\"# 将该sc配置为默认的scprovisioner:nfs-storage# or choose another name, must match deployment's env PROVISIONER_NAME'parameters:archiveOnDelete:\"false\" 部署 kubectl create -f deploy/deployment.yaml -f deploy/class.yaml 查看安装是否成功 $ kubectl get pods -n nfs-storageclass NAME READY STATUS RESTARTS AGE nfs-client-provisioner-699948c999-45p56 1/1 Running 0 108s $ kubectl get sc NAME PROVISIONER AGE managed-nfs-storage (default) nfs-storage 2m37s 测试 kubectl create -f deploy/test-claim.yaml -f deploy/test-pod.yaml 查看 pvc 是否挂载成功 [root@k8s-m1 nfs-clie","date":"2019-10-13","objectID":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/:3:6","tags":["docker","k8s","devops"],"title":"k8s v1.15.0 二进制方式部署","uri":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/"},{"categories":null,"content":"Harbor 阅读 Harbor 官方文档 Harbor-v1.9.1-官方文档阅读笔记 安装 Harbor 本文使用 helm 安装 Harbor，使用 nfs-storageclass 作为 harbor 的后端存储。 参考：https://github.com/goharbor/harbor-helm/tree/1.2.0 在 harbor-helm 发布页面下载发布包 wget https://github.com/goharbor/harbor-helm/archive/v1.2.1.tar.gz 解压发布包 tar -zxvf v1.2.1.tar.gz 编辑 values.yaml ，修改 harbor-helm 的配置 配置这么暴露 harboar 的服务，默认是 Ingress，本文使用 Ingress 暴漏服务，所以不用修改 tls 默认是开启的。如果使用 ingress 暴漏服务的时候，tls是关闭的，则需要在执行 pull/push 命令的时候加上端口号。[见issues](Refer to https://github.com/goharbor/harbor/issues/5291) 配置 tls secret，对应 values.yaml 中的secretName 和 notarySecretName，若 notarySecretName不指定的时候，会使用secretName 的证书和key，secretName 指定 harbor 管理界面域名使用的的https证书，notarySecretName 中指定的 notary 页面使用的 https 证书。secretName 中指定的 secret 需要包含 tls.crt 、tls.key 和 ca.crt， secret 中证书的内容和 ingress secret 的内容相同，所以可以使用 ingress 的证书的手动生成方式或使用 cert-manager 进行生成。secretName和notarySecretName不要配置成一样的，否则其中一个页面无法 https。 配置 expose.ingress.hosts，将 core 和 notary 分别配置为期望的域名。 ingress annotatiaons 中添加 cert-manager.io/cluster-issuer: “cluster-issuer”，使用 cert-manager 自动生成包含证书的 secret。 配置 externalURL，因为本文使用的是 ingress 暴漏服务，所以此处的域名应该和 expose.ingress.hosts.core 配置的域名相同 配置如何持久化数据，默认使用 storageclass 持久化，上面安装了 nfs-storage ，并设为了默认的 sc ，可以使用它来动态提供 pv，满足 pvc 的要求，所以不用修改 helm install 的方式 从 chart 仓库中安装 helm install stable/mysql 从本地的 chart 包安装 helm install foo-0.1.1.tgz 从 chart 包解压的目录进行安装 helm install path/to/foo 通过完整 URL 安装 helm install https://example.com/charts/foo-1.2.3.tgz 修改后的 values.yaml 和原文对比如下： [root@k8s-m1 harbor-helm-1.2.1]# diff values.yaml values.yaml.orig 19c19 \u003c secretName: \"core.harbor.ingress.secret\" --- \u003e secretName: \"\" 23c23 \u003c notarySecretName: \"notary.harbor.ingress.secret\" --- \u003e notarySecretName: \"\" 29,30c29,30 \u003c core: core.harbor.k8s.abc \u003c notary: notary.harbor.k8s.abc --- \u003e core: core.harbor.domain \u003e notary: notary.harbor.domain 41d40 \u003c cert-manager.io/cluster-issuer: \"cluster-issuer\" 102c101 \u003c externalURL: https://core.harbor.k8s.abc --- \u003e externalURL: https://core.harbor.domain annotations 添加 cert-manager.io/cluster-issuer: cluster-issuer，cert-manager 监听到后会新建 ingress tls 的 secret。 安装 harbor-helm 新建 harbor-helm 安装的命名空间 kubectl create ns harbor 在 values.yaml 所在的目录进行安装，所以 install 后面是一个点，表示当前目录。--namespace 指定安装的命名空间，--name 指定安装后的名字 [root@k8s-m1 harbor-helm-1.2.1]# helm install . --namespace harbor --name harbor-helm --values values.yaml helm v3 使用如下命令安装 [root@k8s-m1 harbor-helm-1.2.2]# helm install . --namespace harbor --name-template harbor-helm --values values.yaml 登录管理界面 查看生成的 ingress $ kubectl get ing -n harbor NAME HOSTS ADDRESS PORTS AGE harbor-helm-harbor-ingress core.harbor.k8s.abc,notary.harbor.k8s.abc 192.168.15.144 80, 443 7m51s 在 win10 hosts文件中，添加 192.168.15.144 *.harbor.k8s.abc，然后通过浏览器访问 https://core.harbor.k8s.abc 和 https://notary.harbor.k8s.abc ，https 是安全的。 harbor 管理界面 https://core.harbor.k8s.abc 的默认用户名密码为 admin/Harbor12345 通过 docker client 推送或拉取镜像 docker client 默认使用 https 连接镜像仓库，可以通过这个文档了解如何配置使用 http 连接镜像仓库。 如果一个私有镜像仓库使用http 或使用未知ca的https 提供服务，需要将 --insecure-registry myregistrydomain.com 添加到 docker client 的守护进程启动参数中。 对于使用HTTPS的镜像仓库，如果您有权访问镜像仓库的CA证书，只需将CA证书放在/etc/docker/certs.d/myregistrydomain.com/ca.crt。 下载镜像 如果镜像属于私有的项目，你首先需要登录 docker login core.harbor.k8s.abc 然后才可以下载镜像 docker pull core.harbor.k8s.abc/library/centos:latest 推送镜像 推送镜像之前，必须通过 harbor UI 新建一个对应的项目，如新建一个 demo 项目 然后通过 docker client 登录 harbor docker login core.harbor.k8s.abc 为 镜像 打标签 docker tag centos:latest core.harbor.k8s.abc/demo/centos:latest 推送镜像到 harbor 仓库 docker push core.harbor.k8s.abc/demo/centos:latest ","date":"2019-10-13","objectID":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/:3:7","tags":["docker","k8s","devops"],"title":"k8s v1.15.0 二进制方式部署","uri":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/"},{"categories":null,"content":"tidb 官方文档 https://pingcap.com/docs-cn/v3.0/tidb-in-kubernetes/tidb-operator-overview/ 安装 TiDB-v3.0-安装记录 ","date":"2019-10-13","objectID":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/:3:8","tags":["docker","k8s","devops"],"title":"k8s v1.15.0 二进制方式部署","uri":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/"},{"categories":null,"content":"jenkins jenkins master jenkins master 镜像基于 jenkins/jenkins:lts 进行构建，jinkins/jenkins:lts 的介绍详见： https://github.com/jenkinsci/docker 因为 oracle jdk 无法直接下载（官网下载方式需要登录），所以无法直接在 Dockerfile 里指定下载地址，此处通过在 build 时使用 --build-arg 参数指定 jdk 的下载地址，jdk 的下载地址可以通过 onedriver 生成，生成的地址虽然只有一个小时有效期，但是足够进行构建镜像了。为了加速构建镜像，此处使用 dockerhub 进行镜像的构建，所以需要在 Dockerfile 所在的目录新建一个 hooks 目录，里面放入构建文件 build。 jenkins master 的Dockerfile 等构建资料详见：https://github.com/msdemt/dockerfiles/tree/master/jenkins/jenkins-master 关于为什么不使用 COPY 命令将 jdk 包传入镜像进行构建的原因，是因为 jdk 的 tar 包有 185mb，如果使用 COPY 命令，则生成的镜像会多出一层，这层有185mb ，即使在后面的 RUN 命令中删除了 tar 包，这层还是存在的，从而导致生成的镜像比较大，所以将下载 jdk，删除多余文件都放在了一个 RUN 命令中执行。 关于使用 dockerhub 构建镜像如何传入 ARG 参数的方法详见：https://github.com/docker/hub-feedback/issues/508 使用 dockerhub 生成镜像，名为 hekai/jenkins-slaver-oraclejdk8-debian:20191029 下载镜像后，为镜像重新打 tag ，使镜像名符合 harbor 仓库的要求 docker tag hekai/jenkins-slaver-oraclejdk8-debian:20191029 core.harbor.k8s.abc/library/jenkins-slaver-oraclejdk8-debian:20191029 将生成后的镜像推送到 harbor 仓库 docker push core.harbor.k8s.abc/library/jenkins-slaver-oraclejdk8-debian:20191029 修改 jenkins.yaml 中的镜像名为 core.harbor.k8s.abc/library/jenkins-slaver-oraclejdk8-debian:20191029，执行清单文件，生成 jenkins 服务 kubectl apply -f jenkins.yaml kubectl apply -f jenkins-ing.yml 查看 jenkins master 的 pod，可以看到 jenkins 服务第一次启动生成的默认密码 $ kubectl get pods -n jenkins-ns NAME READY STATUS RESTARTS AGE jenkins-deploy-7dbcdd9848-d8b27 0/1 Running 0 42s $ kubectl logs -n jenkins-ns jenkins-deploy-7dbcdd9848-d8b27 ... ... ************************************************************* Jenkins initial setup is required. An admin user has been created and a password generated. Please use the following password to proceed to installation: a9ad8fbef3e448c0b208c54fd5313a24 This may also be found at: /var/jenkins_home/secrets/initialAdminPassword ************************************************************* ... ... 2019-10-25 07:18:54.943+0000 [id=39] INFO hudson.util.Retrier#start: The attempt #1 to do the action check updates server failed with an allowed exception: java.net.SocketTimeoutException: connect timed out at java.net.PlainSocketImpl.socketConnect(Native Method) ... ... jenkins pod 启动后，会访问 https://updates.jenkins.io/update-center.json ，下载 updates 资源，生成 updates 目录，如果访问不通，则 jenkins 的 pod 会处于 0/1 的状态。 如下 $ kubectl get pods -n jenkins-ns NAME READY STATUS RESTARTS AGE jenkins-deploy-7dbcdd9848-rrgct 0/1 Running 0 77s 查看该 pod 的日志，会发现有个 SocketTimeoutException 的错误。 需要修改 /var/jenkins_home/ 中的 hudson.model.UpdateCenter.xml 文件中指定的更新地址。 因为本文使用的使 nfs 挂载的 jenkins，在 jenkins.yaml 中找到jenkins 容器的 /var/jenkins_home 对应的 pvc 的名字，本文为 jenkins-home-claim，然后在 nfs server 挂载目录中找到对应的挂载文件夹，本文为 /home/data/nfs/data/jenkins-ns-jenkins-home-claim-pvc-4662aeda-4628-4425-a9f1-5a6461425ca5 [root@c48-2 jenkins-ns-jenkins-home-claim-pvc-4662aeda-4628-4425-a9f1-5a6461425ca5]# ls config.xml hudson.model.UpdateCenter.xml jenkins.install.UpgradeWizard.state jobs nodeMonitors.xml plugins secret.key.not-so-secret userContent war copy_reference_file.log identity.key.enc jenkins.telemetry.Correlator.xml logs nodes secret.key secrets users [root@c48-2 jenkins-ns-jenkins-home-claim-pvc-4662aeda-4628-4425-a9f1-5a6461425ca5]# cat hudson.model.UpdateCenter.xml \u003c?xml version='1.1' encoding='UTF-8'?\u003e \u003csites\u003e \u003csite\u003e \u003cid\u003edefault\u003c/id\u003e \u003curl\u003ehttps://updates.jenkins.io/update-center.json\u003c/url\u003e \u003c/site\u003e \u003c/sites\u003e jenkins 第一次启动会访问 hudson.model.UpdateCenter.xml 中指定的地址，生成 updates 目录及其内容，因为默认的地址为 https://updates.jenkins.io/update-center.json ，该地址国内可能无法访问，所以 jenkins 没有启动成功。 将该地址修改为清华大学 jenkins 镜像源的地址 https://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.json，手工修改或执行如下命令修改 sed -i 's/updates.jenkins.io/mirrors.tuna.tsinghua.edu.cn\\/jenkins\\/updates/g' hudson.model.UpdateCenter.xml 修改后，重启 jenkins 对应的 pod kubectl delete pods -n jenkins-ns jenkins-deploy-7dbcdd9848-d8b27 jenkins 服务启动成功，生成了 updates 文件夹 [root@c48-2 jenkins-ns-jenkins-home-claim-pvc-4662aeda-4628-4425-a9f1-5a6461425ca5]# ls config.xml hudson.model.UpdateCenter.xml jenkins","date":"2019-10-13","objectID":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/:3:9","tags":["docker","k8s","devops"],"title":"k8s v1.15.0 二进制方式部署","uri":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/"},{"categories":null,"content":"rook-ceph ","date":"2019-10-13","objectID":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/:3:10","tags":["docker","k8s","devops"],"title":"k8s v1.15.0 二进制方式部署","uri":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/"},{"categories":null,"content":"istio ","date":"2019-10-13","objectID":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/:3:11","tags":["docker","k8s","devops"],"title":"k8s v1.15.0 二进制方式部署","uri":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/"},{"categories":null,"content":"knative ","date":"2019-10-13","objectID":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/:3:12","tags":["docker","k8s","devops"],"title":"k8s v1.15.0 二进制方式部署","uri":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/"},{"categories":null,"content":"wayne ","date":"2019-10-13","objectID":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/:3:13","tags":["docker","k8s","devops"],"title":"k8s v1.15.0 二进制方式部署","uri":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/"},{"categories":null,"content":"prometheus ","date":"2019-10-13","objectID":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/:3:14","tags":["docker","k8s","devops"],"title":"k8s v1.15.0 二进制方式部署","uri":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/"},{"categories":null,"content":"efk ","date":"2019-10-13","objectID":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/:3:15","tags":["docker","k8s","devops"],"title":"k8s v1.15.0 二进制方式部署","uri":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/"},{"categories":null,"content":"gitlab gitlab 安装记录 使用 k8s 安装 gitlab https://docs.gitlab.com/charts/ gitlab 官方的 helm chart 地址：https://gitlab.com/gitlab-org/charts/gitlab 下载 master 分支的 chart mkdir /home/k8s/gitlab \u0026\u0026 cd /home/k8s/gitlab wget https://gitlab.com/gitlab-org/charts/gitlab/-/archive/master/gitlab-master.tar.gz tar -zxf gitlab-master.tar.gz 下载依赖的子chart helm dep up ./gitlab-master coredns 挂载宿主机 /etc/hosts 并配置 hosts 插件 export release=mygitlab kubectl create ns gitlab 使用 CA 证书生成 gitlab-runner 连接 gitlab server 认证用的 secret kubectl create secret generic gitlab.k8s.abc.crt –from-file=gitlab.k8s.abc.crt=ca.crt -n gitlab 生成 gitlab server 与 registry 交互使用的证书 生成私钥和证书签名请求 openssl req -new -newkey rsa:4096 -subj “/CN=gitlab-issuer” -nodes -keyout registry-k8s-abc.key -out registry-k8s-abc.csr 使用CA证书根据证书签名请求签发证书 openssl x509 -req -sha256 -days 365 -in registry-k8s-abc.csr -CA ca.crt -CAkey ca.key -set_serial 01 -out registry-k8s-abc.crt -n gitlab 原文是自签名生成一个证书和私钥 openssl req -new -newkey rsa:4096 -subj “/CN=gitlab-issuer” -nodes -x509 -keyout certs/registry-example-com.key -out certs/registry-example-com.crt 根据生成的证书和私钥建一个secret kubectl create secret generic ${release}-registry-secret –from-file=registry-auth.key=registry-k8s-abc.key –from-file=registry-auth.crt=registry-k8s-abc.crt -n gitlab 然后生成yaml时可以通过 global.registry.certificate.secret 引用该 secret 生成 incoming secret所需的 kubectl create secret generic gitlab-incoming-imap-pwd-secret –from-file=./gitlab-incoming-imap-pwd-key -n gitlab kubectl create secret generic gitlab-outgoing-smtp-pwd-secret –from-file=gitlab-outgoing-smtp-pwd-key=gitlab-incoming-imap-pwd-key -n gitlab 生成 gitlab.yaml helm template ./gitlab-master –name ${release} –set global.edition=ce –set global.time_zone=Beijing –set global.hosts.domain=k8s.abc –set nginx-ingress.enabled=false –set global.ingress.class=nginx –set prometheus.install=false –set certmanager.install=false –set global.ingress.configureCertmanager=false –set gitlab.unicorn.ingress.tls.secretName=release-gitlab-tls –set registry.ingress.tls.secretName=release-registry-tls –set minio.ingress.tls.secretName=release-minio-tls –set global.ingress.annotations.“cert-manager.io/cluster-issuer”=cluster-issuer –set global.registry.certificate.secret=${release}-registry-secret –namespace gitlab \u003e gitlab.yaml 检查 gitlab.yaml，少 namespace的加上 namespace，imageTag 为 latest 的imagePullPolicy配置为Alaways,其他的配置为IfNotPresent 将 gitlab.k8s.abc.crt 挂载到 gitlab-runner pod 的/home/gitlab-runner/.gitlab-runner/certs/ 然后就可以通过 gitlab.yaml 安装gitlab了。 获取gitlab root 用户的密码 kubectl get secret -n gitlab mygitlab-gitlab-initial-root-password -ojsonpath='{.data.password}' | base64 –decode ; echo gitlab 配置smtp，用来新建用户时为用户发送邮件 根据文档里的描述配置 smtp 参数后，新建用户时，查看容器日志 kubectl logs -n gitlab -f mygitlab-sidekiq-all-in-1-b8b4d49cd-947nb 邮件未成功发送，日志中出现错误 [ActiveJob] [ActionMailer::DeliveryJob] [962a7c24-d6a8-4450-a8ff-5356779d2f4a] Sent mail to 276516848@qq.com (17488.3ms) [ActiveJob] [ActionMailer::DeliveryJob] [962a7c24-d6a8-4450-a8ff-5356779d2f4a] Error performing ActionMailer::DeliveryJob (Job ID: 962a7c24-d6a8-4450-a8ff-5356779d2f4a) from Sidekiq(mailers) in 17550.92ms: EOFError (end of file reached): 百度下错误：EOFError (end of file reached) 见：https://www.jianshu.com/p/af142a66d781 如果 smtp 使用 25 端口，则不要配置 ssl 相关的项目 如果 smtp 使用 465 或其他端口，则需要配置如下参数 gitlab_rails['smtp_enable_starttls_auto'] = true gitlab_rails['smtp_tls'] = true gitlab_rails['smtp_openssl_verify_mode'] = 'none' 所以文件配置如下： helm template . \\ --name ${release} \\ --set global.edition=ce \\ --set global.time_zone=Beijing \\ --set global.hosts.domain=k8s.abc \\ --set global.grafana.enabled=false \\ --set global.ingress.class=nginx \\ --set global.ingress.configureCertmanager=false \\ --set gitlab.unicorn.ingress.tls.secretName=release-gitlab-tls \\ --set registry.ingress.tls.secretName=release-registry-tls \\ --set minio.ingress.tls.secretName=release-minio-tls \\ --set global.ingress.annotations.\"cert-manager\\.io\\/cluster-issuer\"=cluster-issuer \\ --set global.registry.certificate.secret=${rel","date":"2019-10-13","objectID":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/:3:16","tags":["docker","k8s","devops"],"title":"k8s v1.15.0 二进制方式部署","uri":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/"},{"categories":null,"content":"external-dns debian中使用 ip 命令 apt install iproute2 容器内获取物理机ip https://blog.csdn.net/kozazyh/article/details/79463688 https://github.com/kubernetes/kubernetes/issues/24657 https://kubernetes.io/docs/tasks/inject-data-application/environment-variable-expose-pod-information/ 使用 hostNetwork 使用环境变量的方式 coredns 的 hosts 插件无法动态更新 /etc/hosts里的地址 ","date":"2019-10-13","objectID":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/:3:17","tags":["docker","k8s","devops"],"title":"k8s v1.15.0 二进制方式部署","uri":"/posts/k8s/k8s-v1.15.0-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2/"},{"categories":null,"content":"helm 官方文档地址：https://helm.sh/docs/ The package manager for Kubernetes helm 是 kubernetes 的包管理器。 NOTE: Kubernetes versions prior to 1.6 have limited or no support for role-based access controls (RBAC). Helm will figure out where to install Tiller by reading your Kubernetes configuration file (usually $HOME/.kube/config). This is the same file that kubectl uses. To find out which cluster Tiller would install to, you can run kubectl config current-context or kubectl cluster-info. [root@k8s-m1 ~]# kubectl config current-context kubernetes-admin@kubernetes ","date":"2019-10-13","objectID":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:0:0","tags":["k8s","tag2","tag3"],"title":"Helm v2.14.3 官方文档阅读笔记","uri":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Understand your Security Context As with all powerful tools, ensure you are installing it correctly for your scenario. 与所有强大的工具一样，请确保针对您的情况正确安装了它。 If you’re using Helm on a cluster that you completely control, like minikube or a cluster on a private network in which sharing is not a concern, the default installation – which applies no security configuration – is fine, and it’s definitely the easiest. To install Helm without additional security steps, install Helm and then initialize Helm. However, if your cluster is exposed to a larger network or if you share your cluster with others – production clusters fall into this category(生产环境集群属于这一类) – you must take extra steps to secure your installation to prevent careless or malicious(恶意的) actors from damaging the cluster or its data. To apply configurations that secure Helm for use in production environments and other multi-tenant scenarios(多租户场景), see Securing a Helm installation If your cluster has Role-Based Access Control (RBAC) enabled, you may want to configure a service account and rules before proceeding. ","date":"2019-10-13","objectID":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:1:0","tags":["k8s","tag2","tag3"],"title":"Helm v2.14.3 官方文档阅读笔记","uri":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Role-based Access Control In Kubernetes, granting a role to an application-specific service account is a best practice to ensure that your application is operating in the scope that you have specified. Read more about service account permissions in the official Kubernetes docs. Bitnami also has a fantastic guide for configuring RBAC in your cluster that takes you through RBAC basics. This guide is for users who want to restrict(限制) Tiller’s capabilities(能力) to install resources to certain namespaces, or to grant a Helm client running access to a Tiller instance. TILLER AND ROLE-BASED ACCESS CONTROL You can add a service account to Tiller using the --service-account \u003cNAME\u003e flag while you’re configuring Helm. As a prerequisite, you’ll have to create a role binding which specifies a role and a service account name that have been set up in advance. Once you have satisfied the pre-requisite and have a service account with the correct permissions, you’ll run a command like this: helm init --service-account \u003cNAME\u003e Example: Service account with cluster-admin role In rbac-config.yaml: apiVersion: v1 kind: ServiceAccount metadata: name: tiller namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: tiller roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: tiller namespace: kube-system Note: The cluster-admin role is created by default in a Kubernetes cluster, so you don’t have to define it explicitly. $ kubectl create -f rbac-config.yaml serviceaccount \"tiller\" created clusterrolebinding \"tiller\" created helm init --service-account tiller --history-max 200 Example: Deploy Tiller in a namespace, restricted to deploying resources only in that namespace In the example above, we gave Tiller admin access to the entire cluster. You are not at all required to give Tiller cluster-admin access for it to work. Instead of specifying a ClusterRole or a ClusterRoleBinding, you can specify a Role and RoleBinding to limit Tiller’s scope to a particular namespace. $ kubectl create namespace tiller-world namespace \"tiller-world\" created $ kubectl create serviceaccount tiller --namespace tiller-world serviceaccount \"tiller\" created Define a Role that allows Tiller to manage all resources in tiller-world like in role-tiller.yaml: kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: name: tiller-manager namespace: tiller-world rules: - apiGroups: [\"\", \"batch\", \"extensions\", \"apps\"] resources: [\"*\"] verbs: [\"*\"] $ kubectl create -f role-tiller.yaml role \"tiller-manager\" created In rolebinding-tiller.yaml, kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: tiller-binding namespace: tiller-world subjects: - kind: ServiceAccount name: tiller namespace: tiller-world roleRef: kind: Role name: tiller-manager apiGroup: rbac.authorization.k8s.io $ kubectl create -f rolebinding-tiller.yaml rolebinding \"tiller-binding\" created Afterwards you can run helm init to install Tiller in the tiller-world namespace. $ helm init --service-account tiller --tiller-namespace tiller-world $HELM_HOME has been configured at /Users/awesome-user/.helm. Tiller (the Helm server side component) has been installed into your Kubernetes Cluster. $ helm install stable/lamp --tiller-namespace tiller-world --namespace tiller-world NAME: wayfaring-yak LAST DEPLOYED: Mon Aug 7 16:00:16 2017 NAMESPACE: tiller-world STATUS: DEPLOYED RESOURCES: ==\u003e v1/Pod NAME READY STATUS RESTARTS AGE wayfaring-yak-alpine 0/1 ContainerCreating 0 0s Example: Deploy Tiller in a namespace, restricted to deploying resources in another namespace In the example above, we gave Tiller admin access to the namespace it was deployed inside. Now, let’s limit Tiller’s scope to deploy resources in a different namespace! For example, let’s install Tiller in the namespace myorg-system and allow Tiller to deploy resources in the namespace myorg-users. $ kubectl create namespac","date":"2019-10-13","objectID":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:1:1","tags":["k8s","tag2","tag3"],"title":"Helm v2.14.3 官方文档阅读笔记","uri":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"INSTALL HELM Download a binary release of the Helm client. You can use tools like homebrew, or look at the official releases page. For more details, or for other options, see the installation guide. There are two parts to Helm: The Helm client (helm) and the Helm server (Tiller). This guide shows how to install the client, and then proceeds to show two ways to install the server. IMPORTANT: If you are responsible for ensuring your cluster is a controlled environment, especially when resources are shared, it is strongly recommended installing Tiller using a secured configuration. For guidance, see Securing your Helm Installation. ","date":"2019-10-13","objectID":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:2:0","tags":["k8s","tag2","tag3"],"title":"Helm v2.14.3 官方文档阅读笔记","uri":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"INSTALLING THE HELM CLIENT Every release of Helm provides binary releases for a variety of OSes. These binary versions can be manually downloaded and installed. Download your desired version Unpack it (tar -zxvf helm-v2.0.0-linux-amd64.tgz) Find the helm binary in the unpacked directory, and move it to its desired destination (mv linux-amd64/helm /usr/local/bin/helm) From there, you should be able to run the client: helm help. ","date":"2019-10-13","objectID":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:2:1","tags":["k8s","tag2","tag3"],"title":"Helm v2.14.3 官方文档阅读笔记","uri":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"INSTALLING TILLER Tiller, the server portion of Helm, typically runs inside of your Kubernetes cluster. But for development, it can also be run locally, and configured to talk to a remote Kubernetes cluster Special Note for RBAC Users: Most cloud providers enable a feature called Role-Based Access Control - RBAC for short. If your cloud provider enables this feature, you will need to create a service account for Tiller with the right roles and permissions to access resources. Check the Kubernetes Distribution Guide to see if there’s any further points of interest on using Helm with your cloud provider. Also check out the guide on Tiller and Role-Based Access Control for more information on how to run Tiller in an RBAC-enabled Kubernetes cluster. Easy In-Cluster Installation The easiest way to install tiller into the cluster is simply to run helm init. This will validate that helm’s local environment is set up correctly (and set it up if necessary). Then it will connect to whatever cluster kubectl connects to by default (kubectl config view). Once it connects, it will install tiller into the kube-system namespace. After helm init, you should be able to run kubectl get pods --namespace kube-system and see Tiller running. You can explicitly tell helm init to… Install the canary(金丝雀) build with the --canary-image flag Install a particular image (version) with --tiller-image Install to a particular cluster with --kube-context Install into a particular namespace with --tiller-namespace Install Tiller with a Service Account with --service-account (for RBAC enabled clusters) Install Tiller without mounting a service account with --automount-service-account false Once Tiller is installed, running helm version should show you both the client and server version. (If it shows only the client version, helm cannot yet connect to the server. Use kubectl to see if any tiller pods are running.) Helm will look for Tiller in the kube-system namespace unless --tiller-namespace or TILLER_NAMESPACE is set. Installing Tiller Canary Builds Canary images are built from the master branch. They may not be stable, but they offer you the chance to test out the latest features. The easiest way to install a canary image is to use helm init with the --canary-image flag: helm init --canary-image This will use the most recently built container image. You can always uninstall Tiller by deleting the Tiller deployment from the kube-system namespace using kubectl. Running Tiller Locally For development, it is sometimes easier to work on Tiller locally, and configure it to connect to a remote Kubernetes cluster. The process of building Tiller is explained above. Once tiller has been built, simply start it: $ bin/tiller Tiller running on :44134 When Tiller is running locally, it will attempt to connect to the Kubernetes cluster that is configured by kubectl. (Run kubectl config view to see which cluster that is.) You must tell helm to connect to this new local Tiller host instead of connecting to the one in-cluster. There are two ways to do this. The first is to specify the --host option on the command line. The second is to set the $HELM_HOST environment variable. export HELM_HOST=localhost:44134 $ helm version # Should connect to localhost. Client: \u0026version.Version{SemVer:\"v2.0.0-alpha.4\", GitCommit:\"db...\", GitTreeState:\"dirty\"} Server: \u0026version.Version{SemVer:\"v2.0.0-alpha.4\", GitCommit:\"a5...\", GitTreeState:\"dirty\"} Importantly, even when running locally, Tiller will store release configuration in ConfigMaps inside of Kubernetes. ","date":"2019-10-13","objectID":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:2:2","tags":["k8s","tag2","tag3"],"title":"Helm v2.14.3 官方文档阅读笔记","uri":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"UPGRADING TILLER As of Helm 2.2.0, Tiller can be upgraded using helm init --upgrade. For older versions of Helm, or for manual upgrades, you can use kubectl to modify the Tiller image: export TILLER_TAG=v2.0.0-beta.1 # Or whatever version you want $ kubectl --namespace=kube-system set image deployments/tiller-deploy tiller=gcr.io/kubernetes-helm/tiller:$TILLER_TAG deployment \"tiller-deploy\" image updated Setting TILLER_TAG=canary will get the latest snapshot of master. ","date":"2019-10-13","objectID":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:2:3","tags":["k8s","tag2","tag3"],"title":"Helm v2.14.3 官方文档阅读笔记","uri":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"DELETING OR REINSTALLING TILLER Because Tiller stores its data in Kubernetes ConfigMaps, you can safely delete and re-install Tiller without worrying about losing any data. The recommended way of deleting Tiller is with kubectl delete deployment tiller-deploy --namespace kube-system, or more concisely helm reset. Tiller can then be re-installed from the client with: helm init ","date":"2019-10-13","objectID":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:2:4","tags":["k8s","tag2","tag3"],"title":"Helm v2.14.3 官方文档阅读笔记","uri":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"INITIALIZE HELM AND INSTALL TILLER Once you have Helm ready, you can initialize the local CLI and also install Tiller into your Kubernetes cluster in one step: helm init --history-max 200 TIP: Setting --history-max on helm init is recommended as configmaps and other objects in helm history can grow large in number if not purged by max limit. Without a max history set the history is kept indefinitely, leaving a large number of records for helm and tiller to maintain. This will install Tiller into the Kubernetes cluster you saw with kubectl config current-context. TIP: Want to install into a different cluster? Use the --kube-context flag. TIP: When you want to upgrade Tiller, just run helm init --upgrade. By default, when Tiller is installed, it does not have authentication enabled. To learn more about configuring strong TLS authentication for Tiller, consult the Tiller TLS guide. INSTALL AN EXAMPLE CHART To install a chart, you can run the helm install command. Helm has several ways to find and install a chart, but the easiest is to use one of the official stable charts. helm repo update # Make sure we get the latest list of charts $ helm install stable/mysql NAME: wintering-rodent LAST DEPLOYED: Thu Oct 18 14:21:18 2018 NAMESPACE: default STATUS: DEPLOYED RESOURCES: ==\u003e v1/Secret NAME AGE wintering-rodent-mysql 0s ==\u003e v1/ConfigMap wintering-rodent-mysql-test 0s ==\u003e v1/PersistentVolumeClaim wintering-rodent-mysql 0s ==\u003e v1/Service wintering-rodent-mysql 0s ==\u003e v1beta1/Deployment wintering-rodent-mysql 0s ==\u003e v1/Pod(related) NAME READY STATUS RESTARTS AGE wintering-rodent-mysql-6986fd6fb-988x7 0/1 Pending 0 0s NOTES: MySQL can be accessed via port 3306 on the following DNS name from within your cluster: wintering-rodent-mysql.default.svc.cluster.local To get your root password run: MYSQL_ROOT_PASSWORD=$(kubectl get secret --namespace default wintering-rodent-mysql -o jsonpath=\"{.data.mysql-root-password}\" | base64 --decode; echo) To connect to your database: 1. Run an Ubuntu pod that you can use as a client: kubectl run -i --tty ubuntu --image=ubuntu:16.04 --restart=Never -- bash -il 2. Install the mysql client: apt-get update \u0026\u0026 apt-get install mysql-client -y 3. Connect using the mysql cli, then provide your password: mysql -h wintering-rodent-mysql -p To connect to your database directly from outside the K8s cluster: MYSQL_HOST=127.0.0.1 MYSQL_PORT=3306 # Execute the following command to route the connection: kubectl port-forward svc/wintering-rodent-mysql 3306 mysql -h ${MYSQL_HOST} -P${MYSQL_PORT} -u root -p${MYSQL_ROOT_PASSWORD} In the example above, the stable/mysql chart was released, and the name of our new release is wintering-rodent. You get a simple idea of the features of this MySQL chart by running helm inspect stable/mysql. Whenever you install a chart, a new release is created. So one chart can be installed multiple times into the same cluster. And each can be independently managed and upgraded. The helm install command is a very powerful command with many capabilities. To learn more about it, check out the Using Helm Guide ","date":"2019-10-13","objectID":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:2:5","tags":["k8s","tag2","tag3"],"title":"Helm v2.14.3 官方文档阅读笔记","uri":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Using Helm This guide explains the basics of using Helm (and Tiller) to manage packages on your Kubernetes cluster. It assumes that you have already installed the Helm client and the Tiller server (typically by helm init). ","date":"2019-10-13","objectID":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:3:0","tags":["k8s","tag2","tag3"],"title":"Helm v2.14.3 官方文档阅读笔记","uri":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"THREE BIG CONCEPTS A Chart is a Helm package. It contains all of the resource definitions necessary to run an application, tool, or service inside of a Kubernetes cluster. Think of it like the Kubernetes equivalent of a Homebrew formula, an Apt dpkg, or a Yum RPM file. A Repository is the place where charts can be collected and shared. It’s like Perl’s CPAN archive or the Fedora Package Database, but for Kubernetes packages. A Release is an instance of a chart running in a Kubernetes cluster. One chart can often be installed many times into the same cluster. And each time it is installed, a new release is created. Consider a MySQL chart. If you want two databases running in your cluster, you can install that chart twice. Each one will have its own release, which will in turn have its own release name. With these concepts in mind, we can now explain Helm like this: Helm installs charts into Kubernetes, creating a new release for each installation. And to find new charts, you can search Helm chart repositories. ","date":"2019-10-13","objectID":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:3:1","tags":["k8s","tag2","tag3"],"title":"Helm v2.14.3 官方文档阅读笔记","uri":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"‘HELM SEARCH’: FINDING CHARTS When you first install Helm, it is preconfigured to talk to the official Kubernetes charts repository. This repository contains a number of carefully curated and maintained charts. This chart repository is named stable by default. You can see which charts are available by running helm search: $ helm search NAME VERSION DESCRIPTION stable/drupal 0.3.2 One of the most versatile open source content m... stable/jenkins 0.1.0 A Jenkins Helm chart for Kubernetes. stable/mariadb 0.5.1 Chart for MariaDB stable/mysql 0.1.0 Chart for MySQL ... With no filter, helm search shows you all of the available charts. You can narrow down your results by searching with a filter: $ helm search mysql NAME VERSION DESCRIPTION stable/mysql 0.1.0 Chart for MySQL stable/mariadb 0.5.1 Chart for MariaDB Now you will only see the results that match your filter. Why is mariadb in the list? Because its package description relates it to MySQL. We can use helm inspect chart to see this: $ helm inspect stable/mariadb Fetched stable/mariadb to mariadb-0.5.1.tgz description: Chart for MariaDB engine: gotpl home: https://mariadb.org keywords: - mariadb - mysql - database - sql ... Search is a good way to find available packages. Once you have found a package you want to install, you can use helm install to install it. ","date":"2019-10-13","objectID":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:3:2","tags":["k8s","tag2","tag3"],"title":"Helm v2.14.3 官方文档阅读笔记","uri":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"‘HELM INSTALL’: INSTALLING A PACKAGE To install a new package, use the helm install command. At its simplest, it takes only one argument: The name of the chart. $ helm install stable/mariadb Fetched stable/mariadb-0.3.0 to /Users/mattbutcher/Code/Go/src/k8s.io/helm/mariadb-0.3.0.tgz NAME: happy-panda LAST DEPLOYED: Wed Sep 28 12:32:28 2016 NAMESPACE: default STATUS: DEPLOYED Resources: ==\u003e extensions/Deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE happy-panda-mariadb 1 0 0 0 1s ==\u003e v1/Secret NAME TYPE DATA AGE happy-panda-mariadb Opaque 2 1s ==\u003e v1/Service NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE happy-panda-mariadb 10.0.0.70 \u003cnone\u003e 3306/TCP 1s Notes: MariaDB can be accessed via port 3306 on the following DNS name from within your cluster: happy-panda-mariadb.default.svc.cluster.local To connect to your database run the following command: kubectl run happy-panda-mariadb-client --rm --tty -i --image bitnami/mariadb --command -- mysql -h happy-panda-mariadb Now the mariadb chart is installed. Note that installing a chart creates a new release object. The release above is named happy-panda. (If you want to use your own release name, simply use the --name flag on helm install.) During installation, the helm client will print useful information about which resources were created, what the state of the release is, and also whether there are additional configuration steps you can or should take. Helm does not wait until all of the resources are running before it exits. Many charts require Docker images that are over 600M in size, and may take a long time to install into the cluster. To keep track of a release’s state, or to re-read configuration information, you can use helm status: $ helm status happy-panda Last Deployed: Wed Sep 28 12:32:28 2016 Namespace: default Status: DEPLOYED Resources: ==\u003e v1/Service NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE happy-panda-mariadb 10.0.0.70 \u003cnone\u003e 3306/TCP 4m ==\u003e extensions/Deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE happy-panda-mariadb 1 1 1 1 4m ==\u003e v1/Secret NAME TYPE DATA AGE happy-panda-mariadb Opaque 2 4m Notes: MariaDB can be accessed via port 3306 on the following DNS name from within your cluster: happy-panda-mariadb.default.svc.cluster.local To connect to your database run the following command: kubectl run happy-panda-mariadb-client --rm --tty -i --image bitnami/mariadb --command -- mysql -h happy-panda-mariadb The above shows the current state of your release. Customizing the Chart Before Installing Installing the way we have here will only use the default configuration options for this chart. Many times, you will want to customize the chart to use your preferred configuration. To see what options are configurable on a chart, use helm inspect values: helm inspect values stable/mariadb Fetched stable/mariadb-0.3.0.tgz to /Users/mattbutcher/Code/Go/src/k8s.io/helm/mariadb-0.3.0.tgz ## Bitnami MariaDB image version ## ref: https://hub.docker.com/r/bitnami/mariadb/tags/ ## ## Default: none imageTag: 10.1.14-r3 ## Specify a imagePullPolicy ## Default to 'Always' if imageTag is 'latest', else set to 'IfNotPresent' ## ref: https://kubernetes.io/docs/user-guide/images/#pre-pulling-images ## # imagePullPolicy: ## Specify password for root user ## ref: https://github.com/bitnami/bitnami-docker-mariadb/blob/master/README.md#setting-the-root-password-on-first-run ## # mariadbRootPassword: ## Create a database user ## ref: https://github.com/bitnami/bitnami-docker-mariadb/blob/master/README.md#creating-a-database-user-on-first-run ## # mariadbUser: # mariadbPassword: ## Create a database ## ref: https://github.com/bitnami/bitnami-docker-mariadb/blob/master/README.md#creating-a-database-on-first-run ## # mariadbDatabase: You can then override any of these settings in a YAML formatted file, and then pass that file during installation. $ cat \u003c\u003c EOF \u003e config.yaml mariadbUser: user0 mariadbDatabase: user0db EOF helm install -f config.yaml stable/mariadb The above will create a defau","date":"2019-10-13","objectID":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:3:3","tags":["k8s","tag2","tag3"],"title":"Helm v2.14.3 官方文档阅读笔记","uri":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"‘HELM UPGRADE’ AND ‘HELM ROLLBACK’: UPGRADING A RELEASE, AND RECOVERING ON FAILURE When a new version of a chart is released, or when you want to change the configuration of your release, you can use the helm upgrade command. An upgrade takes an existing release and upgrades it according to the information you provide. Because Kubernetes charts can be large and complex, Helm tries to perform the least invasive upgrade. It will only update things that have changed since the last release. $ helm upgrade -f panda.yaml happy-panda stable/mariadb Fetched stable/mariadb-0.3.0.tgz to /Users/mattbutcher/Code/Go/src/k8s.io/helm/mariadb-0.3.0.tgz happy-panda has been upgraded. Last Deployed: Wed Sep 28 12:47:54 2016 Namespace: default Status: DEPLOYED ... In the above case, the happy-panda release is upgraded with the same chart, but with a new YAML file: mariadbUser: user1 We can use helm get values to see whether that new setting took effect. $ helm get values happy-panda mariadbUser: user1 The helm get command is a useful tool for looking at a release in the cluster. And as we can see above, it shows that our new values from panda.yaml were deployed to the cluster. Now, if something does not go as planned during a release, it is easy to roll back to a previous release using helm rollback [RELEASE] [REVISION]. helm rollback happy-panda 1 The above rolls back our happy-panda to its very first release version. A release version is an incremental revision. Every time an install, upgrade, or rollback happens, the revision number is incremented by 1. The first revision number is always 1. And we can use helm history [RELEASE] to see revision numbers for a certain release. ","date":"2019-10-13","objectID":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:3:4","tags":["k8s","tag2","tag3"],"title":"Helm v2.14.3 官方文档阅读笔记","uri":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"HELPFUL OPTIONS FOR INSTALL/UPGRADE/ROLLBACK There are several other helpful options you can specify for customizing the behavior of Helm during an install/upgrade/rollback. Please note that this is not a full list of cli flags. To see a description of all flags, just run helm \u003ccommand\u003e --help. --timeout: A value in seconds to wait for Kubernetes commands to complete This defaults to 300 (5 minutes) --wait: Waits until all Pods are in a ready state, PVCs are bound, Deployments have minimum (Desired minus maxUnavailable) Pods in ready state and Services have an IP address (and Ingress if a LoadBalancer) before marking the release as successful. It will wait for as long as the --timeout value. If timeout is reached, the release will be marked as FAILED. Note: In scenario where Deployment has replicas set to 1 and maxUnavailable is not set to 0 as part of rolling update strategy, --wait will return as ready as it has satisfied the minimum Pod in ready condition. --no-hooks: This skips running hooks for the command --recreate-pods (only available for upgrade and rollback): This flag will cause all pods to be recreated (with the exception of pods belonging to deployments) ","date":"2019-10-13","objectID":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:3:5","tags":["k8s","tag2","tag3"],"title":"Helm v2.14.3 官方文档阅读笔记","uri":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"‘HELM DELETE’: DELETING A RELEASE When it is time to uninstall or delete a release from the cluster, use the helm delete command: helm delete happy-panda 使用时加上 --purge 参数，会彻底删除这个 release --purge remove the release from the store and make its name free for later use This will remove the release from the cluster. You can see all of your currently deployed releases with the helm list command: $ helm list NAME VERSION UPDATED STATUS CHART inky-cat 1 Wed Sep 28 12:59:46 2016 DEPLOYED alpine-0.1.0 From the output above, we can see that the happy-panda release was deleted. However, Helm always keeps records of what releases happened. Need to see the deleted releases? helm list --deleted shows those, and helm list --all shows all of the releases (deleted and currently deployed, as well as releases that failed): $ helm list --all NAME VERSION UPDATED STATUS CHART happy-panda 2 Wed Sep 28 12:47:54 2016 DELETED mariadb-0.3.0 inky-cat 1 Wed Sep 28 12:59:46 2016 DEPLOYED alpine-0.1.0 kindred-angelf 2 Tue Sep 27 16:16:10 2016 DELETED alpine-0.1.0 Because Helm keeps records of deleted releases, a release name cannot be re-used. (If you really need to re-use a release name, you can use the --replace flag, but it will simply re-use the existing release and replace its resources.) Note that because releases are preserved in this way, you can rollback a deleted resource, and have it re-activate. ","date":"2019-10-13","objectID":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:3:6","tags":["k8s","tag2","tag3"],"title":"Helm v2.14.3 官方文档阅读笔记","uri":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"‘HELM REPO’: WORKING WITH REPOSITORIES So far, we’ve been installing charts only from the stable repository. But you can configure helm to use other repositories. Helm provides several repository tools under the helm repo command. You can see which repositories are configured using helm repo list: $ helm repo list NAME URL stable https://kubernetes-charts.storage.googleapis.com local http://localhost:8879/charts mumoshu https://mumoshu.github.io/charts And new repositories can be added with helm repo add: helm repo add dev https://example.com/dev-charts Because chart repositories change frequently, at any point you can make sure your Helm client is up to date by running helm repo update. ","date":"2019-10-13","objectID":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:3:7","tags":["k8s","tag2","tag3"],"title":"Helm v2.14.3 官方文档阅读笔记","uri":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"CREATING YOUR OWN CHARTS The Chart Development Guide explains how to develop your own charts. But you can get started quickly by using the helm create command: $ helm create deis-workflow Creating deis-workflow Now there is a chart in ./deis-workflow. You can edit it and create your own templates. As you edit your chart, you can validate that it is well-formatted by running helm lint. When it’s time to package the chart up for distribution, you can run the helm package command: $ helm package deis-workflow deis-workflow-0.1.0.tgz And that chart can now easily be installed by helm install: $ helm install ./deis-workflow-0.1.0.tgz ... Charts that are archived can be loaded into chart repositories. See the documentation for your chart repository server to learn how to upload. Note: The stable repository is managed on the Helm Charts GitHub repository. That project accepts chart source code, and (after audit) packages those for you. ","date":"2019-10-13","objectID":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:3:8","tags":["k8s","tag2","tag3"],"title":"Helm v2.14.3 官方文档阅读笔记","uri":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"TILLER, NAMESPACES AND RBAC In some cases you may wish to scope Tiller or deploy multiple Tillers to a single cluster. Here are some best practices when operating in those circumstances. Tiller can be installed into any namespace. By default, it is installed into kube-system. You can run multiple Tillers provided they each run in their own namespace. Limiting Tiller to only be able to install into specific namespaces and/or resource types is controlled by Kubernetes RBAC roles and rolebindings. You can add a service account to Tiller when configuring Helm via helm init --service-account \u003cNAME\u003e. You can find more information about that here. Release names are unique PER TILLER INSTANCE. Charts should only contain resources that exist in a single namespace. It is not recommended to have multiple Tillers configured to manage resources in the same namespace. ","date":"2019-10-13","objectID":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:3:9","tags":["k8s","tag2","tag3"],"title":"Helm v2.14.3 官方文档阅读笔记","uri":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"CONCLUSION This chapter has covered the basic usage patterns of the helm client, including searching, installation, upgrading, and deleting. It has also covered useful utility commands like helm status, helm get, and helm repo. For more information on these commands, take a look at Helm’s built-in help: helm help. In the next chapter, we look at the process of developing charts. ","date":"2019-10-13","objectID":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:3:10","tags":["k8s","tag2","tag3"],"title":"Helm v2.14.3 官方文档阅读笔记","uri":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Charts Helm uses a packaging format called charts. A chart is a collection of files that describe a related set of Kubernetes resources. A single chart might be used to deploy something simple, like a memcached pod, or something complex, like a full web app stack with HTTP servers, databases, caches, and so on. Charts are created as files laid out in a particular directory tree, then they can be packaged into versioned archives to be deployed. This document explains the chart format, and provides basic guidance for building charts with Helm. ","date":"2019-10-13","objectID":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:4:0","tags":["k8s","tag2","tag3"],"title":"Helm v2.14.3 官方文档阅读笔记","uri":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"THE CHART FILE STRUCTURE A chart is organized as a collection of files inside of a directory. The directory name is the name of the chart (without versioning information). Thus, a chart describing WordPress would be stored in the wordpress/ directory. Inside of this directory, Helm will expect a structure that matches this: wordpress/ Chart.yaml # A YAML file containing information about the chart LICENSE # OPTIONAL: A plain text file containing the license for the chart README.md # OPTIONAL: A human-readable README file requirements.yaml # OPTIONAL: A YAML file listing dependencies for the chart values.yaml # The default configuration values for this chart charts/ # A directory containing any charts upon which this chart depends. templates/ # A directory of templates that, when combined with values, # will generate valid Kubernetes manifest files. templates/NOTES.txt # OPTIONAL: A plain text file containing short usage notes Helm reserves use of the charts/ and templates/ directories, and of the listed file names. Other files will be left as they are. THE CHART.YAML FILE The Chart.yaml file is required for a chart. It contains the following fields: apiVersion:The chart API version, always \"v1\" (required)name:The name of the chart (required)version:A SemVer 2 version (required)kubeVersion:A SemVer range of compatible Kubernetes versions (optional)description:A single-sentence description of this project (optional)keywords:- A list of keywords about this project (optional)home:The URL of this project's home page (optional)sources:- A list of URLs to source code for this project (optional)maintainers:# (optional)- name:The maintainer's name (required for each maintainer)email:The maintainer's email (optional for each maintainer)url:A URL for the maintainer (optional for each maintainer)engine:gotpl# The name of the template engine (optional, defaults to gotpl)icon:A URL to an SVG or PNG image to be used as an icon (optional).appVersion:The version of the app that this contains (optional). This needn't be SemVer.deprecated:Whether this chart is deprecated (optional, boolean)tillerVersion: The version of Tiller that this chart requires. This should be expressed as a SemVer range:\"\u003e2.0.0\"(optional) If you are familiar with the Chart.yaml file format for Helm Classic, you will notice that fields specifying dependencies have been removed. That is because the new Chart format expresses dependencies using the charts/ directory. Other fields will be silently ignored. Charts and Versioning Every chart must have a version number. A version must follow the SemVer 2 standard. Unlike Helm Classic, Kubernetes Helm uses version numbers as release markers. Packages in repositories are identified by name plus version. For example, an nginx chart whose version field is set to version: 1.2.3 will be named: nginx-1.2.3.tgz More complex SemVer 2 names are also supported, such as version: 1.2.3-alpha.1+ef365. But non-SemVer names are explicitly disallowed by the system. NOTE: Whereas Helm Classic and Deployment Manager were both very GitHub oriented when it came to charts, Kubernetes Helm does not rely upon or require GitHub or even Git. Consequently, it does not use Git SHAs for versioning at all. The version field inside of the Chart.yaml is used by many of the Helm tools, including the CLI and the Tiller server. When generating a package, the helm package command will use the version that it finds in the Chart.yaml as a token in the package name. The system assumes that the version number in the chart package name matches the version number in the Chart.yaml. Failure to meet this assumption will cause an error. The appVersion field Note that the appVersion field is not related to the version field. It is a way of specifying the version of the application. For example, the drupal chart may have an appVersion: 8.2.1, indicating that the version of Drupal included in the chart (by default) is 8.2.1. This field is informational, and has no impact on c","date":"2019-10-13","objectID":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:4:1","tags":["k8s","tag2","tag3"],"title":"Helm v2.14.3 官方文档阅读笔记","uri":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"USING HELM TO MANAGE CHARTS The helm tool has several commands for working with charts. It can create a new chart for you: $ helm create mychart Created mychart/ Once you have edited a chart, helm can package it into a chart archive for you: $ helm package mychart Archived mychart-0.1.-.tgz You can also use helm to help you find issues with your chart’s formatting or information: $ helm lint mychart No issues found ","date":"2019-10-13","objectID":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:4:2","tags":["k8s","tag2","tag3"],"title":"Helm v2.14.3 官方文档阅读笔记","uri":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"CHART REPOSITORIES A chart repository is an HTTP server that houses one or more packaged charts. While helm can be used to manage local chart directories, when it comes to sharing charts, the preferred mechanism is a chart repository. Any HTTP server that can serve YAML files and tar files and can answer GET requests can be used as a repository server. Helm comes with built-in package server for developer testing (helm serve). The Helm team has tested other servers, including Google Cloud Storage with website mode enabled, and S3 with website mode enabled. A repository is characterized primarily(主要特征) by the presence(存在) of a special file called index.yaml that has a list of all of the packages supplied by the repository, together with metadata that allows retrieving and verifying those packages. On the client side, repositories are managed with the helm repo commands. However, Helm does not provide tools for uploading charts to remote repository servers. This is because doing so would add substantial requirements(额外的要求) to an implementing server, and thus raise the barrier(障碍) for setting up a repository. ","date":"2019-10-13","objectID":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:4:3","tags":["k8s","tag2","tag3"],"title":"Helm v2.14.3 官方文档阅读笔记","uri":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"CHART STARTER PACKS The helm create command takes an optional –starter option that lets you specify a “starter chart”. Starters are just regular charts, but are located in $HELM_HOME/starters. As a chart developer, you may author charts that are specifically designed to be used as starters. Such charts should be designed with the following considerations in mind: The Chart.yaml will be overwritten by the generator. Users will expect to modify such a chart’s contents, so documentation should indicate how users can do so. All occurrences of \u003cCHARTNAME\u003e in files within the templates directory will be replaced with the specified chart name so that starter charts can be used as templates. Additionally, occurrences of \u003cCHARTNAME\u003e in values.yaml will also be replaced. Currently the only way to add a chart to $HELM_HOME/starters is to manually copy it there. In your chart’s documentation, you may want to explain that process. ","date":"2019-10-13","objectID":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:4:4","tags":["k8s","tag2","tag3"],"title":"Helm v2.14.3 官方文档阅读笔记","uri":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Hooks Helm provides a hook mechanism to allow chart developers to intervene(干预) at certain points in a release’s life cycle. For example, you can use hooks to: Load a ConfigMap or Secret during install before any other charts are loaded. Execute a Job to back up a database before installing a new chart, and then execute a second job after the upgrade in order to restore data. Run a Job before deleting a release to gracefully take a service out of rotation before removing it. Hooks work like regular templates, but they have special annotations that cause Helm to utilize(利用) them differently. In this section, we cover the basic usage pattern for hooks. Hooks are declared as an annotation in the metadata section of a manifest: apiVersion: ... kind: .... metadata: annotations: \"helm.sh/hook\": \"pre-install\" # ... ","date":"2019-10-13","objectID":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:5:0","tags":["k8s","tag2","tag3"],"title":"Helm v2.14.3 官方文档阅读笔记","uri":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"THE AVAILABLE HOOKS The following hooks are defined: pre-install: Executes after templates are rendered, but before any resources are created in Kubernetes. post-install: Executes after all resources are loaded into Kubernetes pre-delete: Executes on a deletion request before any resources are deleted from Kubernetes. post-delete: Executes on a deletion request after all of the release’s resources have been deleted. pre-upgrade: Executes on an upgrade request after templates are rendered, but before any resources are loaded into Kubernetes (e.g. before a Kubernetes apply operation). post-upgrade: Executes on an upgrade after all resources have been upgraded. pre-rollback: Executes on a rollback request after templates are rendered, but before any resources have been rolled back. post-rollback: Executes on a rollback request after all resources have been modified. crd-install: Adds CRD resources before any other checks are run. This is used only on CRD definitions that are used by other manifests in the chart. test-success: Executes when running helm test and expects the pod to return successfully (return code == 0). test-failure: Executes when running helm test and expects the pod to fail (return code != 0). ","date":"2019-10-13","objectID":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:5:1","tags":["k8s","tag2","tag3"],"title":"Helm v2.14.3 官方文档阅读笔记","uri":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"HOOKS AND THE RELEASE LIFECYCLE Hooks allow you, the chart developer, an opportunity to perform operations at strategic points in a release lifecycle. For example, consider the lifecycle for a helm install. By default, the lifecycle looks like this: User runs helm install foo Chart is loaded into Tiller After some verification, Tiller renders(渲染) the foo templates Tiller loads the resulting resources into Kubernetes Tiller returns the release name (and other data) to the client The client exits Helm defines two hooks for the install lifecycle: pre-install and post-install. If the developer of the foo chart implements both hooks, the lifecycle is altered like this: User runs helm install foo Chart is loaded into Tiller After some verification, Tiller renders the foo templates Tiller prepares to execute the pre-install hooks (loading hook resources into Kubernetes) Tiller sorts hooks by weight (assigning a weight of 0 by default) and by name for those hooks with the same weight in ascending order. Tiller then loads the hook with the lowest weight first (negative to positive) Tiller waits until the hook is “Ready” (except for CRDs) Tiller loads the resulting resources into Kubernetes. Note that if the --wait flag is set, Tiller will wait until all resources are in a ready state and will not run the post-install hook until they are ready. Tiller executes the post-install hook (loading hook resources) Tiller waits until the hook is “Ready” Tiller returns the release name (and other data) to the client The client exits What does it mean to wait until a hook is ready? This depends on the resource declared in the hook. If the resources is a Job kind, Tiller will wait until the job successfully runs to completion. And if the job fails, the release will fail. This is a blocking operation, so the Helm client will pause while the Job is run. For all other kinds, as soon as Kubernetes marks the resource as loaded (added or updated), the resource is considered “Ready”. When many resources are declared in a hook, the resources are executed serially(连续地). If they have hook weights (see below), they are executed in weighted order. Otherwise, ordering is not guaranteed. (In Helm 2.3.0 and after, they are sorted alphabetically(按字母顺序). That behavior, though, is not considered binding and could change in the future.) It is considered good practice to add a hook weight, and set it to 0 if weight is not important. Hook resources are not managed with corresponding releases The resources that a hook creates are not tracked or managed as part of the release. Once Tiller verifies that the hook has reached its ready state, it will leave the hook resource alone. Practically speaking, this means that if you create resources in a hook, you cannot rely upon helm delete to remove the resources. To destroy such resources, you need to either write code to perform this operation in a pre-delete or post-delete hook or add \"helm.sh/hook-delete-policy\" annotation to the hook template file. ","date":"2019-10-13","objectID":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:5:2","tags":["k8s","tag2","tag3"],"title":"Helm v2.14.3 官方文档阅读笔记","uri":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"WRITING A HOOK Hooks are just Kubernetes manifest files with special annotations in the metadata section. Because they are template files, you can use all of the normal template features, including reading .Values, .Release, and .Template. For example, this template, stored in templates/post-install-job.yaml, declares a job to be run on post-install: apiVersion: batch/v1 kind: Job metadata: name: \"{{.Release.Name}}\" labels: app.kubernetes.io/managed-by: {{.Release.Service | quote }} app.kubernetes.io/instance: {{.Release.Name | quote }} app.kubernetes.io/version: {{ .Chart.AppVersion }} helm.sh/chart: \"{{.Chart.Name}}-{{.Chart.Version}}\" annotations: # This is what defines this resource as a hook. Without this line, the # job is considered part of the release. \"helm.sh/hook\": post-install \"helm.sh/hook-weight\": \"-5\" \"helm.sh/hook-delete-policy\": hook-succeeded spec: template: metadata: name: \"{{.Release.Name}}\" labels: app.kubernetes.io/managed-by: {{.Release.Service | quote }} app.kubernetes.io/instance: {{.Release.Name | quote }} helm.sh/chart: \"{{.Chart.Name}}-{{.Chart.Version}}\" spec: restartPolicy: Never containers: - name: post-install-job image: \"alpine:3.3\" command: [\"/bin/sleep\",\"{{default \"10\" .Values.sleepyTime}}\"] What makes this template a hook is the annotation: annotations: \"helm.sh/hook\": post-install One resource can implement multiple hooks: annotations: \"helm.sh/hook\": post-install,post-upgrade Similarly, there is no limit to the number of different resources that may implement a given hook. For example, one could declare both a secret and a config map as a pre-install hook. When subcharts declare hooks, those are also evaluated. There is no way for a top-level chart to disable the hooks declared by subcharts. It is possible to define a weight for a hook which will help build a deterministic(确定性的) executing order. Weights are defined using the following annotation: annotations: \"helm.sh/hook-weight\": \"5\" Hook weights can be positive or negative numbers but must be represented as strings. When Tiller starts the execution cycle of hooks of a particular kind (ex. the pre-install hooks or post-install hooks, etc.) it will sort those hooks in ascending(上升的) order. It is also possible to define policies that determine when to delete corresponding hook resources. Hook deletion policies are defined using the following annotation: annotations: \"helm.sh/hook-delete-policy\": hook-succeeded You can choose one or more defined annotation values: \"hook-succeeded\" specifies Tiller should delete the hook after the hook is successfully executed. \"hook-failed\" specifies Tiller should delete the hook if the hook failed during execution. \"before-hook-creation\" specifies Tiller should delete the previous hook before the new hook is launched. By default Tiller will wait for 60 seconds for a deleted hook to no longer exist in the API server before timing out. This behavior can be changed using the helm.sh/hook-delete-timeout annotation. The value is the number of seconds Tiller should wait for the hook to be fully deleted. A value of 0 means Tiller does not wait at all. Defining a CRD with the crd-install Hook Custom Resource Definitions (CRDs) are a special kind in Kubernetes. They provide a way to define other kinds. On occasion, a chart needs to both define a kind and then use it. This is done with the crd-install hook. The crd-install hook is executed very early during an installation, before the rest of the manifests are verified. CRDs can be annotated with this hook so that they are installed before any instances of that CRD are referenced. In this way, when verification happens later, the CRDs will be available. Here is an example of defining a CRD with a hook, and an instance of the CRD: apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: crontabs.stable.example.com annotations: \"helm.sh/hook\": crd-install spec: group: stable.example.com version: v1 scope: Namespaced names: plural: cronta","date":"2019-10-13","objectID":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:5:3","tags":["k8s","tag2","tag3"],"title":"Helm v2.14.3 官方文档阅读笔记","uri":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Chart Development Tips and Tricks This guide covers some of the tips and tricks Helm chart developers have learned while building production-quality charts. KNOW YOUR TEMPLATE FUNCTIONS Helm uses Go templates for templating your resource files. While Go ships several built-in functions, we have added many others. First, we added almost all of the functions in the Sprig library. We removed two for security reasons: env and expandenv (which would have given chart authors access to Tiller’s environment). We also added two special template functions: include and required. The include function allows you to bring in another template, and then pass the results to other template functions. For example, this template snippet(片段) includes a template called mytpl, then lowercases the result, then wraps that in double quotes(双引号). value: {{ include \"mytpl\" . | lower | quote }} The required function allows you to declare a particular values entry as required for template rendering. If the value is empty, the template rendering will fail with a user submitted error message. The following example of the required function declares an entry for .Values.who is required, and will print an error message when that entry is missing: value: {{ required \"A valid .Values.who entry required!\" .Values.who }} When using the include function, you can pass it a custom object tree built from the current context by using the dict function: {{- include \"mytpl\" (dict \"key1\" .Values.originalKey1 \"key2\" .Values.originalKey2) }} QUOTE STRINGS, DON’T QUOTE INTEGERS When you are working with string data, you are always safer quoting the strings than leaving them as bare words: name: {{ .Values.MyName | quote }} But when working with integers do not quote the values. That can, in many cases, cause parsing errors inside of Kubernetes. port: {{ .Values.Port }} This remark does not apply to env variables values which are expected to be string, even if they represent integers: env: -name: HOST value: \"http://host\" -name: PORT value: \"1234\" USING THE ‘INCLUDE’ FUNCTION Go provides a way of including one template in another using a built-in template directive. However, the built-in function cannot be used in Go template pipelines. To make it possible to include a template, and then perform an operation on that template’s output, Helm has a special include function: {{- include \"toYaml\" $value | nindent 2 }} The above includes a template called toYaml, passes it $value, and then passes the output of that template to the nindent function. Using the {{- ... | nindent _n_ }} pattern makes it easier to read the include in context, because it chomps the whitespace to the left (including the previous newline), then the nindent re-adds the newline and indents the included content by the requested amount. Because YAML ascribes significance to indentation levels and whitespace(因为YAML将重要性归因于缩进级别和空白), this is one great way to include snippets of code, but handle indentation in a relevant context. USING THE ‘REQUIRED’ FUNCTION Go provides a way for setting template options to control behavior when a map is indexed with a key that’s not present in the map. This is typically set with template.Options(“missingkey=option”), where option can be default, zero, or error. While setting this option to error will stop execution with an error, this would apply to every missing key in the map. There may be situations where a chart developer wants to enforce this behavior for select values in the values.yml file. The required function gives developers the ability to declare a value entry as required for template rendering. If the entry is empty in values.yml, the template will not render and will return an error message supplied by the developer. For example: {{ required \"A valid foo is required!\" .Values.foo }} The above will render the template when .Values.foo is defined, but will fail to render and exit when .Values.foo is undefined. USING THE ‘TPL’ FUNCTION The tpl function allows developers","date":"2019-10-13","objectID":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:5:4","tags":["k8s","tag2","tag3"],"title":"Helm v2.14.3 官方文档阅读笔记","uri":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"The Chart Repository Guide This section explains how to create and work with Helm chart repositories. At a high level, a chart repository is a location where packaged charts can be stored and shared. The official chart repository is maintained by the Helm Charts, and we welcome participation. But Helm also makes it easy to create and run your own chart repository. This guide explains how to do so. ","date":"2019-10-13","objectID":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:6:0","tags":["k8s","tag2","tag3"],"title":"Helm v2.14.3 官方文档阅读笔记","uri":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"PREREQUISITES Go through the Quickstart Guide Read through the Charts document ","date":"2019-10-13","objectID":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:6:1","tags":["k8s","tag2","tag3"],"title":"Helm v2.14.3 官方文档阅读笔记","uri":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"CREATE A CHART REPOSITORY A chart repository is an HTTP server that houses an index.yaml file and optionally some packaged charts. When you’re ready to share your charts, the preferred way to do so is by uploading them to a chart repository. Note: For Helm 2.0.0, chart repositories do not have any intrinsic(固有) authentication(认证方式). There is an issue tracking progress in GitHub. Because a chart repository can be any HTTP server that can serve YAML and tar files and can answer GET requests, you have a plethora(过多) of options when it comes down to hosting your own chart repository. For example, you can use a Google Cloud Storage (GCS) bucket(桶), Amazon S3 bucket, Github Pages, or even create your own web server. The chart repository structure A chart repository consists of packaged charts and a special file called index.yaml which contains an index of all of the charts in the repository. Frequently(通常地), the charts that index.yaml describes are also hosted on the same server, as are the provenance files(源文件). For example, the layout of the repository https://example.com/charts might look like this: charts/ | |- index.yaml | |- alpine-0.1.2.tgz | |- alpine-0.1.2.tgz.prov In this case, the index file would contain information about one chart, the Alpine chart, and provide the download URL https://example.com/charts/alpine-0.1.2.tgz for that chart. It is not required that a chart package be located on the same server as the index.yaml file. However, doing so is often the easiest. The index file The index file is a yaml file called index.yaml. It contains some metadata about the package, including the contents of a chart’s Chart.yaml file. A valid chart repository must have an index file. The index file contains information about each chart in the chart repository. The helm repo index command will generate an index file based on a given local directory that contains packaged charts. This is an example of an index file: apiVersion: v1 entries: alpine: - created: 2016-10-06T16:23:20.499814565-06:00 description: Deploy a basic Alpine Linux pod digest: 99c76e403d752c84ead610644d4b1c2f2b453a74b921f422b9dcb8a7c8b559cd home: https://k8s.io/helm name: alpine sources: - https://github.com/helm/helm urls: - https://technosophos.github.io/tscharts/alpine-0.2.0.tgz version: 0.2.0 - created: 2016-10-06T16:23:20.499543808-06:00 description: Deploy a basic Alpine Linux pod digest: 515c58e5f79d8b2913a10cb400ebb6fa9c77fe813287afbacf1a0b897cd78727 home: https://k8s.io/helm name: alpine sources: - https://github.com/helm/helm urls: - https://technosophos.github.io/tscharts/alpine-0.1.0.tgz version: 0.1.0 nginx: - created: 2016-10-06T16:23:20.499543808-06:00 description: Create a basic nginx HTTP server digest: aaff4545f79d8b2913a10cb400ebb6fa9c77fe813287afbacf1a0b897cdffffff home: https://k8s.io/helm name: nginx sources: - https://github.com/helm/charts urls: - https://technosophos.github.io/tscharts/nginx-1.1.0.tgz version: 1.1.0 generated: 2016-10-06T16:23:20.499029981-06:00 A generated index and packages can be served from a basic webserver. You can test things out locally with the helm serve command, which starts a local server. $ helm serve --repo-path ./charts Regenerating index. This may take a moment. Now serving you on 127.0.0.1:8879 The above starts a local webserver, serving the charts it finds in ./charts. The serve command will automatically generate an index.yaml file for you during startup. ","date":"2019-10-13","objectID":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:6:2","tags":["k8s","tag2","tag3"],"title":"Helm v2.14.3 官方文档阅读笔记","uri":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"HOSTING CHART REPOSITORIES This part shows several ways to serve a chart repository. ChartMuseum The Helm project provides an open-source Helm repository server called ChartMuseum that you can host yourself. ChartMuseum supports multiple cloud storage backends. Configure it to point to the directory or bucket containing your chart packages, and the index.yaml file will be generated dynamically. It can be deployed easily as a Helm chart: helm install stable/chartmuseum and also as a Docker image: docker run --rm -it \\ -p 8080:8080 \\ -v $(pwd)/charts:/charts \\ -e DEBUG=true \\ -e STORAGE=local \\ -e STORAGE_LOCAL_ROOTDIR=/charts \\ chartmuseum/chartmuseum You can then add the repo to your local repository list: helm repo add chartmuseum http://localhost:8080 ChartMuseum provides other features, such as an API for chart uploads. Please see the README for more info. Github Pages example In a similar way you can create charts repository using GitHub Pages. GitHub allows you to serve static web pages in two different ways: By configuring a project to serve the contents of its docs/ directory By configuring a project to serve a particular branch We’ll take the second approach, though the first is just as easy. The first step will be to create your gh-pages branch. You can do that locally as. git checkout -b gh-pages Or via web browser using Branch button on your Github repository: Next, you’ll want to make sure your gh-pages branch is set as Github Pages, click on your repo Settings and scroll down to Github pages section and set as per below: By default Source usually gets set to gh-pages branch. If this is not set by default, then select it. You can use a custom domain there if you wish so. And check that Enforce HTTPS is ticked, so the HTTPS will be used when charts are served. In such setup you can use master branch to store your charts code, and gh-pages branch as charts repository, e.g.: https://USERNAME.github.io/REPONAME. The demonstration(示范) TS Charts repository is accessible at https://technosophos.github.io/tscharts/. Ordinary(普通的) web servers To configure an ordinary web server to serve Helm charts, you merely need to do the following: Put your index and charts in a directory that the server can serve Make sure the index.yaml file can be accessed with no authentication requirement Make sure yaml files are served with the correct content type (text/yaml or text/x-yaml) For example, if you want to serve your charts out of $WEBROOT/charts, make sure there is a charts/ directory in your web root, and put the index file and charts inside of that folder. ","date":"2019-10-13","objectID":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:6:3","tags":["k8s","tag2","tag3"],"title":"Helm v2.14.3 官方文档阅读笔记","uri":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"MANAGING CHART REPOSITORIES Now that you have a chart repository, the last part of this guide explains how to maintain charts in that repository. Store charts in your chart repository Now that you have a chart repository, let’s upload a chart and an index file to the repository. Charts in a chart repository must be packaged (helm package chart-name/) and versioned correctly (following SemVer 2 guidelines). These next steps compose an example workflow, but you are welcome to use whatever workflow you fancy for storing and updating charts in your chart repository. Once you have a packaged chart ready, create a new directory, and move your packaged chart to that directory. helm package docs/examples/alpine/ mkdir fantastic-charts mv alpine-0.1.0.tgz fantastic-charts/ helm repo index fantastic-charts --url https://fantastic-charts.storage.googleapis.com The last command takes the path of the local directory that you just created and the URL of your remote chart repository and composes an index.yaml file inside the given directory path. Now you can upload the chart and the index file to your chart repository using a sync tool or manually. If you’re using Google Cloud Storage, check out this example workflow using the gsutil client. For GitHub, you can simply put the charts in the appropriate destination branch. Add new charts to an existing repository Each time you want to add a new chart to your repository, you must regenerate the index. The helm repo index command will completely rebuild the index.yaml file from scratch, including only the charts that it finds locally. However, you can use the --merge flag to incrementally add new charts to an existing index.yaml file (a great option when working with a remote repository like GCS). Run helm repo index --help to learn more, Make sure that you upload both the revised index.yaml file and the chart. And if you generated a provenance file, upload that too. Share your charts with others When you’re ready to share your charts, simply let someone know what the URL of your repository is. From there, they will add the repository to their helm client via the helm repo add [NAME] [URL] command with any name they would like to use to reference the repository. helm repo add fantastic-charts https://fantastic-charts.storage.googleapis.com $ helm repo list fantastic-charts https://fantastic-charts.storage.googleapis.com If the charts are backed by HTTP basic authentication, you can also supply the username and password here: helm repo add fantastic-charts https://fantastic-charts.storage.googleapis.com --username my-username --password my-password $ helm repo list fantastic-charts https://fantastic-charts.storage.googleapis.com Note: A repository will not be added if it does not contain a valid index.yaml. After that, your users will be able to search through your charts. After you’ve updated the repository, they can use the helm repo update command to get the latest chart information. Under the hood, the helm repo add and helm repo update commands are fetching the index.yaml file and storing them in the $HELM_HOME/repository/cache/ directory. This is where the helm search function finds information about charts. ","date":"2019-10-13","objectID":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:6:4","tags":["k8s","tag2","tag3"],"title":"Helm v2.14.3 官方文档阅读笔记","uri":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Chart Tests A chart contains a number of Kubernetes resources and components that work together. As a chart author, you may want to write some tests that validate that your chart works as expected when it is installed. These tests also help the chart consumer understand what your chart is supposed to do. A test in a helm chart lives under the templates/ directory and is a pod definition that specifies a container with a given command to run. The container should exit successfully (exit 0) for a test to be considered a success. The pod definition must contain one of the helm test hook annotations: helm.sh/hook: test-success or helm.sh/hook: test-failure. Example tests: - Validate that your configuration from the values.yaml file was properly injected. - Make sure your username and password work correctly - Make sure an incorrect username and password does not work - Assert that your services are up and correctly loadbalanced. - etc. You can run the pre-defined tests in Helm on a release using the command helm test \u003cRELEASE_NAME\u003e. For a chart consumer, this is a great way to sanity check that their release of a chart (or application) works as expected. A BREAKDOWN OF THE HELM TEST HOOKS In Helm, there are two test hooks: test-success and test-failure test-success indicates that test pod should complete successfully. In other words, the containers in the pod should exit 0. test-failure is a way to assert that a test pod should not complete successfully. If the containers in the pod do not exit 0, that indicates success. EXAMPLE TEST Here is an example of a helm test pod definition in an example wordpress chart. The test verifies the access and login to the mariadb database: wordpress/ Chart.yaml README.md values.yaml charts/ templates/ templates/tests/test-mariadb-connection.yaml In wordpress/templates/tests/test-mariadb-connection.yaml: apiVersion: v1 kind: Pod metadata: name: \"{{ .Release.Name }}-credentials-test\" annotations: \"helm.sh/hook\": test-success spec: containers: - name: {{ .Release.Name }}-credentials-test image: {{ .Values.image }} env: - name: MARIADB_HOST value: {{ template \"mariadb.fullname\" . }} - name: MARIADB_PORT value: \"3306\" - name: WORDPRESS_DATABASE_NAME value: {{ default \"\" .Values.mariadb.mariadbDatabase | quote }} - name: WORDPRESS_DATABASE_USER value: {{ default \"\" .Values.mariadb.mariadbUser | quote }} - name: WORDPRESS_DATABASE_PASSWORD valueFrom: secretKeyRef: name: {{ template \"mariadb.fullname\" . }} key: mariadb-password command: [\"sh\", \"-c\", \"mysql --host=$MARIADB_HOST--port=$MARIADB_PORT--user=$WORDPRESS_DATABASE_USER--password=$WORDPRESS_DATABASE_PASSWORD\"] restartPolicy: Never STEPS TO RUN A TEST SUITE ON A RELEASE $ helm install stable/wordpress NAME: quirky-walrus LAST DEPLOYED: Mon Feb 13 13:50:43 2017 NAMESPACE: default STATUS: DEPLOYED $ helm test quirky-walrus RUNNING: quirky-walrus-credentials-test SUCCESS: quirky-walrus-credentials-test NOTES: You can define as many tests as you would like in a single yaml file or spread across several yaml files in the templates/ directory You are welcome to nest your test suite under a tests/ directory like \u003cchart-name\u003e/templates/tests/ for more isolation ","date":"2019-10-13","objectID":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:6:5","tags":["k8s","tag2","tag3"],"title":"Helm v2.14.3 官方文档阅读笔记","uri":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"The Chart Template Developer’s Guide This guide provides an introduction to Helm’s chart templates, with emphasis(重点) on the template language. Templates generate manifest files, which are YAML-formatted resource descriptions that Kubernetes can understand. We’ll look at how templates are structured, how they can be used, how to write Go templates, and how to debug your work. This guide focuses on the following concepts: The Helm template language Using values Techniques for working with templates This guide is oriented toward learning the ins and outs of the Helm template language. Other guides provide introductory material(介绍性质的材料)), examples(示例), and best practices(最佳实践). ","date":"2019-10-13","objectID":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:7:0","tags":["k8s","tag2","tag3"],"title":"Helm v2.14.3 官方文档阅读笔记","uri":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Getting Started with a Chart Template In this section of the guide, we’ll create a chart and then add a first template. The chart we created here will be used throughout the rest of the guide. To get going, let’s take a brief look at a Helm chart. CHARTS As described in the Charts Guide, Helm charts are structured like this: mychart/ Chart.yaml values.yaml charts/ templates/ ... The templates/ directory is for template files. When Tiller evaluates a chart, it will send all of the files in the templates/ directory through the template rendering engine. Tiller then collects the results of those templates and sends them on to Kubernetes. The values.yaml file is also important to templates. This file contains the default values for a chart. These values may be overridden by users during helm install or helm upgrade. The Chart.yaml file contains a description of the chart. You can access it from within a template. The charts/ directory may contain other charts (which we call subcharts). Later in this guide we will see how those work when it comes to template rendering. A STARTER CHART For this guide, we’ll create a simple chart called mychart, and then we’ll create some templates inside of the chart. $ helm create mychart Creating mychart From here on, we’ll be working in the mychart directory. A Quick Glimpse of mychart/templates/ If you take a look at the mychart/templates/ directory, you’ll notice a few files already there. NOTES.txt: The “help text” for your chart. This will be displayed to your users when they run helm install. deployment.yaml: A basic manifest for creating a Kubernetes deployment service.yaml: A basic manifest for creating a service endpoint for your deployment _helpers.tpl: A place to put template helpers that you can re-use throughout the chart And what we’re going to do is… remove them all! That way we can work through our tutorial from scratch. We’ll actually create our own NOTES.txt and _helpers.tpl as we go. rm -rf mychart/templates/*.* When you’re writing production grade charts, having basic versions of these charts can be really useful. So in your day-to-day chart authoring, you probably won’t want to remove them. A FIRST TEMPLATE The first template we are going to create will be a ConfigMap. In Kubernetes, a ConfigMap is simply a container for storing configuration data. Other things, like pods, can access the data in a ConfigMap. Because ConfigMaps are basic resources, they make a great starting point for us. Let’s begin by creating a file called mychart/templates/configmap.yaml: apiVersion: v1 kind: ConfigMap metadata: name: mychart-configmap data: myvalue: \"Hello World\" TIP: Template names do not follow a rigid(刚性，生硬，严格) naming pattern. However, we recommend using the suffix .yaml for YAML files and .tpl for helpers. The YAML file above is a bare-bones ConfigMap, having the minimal necessary fields. In virtue of the fact that this file is in the templates/ directory, it will be sent through the template engine. It is just fine to put a plain YAML file like this in the templates/ directory. When Tiller reads this template, it will simply send it to Kubernetes as-is. With this simple template, we now have an installable chart. And we can install it like this: $ helm install ./mychart NAME: full-coral LAST DEPLOYED: Tue Nov 1 17:36:01 2016 NAMESPACE: default STATUS: DEPLOYED RESOURCES: ==\u003e v1/ConfigMap NAME DATA AGE mychart-configmap 1 1m In the output above, we can see that our ConfigMap was created. Using Helm, we can retrieve the release and see the actual template that was loaded. $ helm get manifest full-coral --- # Source: mychart/templates/configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: mychart-configmap data: myvalue: \"Hello World\" The helm get manifest command takes a release name (full-coral) and prints out all of the Kubernetes resources that were uploaded to the server. Each file begins with --- to indicate the start of a YAML document, and then is followed by an automatica","date":"2019-10-13","objectID":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:7:1","tags":["k8s","tag2","tag3"],"title":"Helm v2.14.3 官方文档阅读笔记","uri":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Built-in Objects Objects are passed into a template from the template engine. And your code can pass objects around (we’ll see examples when we look at the with and range statements). There are even a few ways to create new objects within your templates, like with the list function we’ll see later. Objects can be simple, and have just one value. Or they can contain other objects or functions. For example. the Release object contains several objects (like Release.Name) and the Files object has a few functions. In the previous section, we use {{.Release.Name}} to insert the name of a release into a template. Release is one of the top-level objects that you can access in your templates. Release: This object describes the release itself. It has several objects inside of it: Release.Name: The release name Release.Time: The time of the release Release.Namespace: The namespace to be released into (if the manifest doesn’t override) Release.Service: The name of the releasing service (always Tiller). Release.Revision: The revision number of this release. It begins at 1 and is incremented for each helm upgrade. Release.IsUpgrade: This is set to true if the current operation is an upgrade or rollback. Release.IsInstall: This is set to true if the current operation is an install. Values: Values passed into the template from the values.yaml file and from user-supplied files. By default, Values is empty. Chart: The contents of the Chart.yaml file. Any data in Chart.yaml will be accessible here. For example {{.Chart.Name}}-{{.Chart.Version}} will print out the mychart-0.1.0. The available fields are listed in the Charts Guide Files: This provides access to all non-special files in a chart. While you cannot use it to access templates, you can use it to access other files in the chart. See the section Accessing Files for more. Files.Get is a function for getting a file by name (.Files.Get config.ini) Files.GetBytes is a function for getting the contents of a file as an array of bytes instead of as a string. This is useful for things like images. Capabilities(能力): This provides information about what capabilities the Kubernetes cluster supports. Capabilities.APIVersions is a set of versions. Capabilities.APIVersions.Has $version indicates whether a version (e.g., batch/v1) or resource (e.g., apps/v1/Deployment) is available on the cluster. Note, resources were not available before Helm v2.15. Capabilities.KubeVersion provides a way to look up the Kubernetes version. It has the following values: Major, Minor, GitVersion, GitCommit, GitTreeState, BuildDate, GoVersion, Compiler, and Platform. Capabilities.TillerVersion provides a way to look up the Tiller version. It has the following values: SemVer, GitCommit, and GitTreeState. Template: Contains information about the current template that is being executed Name: A namespaced filepath to the current template (e.g. mychart/templates/mytemplate.yaml) BasePath: The namespaced path to the templates directory of the current chart (e.g. mychart/templates). The values are available to any top-level template. As we will see later, this does not necessarily mean that they will be available everywhere. The built-in values always begin with a capital letter. This is in keeping with Go’s naming convention. When you create your own names, you are free to use a convention that suits your team. Some teams, like the Helm Charts team, choose to use only initial lower case letters in order to distinguish local names from those built-in. In this guide, we follow that convention. ","date":"2019-10-13","objectID":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:7:2","tags":["k8s","tag2","tag3"],"title":"Helm v2.14.3 官方文档阅读笔记","uri":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Values Files In the previous section we looked at the built-in objects that Helm templates offer. One of these built-in objects is Values. This object provides access to values passed into the chart. Its contents come from four sources: The values.yaml file in the chart If this is a subchart, the values.yaml file of a parent chart A values file is passed into helm install or helm upgrade with the -f flag (helm install -f myvals.yaml ./mychart) Individual parameters passed with --set (such as helm install --set foo=bar ./mychart) The list above is in order of specificity: values.yaml is the default, which can be overridden by a parent chart’s values.yaml, which can in turn be overridden by a user-supplied values file, which can in turn be overridden by --set parameters. Values files are plain YAML files. Let’s edit mychart/values.yaml and then edit our ConfigMap template. Removing the defaults in values.yaml, we’ll set just one parameter: favoriteDrink: coffee Now we can use this inside of a template: apiVersion: v1 kind: ConfigMap metadata: name: {{ .Release.Name }}-configmap data: myvalue: \"Hello World\" drink: {{ .Values.favoriteDrink }} Notice on the last line we access favoriteDrink as an attribute of Values: {{ .Values.favoriteDrink }}. Let’s see how this renders. $ helm install --dry-run --debug ./mychart SERVER: \"localhost:44134\" CHART PATH: /Users/mattbutcher/Code/Go/src/k8s.io/helm/_scratch/mychart NAME: geared-marsupi TARGET NAMESPACE: default CHART: mychart 0.1.0 MANIFEST: --- # Source: mychart/templates/configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: geared-marsupi-configmap data: myvalue: \"Hello World\" drink: coffee Because favoriteDrink is set in the default values.yaml file to coffee, that’s the value displayed in the template. We can easily override that by adding a --set flag in our call to helm install: helm install --dry-run --debug --set favoriteDrink=slurm ./mychart SERVER: \"localhost:44134\" CHART PATH: /Users/mattbutcher/Code/Go/src/k8s.io/helm/_scratch/mychart NAME: solid-vulture TARGET NAMESPACE: default CHART: mychart 0.1.0 MANIFEST: --- # Source: mychart/templates/configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: solid-vulture-configmap data: myvalue: \"Hello World\" drink: slurm Since --set has a higher precedence than the default values.yaml file, our template generates drink: slurm. Values files can contain more structured content, too. For example, we could create a favorite section in our values.yaml file, and then add several keys there: favorite: drink: coffee food: pizza Now we would have to modify the template slightly: apiVersion: v1 kind: ConfigMap metadata: name: {{ .Release.Name }}-configmap data: myvalue: \"Hello World\" drink: {{ .Values.favorite.drink }} food: {{ .Values.favorite.food }} While structuring data this way is possible, the recommendation is that you keep your values trees shallow, favoring flatness. When we look at assigning values to subcharts, we’ll see how values are named using a tree structure. DELETING A DEFAULT KEY If you need to delete a key from the default values, you may override the value of the key to be null, in which case Helm will remove the key from the overridden values merge. For example, the stable Drupal chart allows configuring the liveness probe, in case you configure a custom image. Here are the default values: livenessProbe: httpGet: path: /user/login port: http initialDelaySeconds: 120 If you try to override the livenessProbe handler to exec instead of httpGet using --set livenessProbe.exec.command=[cat,docroot/CHANGELOG.txt], Helm will coalesce the default and overridden keys together, resulting in the following YAML: livenessProbe: httpGet: path: /user/login port: http exec: command: - cat - docroot/CHANGELOG.txt initialDelaySeconds: 120 However, Kubernetes would then fail because you can not declare more than one livenessProbe handler. To overcome this, you may instruct Helm to delete the livenessProbe.httpGet by setting it","date":"2019-10-13","objectID":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:7:3","tags":["k8s","tag2","tag3"],"title":"Helm v2.14.3 官方文档阅读笔记","uri":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Template Functions and Pipelines So far, we’ve seen how to place information into a template. But that information is placed into the template unmodified. Sometimes we want to transform the supplied data in a way that makes it more usable to us. Let’s start with a best practice: When injecting strings from the .Values object into the template, we ought to quote these strings. We can do that by calling the quote function in the template directive: apiVersion: v1 kind: ConfigMap metadata: name: {{ .Release.Name }}-configmap data: myvalue: \"Hello World\" drink: {{ quote .Values.favorite.drink }} food: {{ quote .Values.favorite.food }} Template functions follow the syntax functionName arg1 arg2.... In the snippet above, quote .Values.favorite.drink calls the quote function and passes it a single argument. Helm has over 60 available functions. Some of them are defined by the Go template language itself. Most of the others are part of the Sprig template library. We’ll see many of them as we progress through the examples. While we talk about the “Helm template language” as if it is Helm-specific, it is actually a combination of the Go template language, some extra functions, and a variety of wrappers to expose certain objects to the templates. Many resources on Go templates may be helpful as you learn about templating. PIPELINES One of the powerful features of the template language is its concept of pipelines(管道). Drawing on a concept from UNIX, pipelines are a tool for chaining together a series of template commands to compactly express a series of transformations. In other words, pipelines are an efficient way of getting several things done in sequence. Let’s rewrite the above example using a pipeline. apiVersion: v1 kind: ConfigMap metadata: name: {{ .Release.Name }}-configmap data: myvalue: \"Hello World\" drink: {{ .Values.favorite.drink | quote }} food: {{ .Values.favorite.food | quote }} In this example, instead of calling quote ARGUMENT, we inverted the order. We “sent” the argument to the function using a pipeline (|): .Values.favorite.drink | quote. Using pipelines, we can chain several functions together: apiVersion: v1 kind: ConfigMap metadata: name: {{ .Release.Name }}-configmap data: myvalue: \"Hello World\" drink: {{ .Values.favorite.drink | quote }} food: {{ .Values.favorite.food | upper | quote }} Inverting the order is a common practice in templates. You will see .val | quote more often than quote .val. Either practice is fine. When evaluated, that template will produce this: # Source: mychart/templates/configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: trendsetting-p-configmap data: myvalue: \"Hello World\" drink: \"coffee\" food: \"PIZZA\" Note that our original pizza has now been transformed to \"PIZZA\". When pipelining arguments like this, the result of the first evaluation (.Values.favorite.drink) is sent as the last argument to the function. We can modify the drink example above to illustrate with a function that takes two arguments: repeat COUNT STRING: apiVersion: v1 kind: ConfigMap metadata: name: {{ .Release.Name }}-configmap data: myvalue: \"Hello World\" drink: {{ .Values.favorite.drink | repeat 5 | quote }} food: {{ .Values.favorite.food | upper | quote }} The repeat function will echo the given string the given number of times, so we will get this for output: # Source: mychart/templates/configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: melting-porcup-configmap data: myvalue: \"Hello World\" drink: \"coffeecoffeecoffeecoffeecoffee\" food: \"PIZZA\" USING THE DEFAULT FUNCTION One function frequently used in templates is the default function: default DEFAULT_VALUE GIVEN_VALUE. This function allows you to specify a default value inside of the template, in case the value is omitted. Let’s use it to modify the drink example above: drink: {{ .Values.favorite.drink | default \"tea\" | quote }} If we run this as normal, we’ll get our coffee: # Source: mychart/templates/configmap.yaml apiVersion: v1 kind: Confi","date":"2019-10-13","objectID":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:7:4","tags":["k8s","tag2","tag3"],"title":"Helm v2.14.3 官方文档阅读笔记","uri":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Flow Control Control structures (called “actions” in template parlance(用语)) provide you, the template author, with the ability to control the flow of a template’s generation. Helm’s template language provides the following control structures: if/else for creating conditional blocks with to specify a scope range, which provides a “for each”-style loop In addition to these, it provides a few actions for declaring and using named template segments: define declares a new named template inside of your template template imports a named template block declares a special kind of fillable template area In this section, we’ll talk about if, with, and range. The others are covered in the “Named Templates” section later in this guide. IF/ELSE The first control structure we’ll look at is for conditionally including blocks of text in a template. This is the if/else block. The basic structure for a conditional looks like this: {{ if PIPELINE }} # Do something {{ else if OTHER PIPELINE }} # Do something else {{ else }} # Default case {{ end }} Notice that we’re now talking about pipelines instead of values. The reason for this is to make it clear that control structures can execute an entire pipeline, not just evaluate a value. A pipeline is evaluated as false if the value is: a boolean false a numeric zero an empty string a nil (empty or null) an empty collection (map, slice, tuple, dict, array) In any other case, the condition is evaluated to true and the pipeline is executed. Let’s add a simple conditional to our ConfigMap. We’ll add another setting if the drink is set to coffee: apiVersion: v1 kind: ConfigMap metadata: name: {{ .Release.Name }}-configmap data: myvalue: \"Hello World\" drink: {{ .Values.favorite.drink | default \"tea\" | quote }} food: {{ .Values.favorite.food | upper | quote }} {{ if and .Values.favorite.drink (eq .Values.favorite.drink \"coffee\") }}mug: true{{ end }} Note that .Values.favorite.drink must be defined or else it will throw an error when comparing it to “coffee”. Since we commented out drink: coffee in our last example, the output should not include a mug: true flag. But if we add that line back into our values.yaml file, the output should look like this: # Source: mychart/templates/configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: eyewitness-elk-configmap data: myvalue: \"Hello World\" drink: \"coffee\" food: \"PIZZA\" mug: true CONTROLLING WHITESPACE While we’re looking at conditionals, we should take a quick look at the way whitespace is controlled in templates. Let’s take the previous example and format it to be a little easier to read: apiVersion: v1 kind: ConfigMap metadata: name: {{ .Release.Name }}-configmap data: myvalue: \"Hello World\" drink: {{ .Values.favorite.drink | default \"tea\" | quote }} food: {{ .Values.favorite.food | upper | quote }} {{if eq .Values.favorite.drink \"coffee\"}} mug: true {{end}} Initially, this looks good. But if we run it through the template engine, we’ll get an unfortunate result: $ helm install --dry-run --debug ./mychart SERVER: \"localhost:44134\" CHART PATH: /Users/mattbutcher/Code/Go/src/k8s.io/helm/_scratch/mychart Error: YAML parse error on mychart/templates/configmap.yaml: error converting YAML to JSON: yaml: line 9: did not find expected key What happened? We generated incorrect YAML because of the whitespacing above. # Source: mychart/templates/configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: eyewitness-elk-configmap data: myvalue: \"Hello World\" drink: \"coffee\" food: \"PIZZA\" mug: true mug is incorrectly indented. Let’s simply out-dent that one line, and re-run: apiVersion: v1 kind: ConfigMap metadata: name: {{ .Release.Name }}-configmap data: myvalue: \"Hello World\" drink: {{ .Values.favorite.drink | default \"tea\" | quote }} food: {{ .Values.favorite.food | upper | quote }} {{if eq .Values.favorite.drink \"coffee\"}} mug: true {{end}} When we sent that, we’ll get YAML that is valid, but still looks a little funny: # Source: mychart/templates/configmap.ya","date":"2019-10-13","objectID":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:7:5","tags":["k8s","tag2","tag3"],"title":"Helm v2.14.3 官方文档阅读笔记","uri":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Variables With functions, pipelines, objects, and control structures under our belts, we can turn to one of the more basic ideas in many programming languages: variables. In templates, they are less frequently used. But we will see how to use them to simplify code, and to make better use of with and range. In an earlier example, we saw that this code will fail: {{- with .Values.favorite }} drink: {{ .drink | default \"tea\" | quote }} food: {{ .food | upper | quote }} release: {{ .Release.Name }} {{- end }} Release.Name is not inside of the scope that’s restricted in the with block. One way to work around scoping issues is to assign objects to variables that can be accessed without respect to the present scope. In Helm templates, a variable is a named reference to another object. It follows the form $name. Variables are assigned with a special assignment operator: :=. We can rewrite the above to use a variable for Release.Name. apiVersion: v1 kind: ConfigMap metadata: name: {{ .Release.Name }}-configmap data: myvalue: \"Hello World\" {{- $relname := .Release.Name -}} {{- with .Values.favorite }} drink: {{ .drink | default \"tea\" | quote }} food: {{ .food | upper | quote }} release: {{ $relname }} {{- end }} Notice that before we start the with block, we assign $relname := .Release.Name. Now inside of the with block, the $relname variable still points to the release name. Running that will produce this: # Source: mychart/templates/configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: viable-badger-configmap data: myvalue: \"Hello World\" drink: \"coffee\" food: \"PIZZA\" release: viable-badger Variables are particularly useful in range loops. They can be used on list-like objects to capture both the index and the value: toppings: |- {{- range $index, $topping := .Values.pizzaToppings }} {{ $index }}: {{ $topping }} {{- end }} Note that range comes first, then the variables, then the assignment operator, then the list. This will assign the integer index (starting from zero) to $index and the value to $topping. Running it will produce: toppings: |- 0: mushrooms 1: cheese 2: peppers 3: onions For data structures that have both a key and a value, we can use range to get both. For example, we can loop through .Values.favorite like this: apiVersion: v1 kind: ConfigMap metadata: name: {{ .Release.Name }}-configmap data: myvalue: \"Hello World\" {{- range $key, $val := .Values.favorite }} {{ $key }}: {{ $val | quote }} {{- end}} Now on the first iteration, $key will be drink and $val will be coffee, and on the second, $key will be food and $val will be pizza. Running the above will generate this: # Source: mychart/templates/configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: eager-rabbit-configmap data: myvalue: \"Hello World\" drink: \"coffee\" food: \"pizza\" Variables are normally not “global”. They are scoped to the block in which they are declared. Earlier, we assigned $relname in the top level of the template. That variable will be in scope for the entire template. But in our last example, $key and $val will only be in scope inside of the {{range...}}{{end}} block. However, there is one variable that is always global - $ - this variable will always point to the root context. This can be very useful when you are looping in a range and need to know the chart’s release name. An example illustrating this: {{- range .Values.tlsSecrets }} apiVersion: v1 kind: Secret metadata: name: {{ .name }} labels: # Many helm templates would use `.` below, but that will not work, # however `$` will work here app.kubernetes.io/name: {{ template \"fullname\" $ }} # I cannot reference .Chart.Name, but I can do $.Chart.Name helm.sh/chart: \"{{ $.Chart.Name }}-{{ $.Chart.Version }}\" app.kubernetes.io/instance: \"{{ $.Release.Name }}\" # Value from appVersion in Chart.yaml app.kubernetes.io/version: \"{{ $.Chart.AppVersion }}\" app.kubernetes.io/managed-by: \"{{ $.Release.Service }}\" type: kubernetes.io/tls data: tls.crt: {{ .certificate }} tls.key: {{ .key }} --","date":"2019-10-13","objectID":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:7:6","tags":["k8s","tag2","tag3"],"title":"Helm v2.14.3 官方文档阅读笔记","uri":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Named Templates It is time to move beyond one template, and begin to create others. In this section, we will see how to define named templates in one file, and then use them elsewhere. A named template (sometimes called a partial or a subtemplate) is simply a template defined inside of a file, and given a name. We’ll see two ways to create them, and a few different ways to use them. In the “Flow Control” section we introduced three actions for declaring and managing templates: define, template, and block. In this section, we’ll cover those three actions, and also introduce a special-purpose include function that works similarly to the template action. An important detail to keep in mind when naming templates: template names are global. If you declare two templates with the same name, whichever one is loaded last will be the one used. Because templates in subcharts are compiled together with top-level templates, you should be careful to name your templates with chart-specific names. One popular naming convention is to prefix each defined template with the name of the chart: {{ define \"mychart.labels\" }}. By using the specific chart name as a prefix we can avoid any conflicts that may arise due to two different charts that implement templates of the same name. PARTIALS AND _ FILES So far, we’ve used one file, and that one file has contained a single template. But Helm’s template language allows you to create named embedded templates, that can be accessed by name elsewhere. Before we get to the nuts-and-bolts(螺母和螺栓) of writing those templates, there is file naming convention that deserves mention: Most files in templates/ are treated as if they contain Kubernetes manifests The NOTES.txt is one exception But files whose name begins with an underscore (_) are assumed to not have a manifest inside. These files are not rendered to Kubernetes object definitions, but are available everywhere within other chart templates for use. These files are used to store partials and helpers. In fact, when we first created mychart, we saw a file called _helpers.tpl. That file is the default location for template partials. DECLARING AND USING TEMPLATES WITH DEFINE AND TEMPLATE The define action allows us to create a named template inside of a template file. Its syntax goes like this: {{ define \"MY.NAME\" }} # body of template here {{ end }} For example, we can define a template to encapsulate(封装) a Kubernetes block of labels: {{- define \"mychart.labels\" }} labels: generator: helm date: {{ now | htmlDate }} {{- end }} Now we can embed this template inside of our existing ConfigMap, and then include it with the template action: {{- define \"mychart.labels\" }} labels: generator: helm date: {{ now | htmlDate }} {{- end }} apiVersion: v1 kind: ConfigMap metadata: name: {{ .Release.Name }}-configmap {{- template \"mychart.labels\" }} data: myvalue: \"Hello World\" {{- range $key, $val := .Values.favorite }} {{ $key }}: {{ $val | quote }} {{- end }} When the template engine reads this file, it will store away the reference to mychart.labels until template \"mychart.labels\" is called. Then it will render that template inline. So the result will look like this: # Source: mychart/templates/configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: running-panda-configmap labels: generator: helm date: 2016-11-02 data: myvalue: \"Hello World\" drink: \"coffee\" food: \"pizza\" Conventionally(按照惯例), Helm charts put these templates inside of a partials file, usually _helpers.tpl. Let’s move this function there: {{/* Generate basic labels */}} {{- define \"mychart.labels\" }} labels: generator: helm date: {{ now | htmlDate }} {{- end }} By convention, define functions should have a simple documentation block ({{/* ... */}}) describing what they do. Even though this definition is in _helpers.tpl, it can still be accessed in configmap.yaml: apiVersion: v1 kind: ConfigMap metadata: name: {{ .Release.Name }}-configmap {{- template \"mychart.labels\" }} data: myvalue: \"Hello World\" ","date":"2019-10-13","objectID":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:7:7","tags":["k8s","tag2","tag3"],"title":"Helm v2.14.3 官方文档阅读笔记","uri":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Accessing Files Inside Templates In the previous section we looked at several ways to create and access named templates. This makes it easy to import one template from within another template. But sometimes it is desirable to import a file that is not a template and inject its contents without sending the contents through the template renderer. Helm provides access to files through the .Files object. Before we get going with the template examples, though, there are a few things to note about how this works: It is okay to add extra files to your Helm chart. These files will be bundled and sent to Tiller. Be careful, though. Charts must be smaller than 1M because of the storage limitations of Kubernetes objects. Some files cannot be accessed through the .Files object, usually for security reasons. Files in templates/ cannot be accessed. Files excluded using .helmignore cannot be accessed. Charts do not preserve(保留) UNIX mode information, so file-level permissions will have no impact on the availability of a file when it comes to the .Files object. Basic example Path helpers Glob patterns ConfigMap and Secrets utility functions Encoding Lines BASIC EXAMPLE With those caveats(注意事项) behind, let’s write a template that reads three files into our ConfigMap. To get started, we will add three files to the chart, putting all three directly inside of the mychart/ directory. config1.toml: message = \"Hello from config 1\" config2.toml: message = \"This is config 2\" config3.toml: message = \"Goodbye from config 3\" Each of these is a simple TOML file (think old-school Windows INI files). We know the names of these files, so we can use a range function to loop through them and inject their contents into our ConfigMap. apiVersion: v1 kind: ConfigMap metadata: name: {{ .Release.Name }}-configmap data: {{- $files := .Files }} {{- range list \"config1.toml\" \"config2.toml\" \"config3.toml\" }} {{ . }}: |- {{ $files.Get . }} {{- end }} This config map uses several of the techniques discussed in previous sections. For example, we create a $files variable to hold a reference to the .Files object. We also use the list function to create a list of files that we loop through. Then we print each file name ({{.}}: |-) followed by the contents of the file {{ $files.Get . }}. Running this template will produce a single ConfigMap with the contents of all three files: # Source: mychart/templates/configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: quieting-giraf-configmap data: config1.toml: |- message = \"Hello from config 1\" config2.toml: |- message = \"This is config 2\" config3.toml: |- message = \"Goodbye from config 3\" PATH HELPERS When working with files, it can be very useful to perform some standard operations on the file paths themselves. To help with this, Helm imports many of the functions from Go’s path package for your use. They are all accessible with the same names as in the Go package, but with a lowercase first letter. For example, Base becomes base, etc. The imported functions are: Base Dir Ext IsAbs Clean GLOB PATTERNS As your chart grows, you may find you have a greater need to organize your files more, and so we provide a Files.Glob(pattern string) method to assist in extracting certain files with all the flexibility of glob patterns. .Glob returns a Files type, so you may call any of the Files methods on the returned object. For example, imagine the directory structure: foo/: foo.txt foo.yaml bar/: bar.go bar.conf baz.yaml You have multiple options with Globs: {{ $root := . }} {{ range $path, $bytes := .Files.Glob \"**.yaml\" }} {{ $path }}: |- {{ $root.Files.Get $path }} {{ end }} Or {{ $root := . }} {{ range $path, $bytes := .Files.Glob \"foo/*\" }} {{ base $path }}: '{{ $root.Files.Get $path | b64enc }}' {{ end }} CONFIGMAP AND SECRETS UTILITY FUNCTIONS (Not present in version 2.0.2 or prior(之前的)) It is very common to want to place file content into both configmaps and secrets, for mounting into your pods at run time. To help with this, we","date":"2019-10-13","objectID":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:7:8","tags":["k8s","tag2","tag3"],"title":"Helm v2.14.3 官方文档阅读笔记","uri":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Creating a NOTES.txt File In this section we are going to look at Helm’s tool for providing instructions to your chart users. At the end of a helm install or helm upgrade, Helm can print out a block of helpful information for users. This information is highly customizable using templates. To add installation notes to your chart, simply create a templates/NOTES.txt file. This file is plain text, but it is processed like as a template, and has all the normal template functions and objects available. Let’s create a simple NOTES.txt file: Thank you for installing {{ .Chart.Name }}. Your release is named {{ .Release.Name }}. To learn more about the release, try: $ helm status {{ .Release.Name }} $ helm get {{ .Release.Name }} Now if we run helm install ./mychart we will see this message at the bottom: RESOURCES: ==\u003e v1/Secret NAME TYPE DATA AGE rude-cardinal-secret Opaque 1 0s ==\u003e v1/ConfigMap NAME DATA AGE rude-cardinal-configmap 3 0s NOTES: Thank you for installing mychart. Your release is named rude-cardinal. To learn more about the release, try: $ helm status rude-cardinal $ helm get rude-cardinal Using NOTES.txt this way is a great way to give your users detailed information about how to use their newly installed chart. Creating a NOTES.txt file is strongly recommended, though it is not required. ","date":"2019-10-13","objectID":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:7:9","tags":["k8s","tag2","tag3"],"title":"Helm v2.14.3 官方文档阅读笔记","uri":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Subcharts and Global Values To this point we have been working only with one chart. But charts can have dependencies, called subcharts, that also have their own values and templates. In this section we will create a subchart and see the different ways we can access values from within templates. Before we dive into the code, there are a few important details to learn about subcharts. A subchart is considered “stand-alone”, which means a subchart can never explicitly(明确的) depend on its parent chart. For that reason, a subchart cannot access the values of its parent. A parent chart can override values for subcharts. Helm has a concept of global values that can be accessed by all charts. As we walk through the examples in this section, many of these concepts will become clearer. CREATING A SUBCHART For these exercises, we’ll start with the mychart/ chart we created at the beginning of this guide, and we’ll add a new chart inside of it. $ cd mychart/charts $ helm create mysubchart Creating mysubchart $ rm -rf mysubchart/templates/*.* Notice that just as before, we deleted all of the base templates so that we can start from scratch. In this guide, we are focused on how templates work, not on managing dependencies. But the Charts Guide has more information on how subcharts work. ADDING VALUES AND A TEMPLATE TO THE SUBCHART Next, let’s create a simple template and values file for our mysubchart chart. There should already be a values.yaml in mychart/charts/mysubchart. We’ll set it up like this: dessert: cake Next, we’ll create a new ConfigMap template in mychart/charts/mysubchart/templates/configmap.yaml: apiVersion: v1 kind: ConfigMap metadata: name: {{ .Release.Name }}-cfgmap2 data: dessert: {{ .Values.dessert }} Because every subchart is a stand-alone chart, we can test mysubchart on its own: $ helm install --dry-run --debug mychart/charts/mysubchart SERVER: \"localhost:44134\" CHART PATH: /Users/mattbutcher/Code/Go/src/k8s.io/helm/_scratch/mychart/charts/mysubchart NAME: newbie-elk TARGET NAMESPACE: default CHART: mysubchart 0.1.0 MANIFEST: --- # Source: mysubchart/templates/configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: newbie-elk-cfgmap2 data: dessert: cake OVERRIDING VALUES OF A CHILD CHART Our original chart, mychart is now the parent chart of mysubchart. This relationship is based entirely on the fact that mysubchart is within mychart/charts. Because mychart is a parent, we can specify configuration in mychart and have that configuration pushed into mysubchart. For example, we can modify mychart/values.yaml like this: favorite: drink: coffee food: pizza pizzaToppings: - mushrooms - cheese - peppers - onions mysubchart: dessert: ice cream Note the last two lines. Any directives inside of the mysubchart section will be sent to the mysubchart chart. So if we run helm install --dry-run --debug mychart, one of the things we will see is the mysubchart ConfigMap: # Source: mychart/charts/mysubchart/templates/configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: unhinged-bee-cfgmap2 data: dessert: ice cream The value at the top level has now overridden the value of the subchart. There’s an important detail to notice here. We didn’t change the template of mychart/charts/mysubchart/templates/configmap.yaml to point to .Values.mysubchart.dessert. From that template’s perspective, the value is still located at .Values.dessert. As the template engine passes values along, it sets the scope. So for the mysubchart templates, only values specifically for mysubchart will be available in .Values. Sometimes, though, you do want certain values to be available to all of the templates. This is accomplished using global chart values. GLOBAL CHART VALUES Global values are values that can be accessed from any chart or subchart by exactly the same name. Globals require explicit declaration. You can’t use an existing non-global as if it were a global. The Values data type has a reserved section called Values.global where global valu","date":"2019-10-13","objectID":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:7:10","tags":["k8s","tag2","tag3"],"title":"Helm v2.14.3 官方文档阅读笔记","uri":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Debugging Templates Debugging templates can be tricky(棘手的) simply because the templates are rendered on the Tiller server, not the Helm client. And then the rendered templates are sent to the Kubernetes API server, which may reject the YAML files for reasons other than formatting. There are a few commands that can help you debug. helm lint is your go-to tool for verifying that your chart follows best practices helm install --dry-run --debug: We’ve seen this trick already. It’s a great way to have the server render your templates, then return the resulting manifest file. helm get manifest: This is a good way to see what templates are installed on the server. When your YAML is failing to parse, but you want to see what is generated, one easy way to retrieve the YAML is to comment out the problem section in the template, and then re-run helm install --dry-run --debug: apiVersion: v1 # some: problem section # {{ .Values.foo | quote }} The above will be rendered and returned with the comments intact: apiVersion: v1 # some: problem section # \"bar\" This provides a quick way of viewing the generated content without YAML parse errors blocking. ","date":"2019-10-13","objectID":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:7:11","tags":["k8s","tag2","tag3"],"title":"Helm v2.14.3 官方文档阅读笔记","uri":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Wrapping Up This guide is intended to give you, the chart developer, a strong understanding of how to use Helm’s template language. The guide focuses on the technical aspects of template development. But there are many things this guide has not covered when it comes to the practical day-to-day development of charts. Here are some useful pointers to other documentation that will help you as you create new charts: The Helm Charts project is an indispensable(必不可少的) source of charts. That project is also sets the standard for best practices in chart development. The Kubernetes Documentation provides detailed examples of the various resource kinds that you can use, from ConfigMaps and Secrets to DaemonSets and Deployments. The Helm Charts Guide explains the workflow of using charts. The Helm Chart Hooks Guide explains how to create lifecycle hooks. The Helm Charts Tips and Tricks article provides some useful tips for writing charts. The Sprig documentation documents more than sixty of the template functions. The Go template docs explain the template syntax in detail. The Schelm tool is a nice helper utility for debugging charts. Sometimes it’s easier to ask a few questions and get answers from experienced developers. The best place to do this is in the Kubernetes Slack Helm channels: #helm-users #helm-dev #charts Finally, if you find errors or omissions in this document, want to suggest some new content, or would like to contribute, visit The Helm Project. ","date":"2019-10-13","objectID":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:7:12","tags":["k8s","tag2","tag3"],"title":"Helm v2.14.3 官方文档阅读笔记","uri":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"YAML Techniques Most of this guide has been focused on writing the template language. Here, we’ll look at the YAML format. YAML has some useful features that we, as template authors, can use to make our templates less error prone and easier to read. ","date":"2019-10-13","objectID":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:8:0","tags":["k8s","tag2","tag3"],"title":"Helm v2.14.3 官方文档阅读笔记","uri":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"SCALARS AND COLLECTIONS According to the YAML spec, there are two types of collections, and many scalar types. The two types of collections are maps and sequences: map: one: 1 two: 2 three: 3 sequence: - one - two - three Scalar values(标量值) are individual values (as opposed to collections) ","date":"2019-10-13","objectID":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:8:1","tags":["k8s","tag2","tag3"],"title":"Helm v2.14.3 官方文档阅读笔记","uri":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Scalar Types in YAML In Helm’s dialect(方言) of YAML, the scalar data type of a value is determined by a complex set of rules, including the Kubernetes schema for resource definitions. But when inferring types, the following rules tend to hold true. If an integer or float is an unquoted bare word, it is typically treated as a numeric type: count: 1 size: 2.34 But if they are quoted, they are treated as strings: count: \"1\" # \u003c-- string, not int size: '2.34' # \u003c-- string, not float The same is true of booleans: isGood: true # bool answer: \"true\" # string The word for an empty value is null (not nil). Note that port: \"80\" is valid YAML, and will pass through both the template engine and the YAML parser, but will fail if Kubernetes expects port to be an integer. In some cases, you can force a particular type inference(推理) using YAML node tags: coffee: \"yes, please\" age: !!str 21 port: !!int \"80\" In the above, !!str tells the parser that age is a string, even if it looks like an int. And port is treated as an int, even though it is quoted. ","date":"2019-10-13","objectID":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:8:2","tags":["k8s","tag2","tag3"],"title":"Helm v2.14.3 官方文档阅读笔记","uri":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"STRINGS IN YAML Much of the data that we place in YAML documents are strings. YAML has more than one way to represent a string. This section explains the ways and demonstrates how to use some of them. There are three “inline” ways of declaring a string: way1: bare words way2: \"double-quoted strings\" way3: 'single-quoted strings' All inline styles must be on one line. Bare words are unquoted, and are not escaped. For this reason, you have to be careful what characters you use. Double-quoted strings can have specific characters escaped with \\. For example \"\\\"Hello\\\", she said\". You can escape line breaks with \\n. Single-quoted strings are “literal” strings, and do not use the \\ to escape characters. The only escape sequence is '', which is decoded as a single '. In addition to the one-line strings, you can declare multi-line strings: coffee: | Latte Cappuccino Espresso The above will treat the value of coffee as a single string equivalent to Latte\\nCappuccino\\nEspresso\\n. Note that the first line after the | must be correctly indented. So we could break the example above by doing this: coffee: | Latte Cappuccino Espresso Because Latte is incorrectly indented, we’d get an error like this: Error parsing file: error converting YAML to JSON: yaml: line 7: did not find expected key In templates, it is sometimes safer to put a fake “first line” of content in a multi-line document just for protection from the above error: coffee: | # Commented first line Latte Cappuccino Espresso Note that whatever that first line is, it will be preserved in the output of the string. So if you are, for example, using this technique to inject a file’s contents into a ConfigMap, the comment should be of the type expected by whatever is reading that entry. ","date":"2019-10-13","objectID":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:8:3","tags":["k8s","tag2","tag3"],"title":"Helm v2.14.3 官方文档阅读笔记","uri":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Controlling Spaces in Multi-line Strings In the example above, we used | to indicate a multi-line string. But notice that the content of our string was followed with a trailing \\n. If we want the YAML processor to strip off the trailing newline, we can add a - after the |: coffee: |- Latte Cappuccino Espresso Now the coffee value will be: Latte\\nCappuccino\\nEspresso (with no trailing \\n). Other times, we might want all trailing whitespace to be preserved(保留所有尾随的空白). We can do this with the |+ notation: coffee: |+ Latte Cappuccino Espresso another: value Now the value of coffee will be Latte\\nCappuccino\\nEspresso\\n\\n\\n. Indentation inside of a text block is preserved, and results in the preservation of line breaks, too(保留文本块内部的缩进，并且也导致换行符的保留): coffee: |- Latte 12 oz 16 oz Cappuccino Espresso In the above case, coffee will be Latte\\n 12 oz\\n 16 oz\\nCappuccino\\nEspresso. ","date":"2019-10-13","objectID":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:8:4","tags":["k8s","tag2","tag3"],"title":"Helm v2.14.3 官方文档阅读笔记","uri":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Indenting and Templates When writing templates, you may find yourself wanting to inject the contents of a file into the template. As we saw in previous chapters, there are two ways of doing this: Use {{ .Files.Get \"FILENAME\" }} to get the contents of a file in the chart. Use {{ include \"TEMPLATE\" . }} to render a template and then place its contents into the chart. When inserting files into YAML, it’s good to understand the multi-line rules above. Often times, the easiest way to insert a static file is to do something like this: myfile: | {{ .Files.Get \"myfile.txt\" | indent 2 }} Note how we do the indentation above: indent 2 tells the template engine to indent every line in “myfile.txt” with two spaces. Note that we do not indent that template line. That’s because if we did, the file content of the first line would be indented twice. ","date":"2019-10-13","objectID":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:8:5","tags":["k8s","tag2","tag3"],"title":"Helm v2.14.3 官方文档阅读笔记","uri":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Folded Multi-line Strings Sometimes you want to represent a string in your YAML with multiple lines, but want it to be treated as one long line when it is interpreted(被解释). This is called “folding”. To declare a folded block, use \u003e instead of |: coffee: \u003e Latte Cappuccino Espresso The value of coffee above will be Latte Cappuccino Espresso\\n. Note that all but the last line feed will be converted to spaces. You can combine the whitespace controls with the folded text marker, so \u003e- will replace or trim all newlines. Note that in the folded syntax, indenting text will cause lines to be preserved. coffee: \u003e- Latte 12 oz 16 oz Cappuccino Espresso The above will produce Latte\\n 12 oz\\n 16 oz\\nCappuccino Espresso. Note that both the spacing and the newlines are still there. ","date":"2019-10-13","objectID":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:8:6","tags":["k8s","tag2","tag3"],"title":"Helm v2.14.3 官方文档阅读笔记","uri":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"EMBEDDING MULTIPLE DOCUMENTS IN ONE FILE It is possible to place more than one YAML documents into a single file. This is done by prefixing a new document with --- and ending the document with ... --- document:1 ... --- document: 2 ... In many cases, either the --- or the ... may be omitted. Some files in Helm cannot contain more than one doc. If, for example, more than one document is provided inside of a values.yaml file, only the first will be used. Template files, however, may have more than one document. When this happens, the file (and all of its documents) is treated as one object during template rendering. But then the resulting YAML is split into multiple documents before it is fed to Kubernetes. We recommend only using multiple documents per file when it is absolutely necessary. Having multiple documents in a file can be difficult to debug. ","date":"2019-10-13","objectID":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:8:7","tags":["k8s","tag2","tag3"],"title":"Helm v2.14.3 官方文档阅读笔记","uri":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"YAML IS A SUPERSET OF JSON Because YAML is a superset of JSON, any valid JSON document should be valid YAML. { \"coffee\": \"yes, please\", \"coffees\": [ \"Latte\", \"Cappuccino\", \"Espresso\" ] } The above is another way of representing this: coffee:yes,pleasecoffees:- Latte- Cappuccino- Espresso And the two can be mixed (with care): coffee:\"yes, please\"coffees:[\"Latte\",\"Cappuccino\",\"Espresso\"] All three of these should parse into the same internal representation. While this means that files such as values.yaml may contain JSON data, Helm does not treat the file extension .json as a valid suffix. ","date":"2019-10-13","objectID":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:8:8","tags":["k8s","tag2","tag3"],"title":"Helm v2.14.3 官方文档阅读笔记","uri":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"YAML ANCHORS(锚)) The YAML spec provides a way to store a reference to a value, and later refer to that value by reference. YAML refers to this as “anchoring”: coffee:\"yes, please\"favorite:\u0026favoriteCoffee\"Cappucino\"coffees:- Latte- *favoriteCoffee- Espresso In the above, \u0026favoriteCoffee sets a reference to Cappuccino. Later, that reference is used as *favoriteCoffee. So coffees becomes Latte, Cappuccino, Espresso. While there are a few cases where anchors are useful, there is one aspect of them that can cause subtle bugs: The first time the YAML is consumed, the reference is expanded and then discarded. So if we were to decode and then re-encode the example above, the resulting YAML would be: coffee:yes,pleasefavorite:Cappucinocoffees:- Latte- Cappucino- Espresso Because Helm and Kubernetes often read, modify, and then rewrite YAML files, the anchors will be lost. ","date":"2019-10-13","objectID":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:8:9","tags":["k8s","tag2","tag3"],"title":"Helm v2.14.3 官方文档阅读笔记","uri":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":"Appendix: Go Data Types and Templates The Helm template language is implemented in the strongly typed Go programming language. For that reason, variables in templates are typed. For the most part, variables will be exposed as one of the following types: string: A string of text bool: a true or false int: An integer value (there are also 8, 16, 32, and 64 bit signed and unsigned variants of this) float64: a 64-bit floating point value (there are also 8, 16, and 32 bit varieties of this) a byte slice ([]byte), often used to hold (potentially) binary data struct: an object with properties and methods a slice (indexed list) of one of the previous types a string-keyed map (map[string]interface{}) where the value is one of the previous types There are many other types in Go, and sometimes you will have to convert between them in your templates. The easiest way to debug an object’s type is to pass it through printf \"%t\" in a template, which will print the type. Also see the typeOf and kindOf functions. ","date":"2019-10-13","objectID":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/:9:0","tags":["k8s","tag2","tag3"],"title":"Helm v2.14.3 官方文档阅读笔记","uri":"/posts/k8s/helm-v2.14.3-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"categories":null,"content":" 参考： https://blog.csdn.net/l1394049664/article/details/81811642 https://blog.csdn.net/zhangliao613/article/details/79021606 查看 CPU 型号 cat /proc/cpuinfo | grep name | cut -f2 -d: | uniq -c 以下表示该主机的 CPU 型号为 Intel(R) Xeon(R) CPU E5-2620 v2 @ 2.10GHz ，并且该主机共有 24 个逻辑 CPU。 [root@k8s-m1 ~]# cat /proc/cpuinfo | grep name | cut -f2 -d: | uniq -c 24 Intel(R) Xeon(R) CPU E5-2620 v2 @ 2.10GHz 查看物理 CPU 个数 cat /proc/cpuinfo| grep \"physical id\"| sort| uniq| wc -l 以下表示该主机存在两个物理 CPU： [root@k8s-m1 ~]# cat /proc/cpuinfo| grep \"physical id\"| sort| uniq| wc -l 2 查看每个 CPU 中 core 的个数（即核数） cat /proc/cpuinfo| grep \"cpu cores\"| uniq 以下表示该主机每个物理 CPU 存在 6 个核心。 [root@k8s-m1 ~]# cat /proc/cpuinfo| grep \"cpu cores\"| uniq cpu cores : 6 查看逻辑 CPU 的个数 cat /proc/cpuinfo| grep \"processor\"| wc -l 以下表示该主机存在 24 个逻辑 CPU [root@k8s-m1 ~]# cat /proc/cpuinfo| grep \"processor\"| wc -l 24 CPU 总核数 = 物理 CPU 个数 x 每颗物理 CPU 的核数 逻辑 CPU 总数 = 物理 CPU 个数 x 每颗物理 CPU 的核数 x 超线程数 CPU 架构 单核 CPU，每个 CPU 通过总线进行通信，效率比较低，如下图 多核 CPU，每个 CPU 内不同的核心通过 L2 cache 进行通信，存储和外设通过总线与 CPU 通信，如下图 多核超线程 CPU，每个 CPU 内的每个核心都有两个逻辑处理单元（逻辑 CPU），两个逻辑 CPU 共享一个核心的资源，如下图 查看内存信息 cat /proc/meminfo 如下所示： [root@k8s-m1 ~]# cat /proc/meminfo MemTotal: 16379240 kB MemFree: 9860512 kB MemAvailable: 14610972 kB Buffers: 2128 kB Cached: 4662128 kB SwapCached: 0 kB Active: 2300852 kB Inactive: 3258944 kB Active(anon): 763292 kB Inactive(anon): 964 kB Active(file): 1537560 kB Inactive(file): 3257980 kB Unevictable: 0 kB Mlocked: 0 kB SwapTotal: 0 kB SwapFree: 0 kB Dirty: 332 kB Writeback: 0 kB AnonPages: 874296 kB Mapped: 469584 kB Shmem: 2140 kB KReclaimable: 351024 kB Slab: 703168 kB SReclaimable: 351024 kB SUnreclaim: 352144 kB KernelStack: 15824 kB PageTables: 9076 kB NFS_Unstable: 0 kB Bounce: 0 kB WritebackTmp: 0 kB CommitLimit: 8189620 kB Committed_AS: 3189116 kB VmallocTotal: 34359738367 kB VmallocUsed: 0 kB VmallocChunk: 0 kB Percpu: 41472 kB HardwareCorrupted: 0 kB AnonHugePages: 428032 kB ShmemHugePages: 0 kB ShmemPmdMapped: 0 kB HugePages_Total: 0 HugePages_Free: 0 HugePages_Rsvd: 0 HugePages_Surp: 0 Hugepagesize: 2048 kB Hugetlb: 0 kB DirectMap4k: 313004 kB DirectMap2M: 8040448 kB DirectMap1G: 8388608 kB MemTotal 为当前主机的总内存，16379240 kB = 15GB 或者使用 free -h 查看内存容量情况 [root@k8s-m1 ~]# free -h total used free shared buff/cache available Mem: 15G 1.1G 9.4G 2.1M 5.1G 13G Swap: 0B 0B 0B 更多命令 uname -a # 查看内核/操作系统/CPU信息的linux系统信息 head -n l /etc/issue # 查看操作系统版本 cat /proc/cpuinfo # 查看CPU信息 hostname # 查看计算机名的linux系统信息命令 lspci -tv # 列出所有PCI设备 lsusb -tv # 列出所有USB设备的linux系统信息命令 lsmod # 列出加载的内核模块 env # 查看环境变量资源 free -m # 查看内存使用量和交换区使用量 df -h # 查看各分区使用情况 du -sh # 查看指定目录的大小 grep MemTotal /proc/meminfo # 查看内存总量 grep MemFree /proc/meminfo # 查看空闲内存量 uptime # 查看系统运行时间、用户数、负载 cat /proc/loadavg # 查看系统负载磁盘和分区 mount | column -t # 查看挂接的分区状态 fdisk -l # 查看所有分区 swapon -s # 查看所有交换分区 hdparm -i /dev/hda # 查看磁盘参数(仅适用于IDE设备) dmesg | grep IDE # 查看启动时IDE设备检测状况网络 ifconfig # 查看所有网络接口的属性 iptables -L # 查看防火墙设置 route -n # 查看路由表 netstat -lntp # 查看所有监听端口 netstat -antp # 查看所有已经建立的连接 netstat -s # 查看网络统计信息进程 ps -ef # 查看所有进程 top # 实时显示进程状态用户 w # 查看活动用户 id # 查看指定用户信息 last # 查看用户登录日志 cut -d: -f1 /etc/passwd # 查看系统所有用户 cut -d: -f1 /etc/group # 查看系统所有组 crontab -l # 查看当前用户的计划任务服务 chkconfig –list # 列出所有系统服务 chkconfig –list | grep on # 列出所有启动的系统服务程序 rpm -qa # 查看所有安装的软件包 cat /proc/cpuinfo ：查看CPU相关参数的linux系统命令 cat /proc/partitions ：查看linux硬盘和分区信息的系统信息命令 cat /proc/meminfo ：查看linux系统内存信息的linux系统命令 cat /proc/version ：查看版本，类似uname -r cat /proc/ioports ：查看设备io端口 cat /proc/interrupts ：查看中断 cat /proc/pci ：查看pci设备的信息 cat /proc/swaps ：查看所有swap分区的信息 ","date":"2019-10-12","objectID":"/posts/linux/linux%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%BF%A1%E6%81%AF%E6%9F%A5%E8%AF%A2/:0:0","tags":null,"title":"Linux服务器信息查询","uri":"/posts/linux/linux%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%BF%A1%E6%81%AF%E6%9F%A5%E8%AF%A2/"},{"categories":null,"content":" 参考： https://blog.csdn.net/fengwuxichen/article/details/87877214 k8s集群删除安装的一些插件后，发现，如果删除 namespace ，有些 namespace 会一直处于 terminating 状态，如下所示： [root@k8s-m1 ~]# kubectl get ns NAME STATUS AGE c-gbkmh Terminating 84d cattle-system Terminating 84d default Active 84d knative-monitoring Terminating 10d knative-serving Terminating 10d kube-node-lease Active 84d kube-public Active 84d kube-system Active 84d local Active 84d logging Active 13d monitoring Terminating 14d nfs-storageclass Terminating 84d p-9628v Active 84d p-bpmct Active 84d p-hrt5k Active 84d p-jb9sx Active 84d rook-ceph Terminating 14d tidb-admin Terminating 12d tidb-cluster Terminating 12d user-b7qf5 Active 84d wayne Active 84d 使用kubectl delete ns 命名空间名 --force 也无法删除。 最终是使用两个方法删除了处于terminating状态的namespace。 使用 kubectl edit ns 命名空间名称，将finalizers对应的内容删除，然后保存退出（:wq）。 具体步骤如下： [root@k8s-m1 ~]# kubectl edit ns harbor # Please edit the object below. Lines beginning with a '#' will be ignored, # and an empty file will abort the edit. If an error occurs while saving this file will be # reopened with the relevant failures. # apiVersion: v1 kind: Namespace metadata: annotations: cattle.io/status: '{\"Conditions\":[{\"Type\":\"ResourceQuotaInit\",\"Status\":\"True\",\"Message\":\"\",\"LastUpdateTime\":\"2019-07-17T15:50:49Z\"},{\"Type\":\"InitialRolesPopulated\",\"Status\":\"True\",\"Message\":\"\",\"LastUpdateTime\":\"2019-07-17T15:50:51Z\"}]}' lifecycle.cattle.io/create.namespace-auth: \"true\" creationTimestamp: \"2019-07-17T15:23:03Z\" deletionGracePeriodSeconds: 0 deletionTimestamp: \"2019-10-10T01:15:49Z\" finalizers: - controller.cattle.io/namespace-auth name: harbor resourceVersion: \"25421542\" selfLink: /api/v1/namespaces/harbor uid: 93e447e2-1bac-4e1c-825c-8578af09fa0b spec: {} status: phase: Terminating 将如下内容删掉 finalizers: - controller.cattle.io/namespace-auth 保存退出（:wq） 使用 kubectl get ns 查看当前的命名空间，发现 harbor 命名空间已删除。 使用该方法删除了绝大多数的命名空间，但是存在两个命名空间无法删除，分别为 cattle-system 和 c-gbkmh 。 使用 kube-proxy 删除处于 terminating 状态的命名空间 cattle-system命名空间的内容如下： [root@k8s-m1 ~]# kubectl edit ns cattle-system # Please edit the object below. Lines beginning with a '#' will be ignored, # and an empty file will abort the edit. If an error occurs while saving this file will be # reopened with the relevant failures. # apiVersion: v1 kind: Namespace metadata: annotations: cattle.io/status: '{\"Conditions\":[{\"Type\":\"ResourceQuotaInit\",\"Status\":\"True\",\"Message\":\"\",\"LastUpdateTime\":\"2019-07-17T15:50:47Z\"},{\"Type\":\"InitialRolesPopulated\",\"Status\":\"True\",\"Message\":\"\",\"LastUpdateTime\":\"2019-07-17T15:50:53Z\"}]}' field.cattle.io/projectId: local:p-bpmct kubectl.kubernetes.io/last-applied-configuration: | {\"apiVersion\":\"v1\",\"kind\":\"Namespace\",\"metadata\":{\"annotations\":{},\"name\":\"cattle-system\"}} lifecycle.cattle.io/create.namespace-auth: \"true\" creationTimestamp: \"2019-07-17T15:46:04Z\" deletionTimestamp: \"2019-10-10T01:15:18Z\" name: cattle-system resourceVersion: \"25439200\" selfLink: /api/v1/namespaces/cattle-system uid: 840d8744-81f2-4b0f-be0e-5bfb3d91f471 spec: finalizers: - kubernetes status: phase: Terminating 它不像上面 harbor 命名空间，它的 finalizers 位于 spec ，且 finalizers 为 kubernetes 。 尝试手动删除 finalizers 字段，保存退出后，namespace 没有被删除。 在终端中依次输入以下语句 NAMESPACE=cattle-system kubectl proxy \u0026 kubectl get namespace $NAMESPACE -o json |jq '.spec = {\"finalizers\":[]}' \u003etemp.json curl -k -H \"Content-Type: application/json\" -X PUT --data-binary @temp.json 127.0.0.1:8001/api/v1/namespaces/$NAMESPACE/finalize 在一个 k8s 节点（k8s-m1）上返回错误（可能是 k8s-m1 节点的 kube-proxy 存在问题） [root@k8s-m1 ~]# NAMESPACE=cattle-system [root@k8s-m1 ~]# kubectl proxy \u0026 [1] 33592 [root@k8s-m1 ~]# kubectl get namespace $NAMESPACE -o json |jq '.spec = {\"finalizers\":[]}' \u003etemp.json Starting to serve on 127.0.0.1:8001 [root@k8s-m1 ~]# curl -k -H \"Content-Type: application/json\" -X PUT --data-binary @temp.json 127.0.0.1:8001/api/v1/namespaces/$NAMESPACE/finalize curl: (52) Empty reply from server 在另一个节点（k8s-m2）上删除成功，成功如下所示： [root@k8s-m2 ~]# NAMESPACE=cattle-system [root@k8s-m2 ~]# kubectl proxy \u0026 [1] 25118 [root@","date":"2019-10-10","objectID":"/posts/k8s/k8s%E5%88%A0%E9%99%A4%E5%A4%84%E4%BA%8Eterminating%E7%8A%B6%E6%80%81%E7%9A%84namespace/:0:0","tags":null,"title":"K8s删除处于terminating状态的namespace","uri":"/posts/k8s/k8s%E5%88%A0%E9%99%A4%E5%A4%84%E4%BA%8Eterminating%E7%8A%B6%E6%80%81%E7%9A%84namespace/"},{"categories":null,"content":" idea支持新建类自动添加注释 File-Settings-Editor-File and Code Templates-Files-Classes 在public class上方添加如下注释 /**\r* @Description: ${DESCRIPTION}\r* @Author: ${USER}\r* @Date: ${DATE} ${TIME}\r*/\r idea支持方法自动注释 File-Settings-Editor-Live Templates 选择右方+号， 选择2.Template Group新建group，命名为MyGroup 选中新建的MyGroup，再次选中右方+号，选择1.Live Template新建模板 Abbreviation中填*， Template text内容如下： **\r* @Description $end$\r* @Author $author$\r* @Date $date$ $time$\r* @Param $param$ * @Return $return$\r*/\r 选择右方Edit variables按钮为上文中的变量配置对应表达式（Expression） author 对应的Expression为 user() date 对应的Expression为 date() time 对应的Expression为 time() param 对应的Expression为 methodParameters() return 对应的Expression为 methodReturnType() 此时下方会有一个警告，No application contexts. Define 选中Define按钮，选择Java 然后选择ok保存。 ","date":"2019-08-21","objectID":"/posts/idea/idea%E6%94%AF%E6%8C%81%E8%87%AA%E5%8A%A8%E6%B3%A8%E9%87%8A/:0:0","tags":null,"title":"Idea支持自动注释","uri":"/posts/idea/idea%E6%94%AF%E6%8C%81%E8%87%AA%E5%8A%A8%E6%B3%A8%E9%87%8A/"},{"categories":null,"content":"win10 安装 Hugo 在 Hugo Release 页面下载对应操作系统的 Hugo 二进制文件。 如 win10 64 位系统下载 hugo_0.56.3_Windows-64bit.zip ，将该文件解压到指定目录，如 D:\\Develop\\Hugo\\bin 。 配置 Hugo 环境变量 win10 系统变量 PATH 添加 D:\\Develop\\Hugo\\bin ","date":"2019-08-01","objectID":"/posts/hugo/hugo%E6%90%AD%E5%BB%BAgithub%E5%8D%9A%E5%AE%A2/:1:0","tags":null,"title":"Hugo搭建GitHub博客","uri":"/posts/hugo/hugo%E6%90%AD%E5%BB%BAgithub%E5%8D%9A%E5%AE%A2/"},{"categories":null,"content":"使用 Hugo 创建静态网页 进入期望存储项目的目录，如 D:\\workspace\\vscode\\git ，在该目录打开终端（ windows 下为 cmd 或 powershell）进行操作，或者使用 vscode 打开期望存储项目的目录，点击 ctrl+~ 打开 terminal ，进行操作 在当前目录生成名为 mysite 的站点 命令如下， mysite 为生成的项目名 hugo new site mysite 执行后内容如下： PS D:\\workspace\\vscode\\git\u003e hugo new site mysite Congratulations! Your new Hugo site is created in D:\\workspace\\vscode\\git\\mysite. Just a few more steps and you're ready to go: 1. Download a theme into the same-named folder. Choose a theme from https://themes.gohugo.io/ or create your own with the \"hugo new theme \u003cTHEMENAME\u003e\" command. 2. Perhaps you want to add some content. You can add single files with \"hugo new \u003cSECTIONNAME\u003e\\\u003cFILENAME\u003e.\u003cFORMAT\u003e\". 3. Start the built-in live server via \"hugo server\". Visit https://gohugo.io/ for quickstart guide and full documentation. 进入 mysite 站点，查看生成的项目目录结构 PS D:\\workspace\\vscode\\git\u003e cd .\\mysite\\ PS D:\\workspace\\vscode\\git\\mysite\u003e ls 目录: D:\\workspace\\vscode\\git\\mysite Mode LastWriteTime Length Name ---- ------------- ------ ---- d----- 2019-08-01 11:25 archetypes d----- 2019-08-01 11:25 content d----- 2019-08-01 11:25 data d----- 2019-08-01 11:25 layouts d----- 2019-08-01 11:25 static d----- 2019-08-01 11:25 themes -a---- 2019-08-01 11:25 82 config.toml config.toml 是网站的配置文件，包括 baseurl， title， copyright 等等网站参数 archetypes：包括内容类型，在创建新内容时自动生成内容的配置 content：包括网站内容，全部使用 markdown 格式 layouts：包括了网站的模版，决定内容如何呈现 static：包括了 css，js， fonts， media 等，决定网站的外观 安装主题 Hugo 社区提供了一些可用的主题，使用如下命令将所有的主题下载到 themes 目录下 git clone --depth 1 --recursive https://github.com/gohugoio/hugoThemes.git themes 或者下载特定的主题，使用如下命令下载 beautifulhugo 主题到 themes/beautifulhugo 目录 git clone https://github.com/halogenica/beautifulhugo.git themes/beautifulhugo 将 themes/beautifulhugo/exampleSite/config.toml 文件内容拷贝到 mysite 目录中，覆盖原文件内容。 创建新页面 PS D:\\workspace\\vscode\\git\\mysite\u003e hugo new post/about.md D:\\workspace\\vscode\\git\\mysite\\content\\post\\about.md created 进入 content/ 目录，可以看到多了一个 post 目录，里面有新建的 about.md 文件。打开文件可以看到时间和文件名等信息已经自动加到文件开头，包括标题、创建时间、是否为草稿等。 about.md 中添加 Hello Hugo! ，添加后内容如下所示： PS D:\\workspace\\vscode\\git\\mysite\u003e cat .\\content\\post\\about.md --- title: \"About\" date: 2019-08-01T13:11:32+08:00 draft: true --- Hello Hugo! — 标记的内容是 YAML 格式的，可以换成 TOML 格式（使用 +++ 标记）或者 JSON 格式。 hugo new 命令生成的文章前面包括的那几行，是用来设置文章属性的，这些属性使用的是 yaml 语法。 title 设置文章第一级标题，一个 md 文档中只能有一个一级标题。 date 时间标签，页面上默认显示 n 篇最新的文章。 draft 设置为 false 的时候会被编译为 HTML ， true 则不会编译和发表。 tags 数组，可以设置多个标签，逗号隔开， Hugo 会自动在你博客主页下生成标签的子 URL ，通过这个 URL 可以看到所有具有该标签的文章。 categories 文章分类，跟 Tag 功能差不多，只能设置一个字符串。 更多详见： https://gohugo.io/content-management/front-matter/ 运行 hugo hugo server -t beautifulhugo --buildDrafts -t 参数的意思是使用特定主题渲染页面，注意到 about.md 目前是作为草稿，即 draft 参数设置为 true ，运行 Hugo 时要加上 --buildDrafts 参数才会将标记为草稿的页面生成 HTML 。 在浏览器输入 http://localhost:1313 ，就可以看到我们刚刚创建的页面。 PS D:\\workspace\\vscode\\git\\mysite\u003e hugo server -t beautifulhugo --buildDrafts Building sites … | EN +------------------+-----+ Pages | 10 Paginator pages | 0 Non-page files | 0 Static files | 183 Processed images | 0 Aliases | 2 Sitemaps | 1 Cleaned | 0 Total in 2581 ms Watching for changes in D:\\workspace\\vscode\\git\\mysite\\{archetypes,content,data,layouts,static,themes} Watching for config changes in D:\\workspace\\vscode\\git\\mysite\\config.toml Environment: \"development\" Serving pages from memory Running in Fast Render Mode. For full rebuilds on change: hugo server --disableFastRender Web Server is available at http://localhost:1313/ (bind address 127.0.0.1) Press Ctrl+C to stop ","date":"2019-08-01","objectID":"/posts/hugo/hugo%E6%90%AD%E5%BB%BAgithub%E5%8D%9A%E5%AE%A2/:2:0","tags":null,"title":"Hugo搭建GitHub博客","uri":"/posts/hugo/hugo%E6%90%AD%E5%BB%BAgithub%E5%8D%9A%E5%AE%A2/"},{"categories":null,"content":"将静态网页部署到 GitHub 首先在 GitHub 上创建一个 Repository ，命名为 msdemt.github.io （ msdemt 替换为你的 github 用户名）。 修改 mysite/config.toml 文件， baseurl 配置为自己的 github 用户名，如本文的配置为 baseurl = \"https://msdemt.github.io\" 在站点根目录执行 Hugo 命令生成最终页面： hugo --theme=beautifulhugo 如果一切顺利，所有 draft 为 false 的静态页面都会生成到 public 目录，将 pubilc 目录里所有文件 push 到刚创建的 Repository 的 master 分支。 在 git shell 或 cmd 中执行如下命令将 public 中的内容提交到 github 中 cd public git init git remote add origin https://github.com/msdemt/msdemt.github.io.git git add -A git commit -m \"init\" git push -u origin master 详细步骤如下： PS D:\\workspace\\vscode\\git\\mysite\u003e hugo --theme=beautifulhugo Building sites … | EN +------------------+-----+ Pages | 10 Paginator pages | 0 Non-page files | 0 Static files | 183 Processed images | 0 Aliases | 2 Sitemaps | 1 Cleaned | 0 Total in 4167 ms PS D:\\workspace\\vscode\\git\\mysite\u003e cd .\\public\\ PS D:\\workspace\\vscode\\git\\mysite\\public\u003e git init Initialized empty Git repository in D:/workspace/vscode/git/mysite/public/.git/ PS D:\\workspace\\vscode\\git\\mysite\\public\u003e git remote add origin https://github.com/msdemt/msdemt.github.io.git PS D:\\workspace\\vscode\\git\\mysite\\public\u003e git add -A PS D:\\workspace\\vscode\\git\\mysite\\public\u003e git commit -m \"init\" [master (root-commit) f5d1d02] init 196 files changed, 23076 insertions(+) create mode 100644 404.html create mode 100644 categories/index.html create mode 100644 categories/index.xml create mode 100644 css/bootstrap.min.css create mode 100644 css/codeblock.css create mode 100644 css/fonts.css ... ... create mode 100644 tags/index.html create mode 100644 tags/index.xml PS D:\\workspace\\vscode\\git\\mysite\\public\u003e git push -u origin master Counting objects: 216, done. Delta compression using up to 4 threads. Compressing objects: 100% (210/210), done. Writing objects: 100% (216/216), 4.70 MiB | 156.00 KiB/s, done. Total 216 (delta 10), reused 0 (delta 0) remote: Resolving deltas: 100% (10/10), done. To https://github.com/msdemt/msdemt.github.io.git * [new branch] master -\u003e master Branch 'master' set up to track remote branch 'master' from 'origin'. ","date":"2019-08-01","objectID":"/posts/hugo/hugo%E6%90%AD%E5%BB%BAgithub%E5%8D%9A%E5%AE%A2/:3:0","tags":null,"title":"Hugo搭建GitHub博客","uri":"/posts/hugo/hugo%E6%90%AD%E5%BB%BAgithub%E5%8D%9A%E5%AE%A2/"},{"categories":null,"content":"实战 也可以使用子模块的方式安装主题， 如安装 loveit 主题到 themes 目录下： git submodule add https://github.com/dillonzq/LoveIt.git themes/LoveIt 安装 zozo 主题到 themes 目录下 git submodule add https://github.com/varkai/hugo-theme-zozo.git themes/zozo 为了便于开发和部署，可以将 hugo 生成的静态网页存放的 public 目录作为 github 静态网页仓库的子模块 如： git submodule add https://github.com/msdemt/msdemt.github.io.git public 下载mysite项目，同时下载依赖的子模块 git clone https://github.com/msdemt/mysite.git --recursive 将子模块更新到最新版本 git submodule update --remote 进入 public 目录，执行如下命令，执行了之后才可以提交该子模块的内容 cd public git checkout master 进入mysite目录，增加 博客文章，之后执行 hugo 命令生成博客内容 cd .. hugo 在进入public 目录，将博客内容提交到github cd public git add . git commit -m \"test\" git push origin master 再进入mysite目录，更新子模块public到最新版本 cd .. git submodule update --remote 然后将mysite项目提交到github git add . git commit -m \"test\" git push origin master ","date":"2019-08-01","objectID":"/posts/hugo/hugo%E6%90%AD%E5%BB%BAgithub%E5%8D%9A%E5%AE%A2/:4:0","tags":null,"title":"Hugo搭建GitHub博客","uri":"/posts/hugo/hugo%E6%90%AD%E5%BB%BAgithub%E5%8D%9A%E5%AE%A2/"},{"categories":null,"content":"问题汇总 问题1： 生成最终页面时指定了 baseUrl ，将页面部署到 github 上时，页面出错 解决：baseUrl 在 hugo 命令是指定或在 config.toml 中指定一次即可。 hugo --theme=beautifulhugo --baseUrl=\"http://msdemt.github.io/\" 问题2： 生成最终页面时未指定baseUrl，同时也未在mysite/config.toml文件中指定baseUrl时，将页面部署到github上，页面出错 解决：baseUrl 在 hugo 命令是指定或在 config.toml 中指定一次即可。 hugo --theme=beautifulhugo 问题3： 默认 Hugo 生成的 HTML 页面只有两级标题，对于多级标题的文章，生成标题目录（ tables of content）后会发现本来为三级标题但是显示的是二级标题。 github 上的大佬已经有了解决办法，详见： https://github.com/gohugoio/hugo/issues/1778 https://gist.github.com/pyrrho/1d77cdb98ba58c7547f2cdb3fb325c62 解决办法： 将 tables-of-contents.html 放入 themes/beautifulhugo/layouts/partials 目录下，作者认为 4 级标题就够用了，所以配置了最高级别 4 ，下面的文件中配置为了 6 ，tables-of-contents.html 的内容如下： {{/* This partial was derived from the conversations in and around https://github.com/gohugoio/hugo/issues/1778 -- most directly skyzyx's gist; https://gist.github.com/skyzyx/a796d66f6a124f057f3374eff0b3f99a */}} {{/* Minimum heading level to include; default is h2. */}} {{- $minLevel := 2 -}} {{/* Minimum heading level to include; default is h4. */}} {{- $maxLevel := 6 -}} {{/* Search for headings as specified by $minLevel and $maxLevel, ignoring those that contain no text (ex; \"\u003ch2\u003e\u003c/h2\u003e\" will be ignored). */}} {{- $regex := printf \"\u003ch[%d-%d].*?\u003e(.|\\n])+?\u003c/h[%d-%d]\u003e\" $minLevel $maxLevel $minLevel $maxLevel -}} {{- $headings := findRE $regex .Content -}} {{/* Skip generation if there are no (suitable) headings. */}} {{- $hasHeadings := ge (len $headings) 1 -}} {{- if $hasHeadings -}} \u003cnav class=\"enclosing-block\" class=\"customize-as-needed\"\u003e {{- .Scratch.Set \"toc__last-level\" (sub $minLevel 1) -}} {{- range $heading := $headings -}} {{- $headingLevel := substr $heading 2 1 | int -}} {{- $headingID := index (findRE \"id=.([a-z0-9-_])*\" $heading) 0 | after 4 -}} {{- $headingTextRaw := substr (index (findRE \"\u003e.*\u003c/h\" $heading 1) 0) 1 -3 -}} {{/* There may be an anchor tag wrapping some or all of the heading text. We don't want those tags in our ToC text, so lets get rid of them. */}} {{- $headingText := replaceRE \"\u003c/?a.*?\u003e\" \"\" $headingTextRaw | markdownify }} {{- $lastLevel := $.Scratch.Get \"toc__last-level\" -}} {{- $href := printf \"#%s\" $headingID -}} {{- $levelSeq := seq $lastLevel $headingLevel | after 1 -}} {{- $.Scratch.Set \"toc__last-level\" $headingLevel -}} {{- if gt $headingLevel $lastLevel -}} {{- range $l := $levelSeq -}} \u003cul class=\"table-of-contents__h{{ $l }}\"\u003e\u003cli\u003e {{- end -}} {{- else if lt $headingLevel $lastLevel -}} {{- range $l := $levelSeq -}} \u003c/li\u003e\u003c/ul\u003e {{- end -}} \u003c/li\u003e\u003cli\u003e {{- else -}} \u003c/li\u003e\u003cli\u003e {{- end -}} \u003ca href=\"{{ $href }}\"\u003e{{ $headingText }}\u003c/a\u003e {{- end -}} {{- range seq ($.Scratch.Get \"toc__last-level\") (sub $minLevel 1) | after 1 -}} \u003c/li\u003e\u003c/ul\u003e {{- end -}} \u003c/nav\u003e {{- end -}} 在 themes/beautifulhugo/layouts/_default/single.html 页面中加入 {{- partial \"tables-of-contents.html\" . -}} 加入后 single.html 内容如下所示： {{ define \"main\" }} \u003cdiv class=\"container\" role=\"main\"\u003e \u003cdiv class=\"row\"\u003e \u003cdiv class=\"col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1\"\u003e \u003carticle role=\"main\" class=\"blog-post\"\u003e {{- partial \"tables-of-contents.html\" . -}} {{ .Content }} {{ if .Params.tags }} \u003cdiv class=\"blog-tags\"\u003e {{ range .Params.tags }} \u003ca href=\"{{ $.Site.LanguagePrefix | absURL }}/tags/{{ . | urlize }}/\"\u003e{{ . }}\u003c/a\u003e\u0026nbsp; {{ end }} \u003c/div\u003e {{ end }} {{ if $.Param \"socialShare\" }} \u003chr/\u003e \u003csection id=\"social-share\"\u003e \u003cdiv class=\"list-inline footer-links\"\u003e {{ partial \"share-links\" . }} \u003c/div\u003e \u003c/section\u003e {{ end }} {{ if .Site.Params.showRelatedPosts }} {{ $related := .Site.RegularPages.Related . | first 3 }} {{ with $related }} \u003ch4 class=\"see-also\"\u003e{{ i18n \"seeAlso\" }}\u003c/h4\u003e \u003cul\u003e {{ range . }} \u003cli\u003e\u003ca href=\"{{ .RelPermalink }}\"\u003e{{ .Title }}\u003c/a\u003e\u003c/li\u003e {{ end }} \u003c/ul\u003e {{ end }} {{ end }} \u003c/article\u003e {{ if ne .Type \"page\" }} \u003cul class=\"pager blog-pager\"\u003e {{ if .PrevInSection }} \u003cli class=\"previous\"\u003e \u003ca href=\"{{ .PrevInSection.Permalink }}\" data-toggle=\"tooltip\" data-placement=\"top\" title=\"{{ .PrevInSection.Title }}\"\u003e\u0026larr; {{ i18n \"previousPost\" }}\u003c/a\u003e \u003c/li\u003e {{ end }} {{ if .","date":"2019-08-01","objectID":"/posts/hugo/hugo%E6%90%AD%E5%BB%BAgithub%E5%8D%9A%E5%AE%A2/:5:0","tags":null,"title":"Hugo搭建GitHub博客","uri":"/posts/hugo/hugo%E6%90%AD%E5%BB%BAgithub%E5%8D%9A%E5%AE%A2/"},{"categories":null,"content":"Hello Hugo! ","date":"2019-08-01","objectID":"/posts/about/:0:0","tags":null,"title":"About","uri":"/posts/about/"}]